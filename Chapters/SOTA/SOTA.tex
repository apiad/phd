\chapter{Estado de la Cuestión}\label{Chap:SOTA}
\markboth{\MakeUppercase{Estado de la Cuesti\'on}}{}

Intro del estado del arte

- el descubrimiento de conocimiento en lenguaje natural

Los humanos consumen la información ambiental y la transforman en conceptos independientes, que pueden relacionarse entre sí.
En general, solo almacenamos una pequeña parte de toda la información que procesamos todos los días.
Esta información consta de los conceptos y relaciones más importantes, que pueden ser útiles en situaciones futuras.
Así, aprendemos básicamente separando lo que podría considerarse ruido de la información que pensamos que será útil en el futuro.
En general, dijimos dos tipos de recuerdos: uno para el conocimiento reciente y otro donde almacenamos el conocimiento durante mucho tiempo.

Hoy en día, una de las mayores fuentes de información de Internet
es texto en lenguaje natural, que aparece en noticias, opiniones, mensajes, enciclopedias y muchas otras formas de comunicación digital.
Esto no es accidental, ya que las palabras ~ (representadas computacionalmente como texto) son uno de los canales de comunicación más importantes a lo largo de la historia de la humanidad.
Uno de los conceptos más básicos reconocibles en el lenguaje natural que brindan información útil son las acciones. Por lo general, se asocian con dos conceptos distintivos, el sujeto y el objetivo. El primero es quién realiza la acción, y el segundo es quién recibe las consecuencias de dicha acción.
Identificar correctamente las acciones, sus correspondientes sujetos y destinatarios es uno de los retos más básicos y fundamentales para comprender la información.

El crecimiento acelerado de Internet ha producido un excedente de información (noticias, correo electrónico, redes sociales, blogs) que supera con creces nuestra capacidad de procesamiento y consumo de datos.
Así, la construcción de sistemas automáticos que puedan extraer conocimiento de este flujo de información se ha convertido en uno de los campos de investigación más activos en la informática. Este fenómeno ha sido
denominado Big Data ~\cite{bigdata}, y su estudio ha atraído la atención de diferentes comunidades de investigación como inteligencia empresarial ~\cite{chen2012business}, ingeniería que incluye física y biológica ~\cite{wu2014data} y redes sociales ~\cite{shah2015big}.

- importancia de la semántica

% PAPER ICAII

Los seres humanos intentan representar el mundo en el que vivimos y los hechos y eventos que suceden usando el lenguaje. El lenguaje permite estructurar las ideas para que puedan compartirse con otras personas.
Generalmente, una de las formas más comunes de representar piezas arbitrarias de conocimiento es a través de la noción de conceptos y acciones ~\cite{teleology}.
Al comprender el mundo como compuesto de conceptos que interactúan entre sí a través de acciones, los humanos pueden comunicar una gran variedad y complejidad de información y conocimiento.
La forma más sencilla de combinar conceptos y acciones es a través de relaciones binarias, donde un concepto actúa como sujeto y otro como objetivo.
Esta estructura simple es muy común en los lenguajes humanos.
Más del 75 \% de los idiomas tienen una estructura sintáctica que sigue el orden SVO (sujeto-verbo-objeto) o SOV (sujeto-objeto-verbo) ~\cite{cambridge}.
Además, Sujeto-Verbo-Objeto es el orden más común en las lenguas criollas, lo que se ha sugerido como una indicación de que es el orden más natural.
para la psicología humana. ~\cite{diamond2013rise}.
Como ejemplo, de la oración `` Arnold conduce un automóvil '' y obtenemos un triplete Sujeto-Verbo-Objeto \verb|(Arnold, conduce, automóvil)|.

Esta estructura es omnipresente en muchos problemas diferentes en el campo del procesamiento del lenguaje natural.
Diferentes autores han empleado un Sujeto-Verbo-Objeto para una estructura similar o para representar conocimiento extraído de un texto natural, en una variedad de dominios ~\cite{mitchell2015never, emotinet}.
Así, una representación basada en el triplete Sujeto-Verbo-Objeto es adecuada para la representación genérica del conocimiento, independientemente del dominio.

Las ontologías son una de las representaciones más comunes del conocimiento en formato digital.
Las ontologías, las bases de conocimiento, las redes semánticas y las representaciones computacionales similares a menudo se basan en el concepto del triplete Sujeto-Verbo-Objeto, incluso si se hace referencia a los componentes individuales usando términos diferentes (es decir, relaciones en tripletes RDF o predicados en lógica bases de conocimiento).

El auge de la web semántica ha fomentado la creación de ontologías y bases de conocimiento a gran escala en varios dominios.
Algunas ontologías representan dominios generales, como DBPedia, y recopilan una gran proporción del conocimiento humano común.
Otros, como Ivanovic y Budimac ~ (\ cite {IVANOVIC20145158}) operan en un dominio más específico pero incorporan datos más detallados sobre el dominio.
Uno de los mayores obstáculos para el desarrollo de la web semántica es la cantidad de esfuerzo y tiempo que necesitan los expertos humanos para construir ontologías a mano ~\cite{gomez2006ontological, petasis2011ontology}.
En este sentido, el campo del aprendizaje de la ontología ~\cite{buitelaar2005ontology} estudia las técnicas y metodologías que permiten la extracción automática o semiautomática de ontologías de fuentes de información no estructuradas, como el texto natural ~\cite{mitchell2015never, emotinet}.

Una de las cuestiones más importantes en el aprendizaje de ontología a partir del texto natural es cómo reconocer qué conocimiento es relevante en un dominio. Por lo general, en un corpus de texto natural, una gran parte de la información será falsa o sin importancia ~\cite{Kanya2009InformationE}.
Los seres humanos tienen una capacidad innata para olvidar los hechos espurios o sin importancia que se nos presentan a diario, y solo almacenan a largo plazo ese conocimiento que es relevante para un propósito particular.
Se han propuesto varias métricas relevantes ~\cite{manning2008introduction, brank2005survey}.
En general, el conocimiento más relevante puede estar relacionado con las acciones y conceptos que aparecen con mayor frecuencia en un dominio.

% PAPER JBI

El crecimiento exponencial de Internet en las últimas décadas ha producido un excedente masivo de información textual en todas las áreas del quehacer humano. Este escenario presenta tanto una oportunidad como un desafío para los investigadores. Por un lado, se dispone de una cantidad cada vez mayor de literatura científica, donde se podrían encontrar posibles soluciones para problemas críticos vinculando resultados parciales publicados en distintos documentos. Por otro lado, la extensión de la información disponible no puede ser procesada por humanos solos en un período de tiempo razonable. Por lo tanto, los esfuerzos se han dirigido recientemente hacia el diseño de técnicas automáticas que pueden descubrir piezas de información relevantes en grandes corpora, establecer conexiones lógicas y sintetizar conocimientos útiles.
El primer paso en muchas de estas técnicas implica la recopilación, el procesamiento y la anotación de datos que se pueden usar para entrenar algoritmos de aprendizaje automático o construir sistemas expertos mediante el uso de técnicas de procesamiento del lenguaje natural.

El sector de la salud digital es de gran interés para la comunidad investigadora dados los posibles beneficios sociales derivados de la aplicación de tecnologías de descubrimiento automático del conocimiento. La comunidad de investigación ha producido una gran cantidad de corpus anotados en diferentes subdominios de este sector, desde interacciones específicas (p. Ej., Fármaco-enfermedad ~\cite{goldberg1996drug} o interacciones gen-proteína ~\cite{tanabe2005genetag}) hasta amplio alcance y dominio (por ejemplo, informes de ensayos clínicos ~\cite{nye2018corpus}).
Los corpus y las tecnologías de dominios específicos son de vital importancia en la medicina de alta precisión.
Sin embargo, los sistemas construidos para dominios muy específicos son posiblemente más difíciles de generalizar y extender que los sistemas construidos sobre conceptualizaciones de propósito general.
Como tal, existe un interés creciente en diseñar modelos de anotación y corpus con semántica de propósito general que se puedan usar en una variedad de dominios o como un componente en sistemas más especializados.

Además del dominio, el lenguaje es otra dimensión que ha sido el foco de investigaciones recientes.
La mayoría de los recursos lingüísticos más importantes se basan en fuentes del inglés, motivados en parte por la abundancia de materia prima disponible ~ (por ejemplo, enciclopedias en línea, trabajos de investigación), lo cual no es sorprendente dado que el inglés es el idioma más predominante en ciencia, tecnología y comunicaciones.
Sin embargo, los recursos en inglés no siempre son directamente aplicables a otros idiomas.
Aunque la traducción automática ha alcanzado una precisión impresionante en dominios abiertos, sigue siendo un desafío crear recursos en varios idiomas, como el español, que es menos predominante en los dominios técnicos ~\cite{villegas2018mespen}.
En lugar de centrarse en lenguajes de nicho específicos, una posible línea de investigación es diseñar recursos que sean agnósticos del lenguaje, en el sentido de que se puedan generalizar a varios lenguajes con poco esfuerzo, en virtud de que se basan en características comunes subyacentes compartidas por muchos lenguajes. .

El diseño de modelos de anotaciones que puedan generalizarse a varios dominios requiere decidir una representación básica del lenguaje que cubra una amplia gama de semánticas.
Además, estas representaciones deben ser lo más independientes posible de la sintaxis y las reglas gramaticales, si se espera que se generalicen a varios idiomas.
Un trabajo reciente ~\cite{estevez2018gathering} sugiere que los tripletes Sujeto-Acción-Objetivo pueden usarse para detectar una gran cantidad de interacciones semánticas en lenguaje natural, independientemente del dominio y relativamente independientes del lenguaje, ya que
más del 75 \% de los lenguajes humanos emplean alguna variación de la estructura gramatical Sujeto-Verbo-Objeto ~\cite{crystal2004cambridge}.
Asimismo, varias representaciones ontológicas a menudo concuerdan en una serie de relaciones de propósito general, (por ejemplo, \textit{es-a} hipónimos, \textit{parte-de} holónimos) que son útiles en cualquier dominio.
Otras conceptualizaciones permiten capturar la semántica más cercana al lenguaje natural, como la Representación de significado abstracto, AMR ~\cite{banarescu2013abstract}.
La construcción de corpus anotados con estructuras semánticas de propósito general como sujeto-acción-objetivo y relaciones ontológicas de alto nivel es el primer paso en el diseño de sistemas que pueden descubrir conocimiento automáticamente en una variedad de dominios y escenarios.

La investigación en el descubrimiento del conocimiento requiere no solo recursos lingüísticos ~ (por ejemplo, corpus anotados) sino también recursos e infraestructuras computacionales que permitan a los investigadores evaluar sistemáticamente sus resultados y compararlos objetivamente con enfoques alternativos.
Esto implica la definición formal de tareas y el diseño de métricas de evaluación objetivas que aseguren que es posible una comparación justa.
Aún mejor es un sistema de evaluación disponible públicamente donde los investigadores pueden presentar sus resultados, garantizando que se apliquen los mismos criterios de evaluación y liberando a los investigadores de reproducir el entorno de evaluación. Dicho sistema también garantizaría un proceso de investigación más transparente y reproducible, y proporcionaría un depósito centralizado de los enfoques existentes, lo que ayudaría a los nuevos investigadores a actualizarse sobre el estado de la técnica.

% RESUMEN DEL PROCESO

- explicar el proceso y las tareas
  - definir un esquema de anotacion, modelo semántico, que sea bueno
  - herramientas con las que hacer la anotacion
  - anotación asistida y utilidades para anotar mejor
  - anotar un corpus con metricas de calidad y merging, hablar de la metodologia de anotacion
  - entrenar sistemas de machine learning para la extraccion automática
  - diseñar entornos de evaluación (challenges) para comparar sistemas
  - construir ontologías

\section{Modelos Semánticos de Anotación}

El descubrimiento del conocimiento es un campo de la informática que muestra un crecimiento acelerado en las últimas tres décadas.
Los avances en esta área se han aplicado en muchos dominios, desde bases de datos ~\cite{fayyad1996data, knowledgeDatabase} hasta imágenes ~\cite{lu2016visual} y texto en lenguaje natural ~\cite{carlson2010toward}.
Específicamente en el texto en lenguaje natural, este campo es de gran relevancia en los dominios biomédico y de salud, donde se utiliza para realizar tareas como
Reconocimiento de entidades nombradas ~ (NER), extracción de relaciones y generación de hipótesis, entre otros. ~\Cite{simpson2012biomedical}.
Estas tareas generalmente utilizan corpus anotados para aprender las características que aparecen en el texto y mapearlas con las estructuras de conocimiento.
Para cada tarea, se han diseñado modelos de anotaciones específicos que se enfocan en elementos específicos del texto.
Por ejemplo, en las tareas NER es más importante centrarse en frases nominales que en otras construcciones gramaticales.

A pesar de que estas tareas específicas de dominio son diferentes, la mayoría de ellas comparten características comunes. Por ejemplo, la mayoría de las tareas se ocupan de la detección de entidades relevantes y sus relaciones. Por lo tanto, la promoción de modelos de anotación de propósito general permitiría el diseño de técnicas de descubrimiento de conocimiento reutilizables y entre dominios.
En esta línea, se han desarrollado varias representaciones semánticas independientes del dominio ~ (por ejemplo, AMR ~\cite{amr}, PropBank ~\cite{propbank}, FrameNet ~\cite{framenet}).
Sin embargo, estas representaciones se basan en gran medida en léxicos detallados que definen roles semánticos específicos para el significado de cada palabra. Por tanto, desarrollar sistemas de descubrimiento de conocimiento con este nivel de detalle supone grandes retos. El uso de una representación semántica más burda, incluso con la pérdida de cierta capacidad de representación, simplificaría la creación de técnicas automáticas basadas en el aprendizaje automático.
Esta representación también podría usarse como la primera etapa en una canalización para una tarea específica de dominio, reutilizando así recursos y técnicas en dominios con pocos recursos disponibles.
En esta sección presentamos una revisión de los modelos de anotaciones relevantes en los que nos inspiramos.
Nos centramos en los modelos de anotación de propósito general~\ref{sec: general} así como en los modelos de anotación que se han aplicado al dominio de salud~\ref{sec: health}.

Para proporcionar resultados de búsqueda más detallados, los documentos se pueden procesar para extraer las entidades semánticas relevantes y los hechos mencionados.
La tarea de descubrir automáticamente el conocimiento semántico a partir del texto está cubierta por áreas de investigación como el aprendizaje de ontología~\cite{cimiano2009ontology} y {aprender leyendo}~\cite{barker2007learning},
cuyo propósito es construir redes semánticas que {capturen} el conocimiento presente en grandes colecciones de texto.
Estas redes semánticas permiten el uso de motores de búsqueda que brindan un análisis más allá de la relevancia del contenido textual, explotando la estructura semántica de la red.
En este contexto, el procesamiento de contenidos textuales de salud ha despertado un {gran interés}~\cite{gonzalez2017capturing}, motivado por la gran cantidad de documentos médicos publicados anualmente.

Existen varios enfoques para construir representaciones semánticas del conocimiento.
En muchos casos, estas representaciones utilizan una conceptualización específica de dominio.
Aunque esto proporciona una representación más especializada, hace que estos enfoques sean más difíciles de aplicar a una amplia gama de dominios.
Alternativamente, se podría utilizar una conceptualización de propósito general, que sea capaz de representar entidades y hechos de múltiples dominios de conocimiento.
Tal conceptualización debería ser lo suficientemente general como para dar cabida a muchos dominios diferentes, pero aún para proporcionar un grado de expresividad necesario para las tareas de extracción de conocimiento.
Una posible conceptualización es utilizar
{Trillizos de sujeto-acción-objetivo}~\cite{suilan2018}.
Esta estructura ha demostrado ser útil para representar el conocimiento en dominios específicos, como críticas de películas~\cite{suilan2018} o sentimiento.
minería~\cite{emotinet} y en {aprendizaje de ontología de dominio general}~\cite{mitchell2018never}.la cita
Además, los tripletes Sujeto-Acción-Destino extraídos automáticamente del texto se pueden vincular posteriormente a relaciones específicas de dominio mediante el uso de redes semánticas.
Como ejemplo, el sistema \textit{SemRep}~\cite{semrep} extrae tripletes Sujeto-Predicado-Objeto de textos naturales de eSalud. Los predicados están vinculados a relaciones específicas en la red semántica UMLS~\cite{umls}.

Un trabajo reciente en el desarrollo de teleologías~\cite{teleologies} sugiere que los tripletes Acción-Sujeto-Objetivo pueden ser la base para conceptualizaciones de propósito general en muchos dominios diferentes, ya que este triplete permite la captura de interacciones entre objetos a través de las acciones que realizan en El uno al otro.
Un pequeño conjunto de relaciones semánticas, como \textit{hiponomía} y \textit{holonomía} pueden proporcionar una estructura semántica adicional a la RAM.
representación. Estas relaciones ``generales'' son comunes en la mayoría de las bases de conocimiento, independientemente del dominio, como WordNet~\cite{miller1998wordnet},
DBPedia~\cite{lehmann2015dbpedia} y ConceptNet~\cite{conceptnet}.
Otras posibles conceptualizaciones permiten capturar la semántica del lenguaje natural, como la Representación de significado abstracto ~ (AMR)~\cite{amr}. A pesar del poder de representación superior de AMR sobre estructuras simples como los tripletes Acción-Sujeto-Objetivo {y semántica básica
relaciones}, el proceso de anotación para la resistencia a los antimicrobianos es considerablemente más complejo tanto para humanos como para técnicas automatizadas.

La construcción de corpus anotados con la estructura Acción-Sujeto-Objetivo es el primer paso hacia el diseño de sistemas que puedan extraer automáticamente estas anotaciones. Existen varios corpus en la literatura, anotados con una variedad de esquemas diferentes, como CLEF~\cite{kelly2016overview}, Yago~\cite{fabian2007yago} y Emotinet~\cite{emotinet}.
Sin embargo, la mayoría de estos recursos están anotados con conceptualizaciones específicas de dominio que son difíciles de extender a diferentes dominios de conocimiento.

\subsection{General-purpose annotation models}\label{sec:general}

Se han desarrollado varios modelos de anotación semántica de propósito general, que intentan representar la semántica de una oración más allá de la estructura sintáctica.
Estos modelos se basan libremente en la estructura gramatical Sujeto-Verbo-Objeto que es omnipresente en el lenguaje humano.

\paragraph{PropBank}

PropBank~\cite{propbank} propone un esquema de anotación de propósito general, basado en predicados de anotación (verbos) como los principales constituyentes semánticos de una oración. El esquema de anotación de ProbBank es capaz de representar varias relaciones semánticas, incluido el agente que causa una acción, el receptor de los efectos de una acción, modificadores de tiempo y ubicación y relaciones causales.
Una característica clave de PropBank es que cada predicado define roles semánticos personalizados, es decir, el predicado `` \textit{accept} '' define roles para el agente que acepta ~ (\texttt{ARG0}), el objeto que se acepta ~ ( \texttt{ARG1}) y el agente de quien se acepta ese objeto.

Nuestro esquema de anotaciones se desarrolló a partir de un consenso entre los grupos de investigación de BBN, MITRE, la Universidad de Nueva York y Penn. Se acordó que la primera fase de la anotación se centraría en los predicados verbales, dejando a un lado los adjetivos, los sustantivos deverbales y los predicados nominativos para una etapa posterior.
Se eligieron etiquetas de argumentos que pudieran mapearse fácilmente en las etiquetas utilizadas en la mayoría de las teorías modernas de estructura de argumentos sin estar especialmente en deuda con ninguna teoría en particular.
Por tanto, los argumentos se numeran como Arg0, Arg1, Arg2, etc., según la valencia del verbo en cuestión. El significado de cada etiqueta de argumento se define en relación con cada verbo en un léxico de 'Archivos de marcos'. Cada conjunto de etiquetas de argumento y sus definiciones se denomina conjunto de marcos y proporciona un identificador único para el sentido del verbo, un significado para ese sentido del verbo y el conjunto de argumentos esperados que dan tanto los números Arg como una etiqueta o descripción mnemotécnica para ese Arg.
A continuación de las definiciones hay una serie de oraciones de ejemplo que demuestran varias realizaciones sintácticas para ese conjunto de marcos.

El material de capacitación para el reconocimiento de propuestas, PropBank, está siendo anotado en inglés, basado en un consenso desarrollado en 2000 entre grupos de investigación de BBN, MITRE, New York University y Penn. Tomando como punto de partida el Corpus del Penn Treebank II Wall Street Journal de un millón de palabras (Marcus 1994), estamos agregando una anotación de estructura de argumento predicado. Aproximadamente una cuarta parte del TreeBank, que comprende en gran parte textos de informes financieros, ha sido extraído y sirve como nuestro enfoque inicial para la capacitación y para proporcionar una entrega más temprana de texto con anotaciones completas. Este subcorpus debe completarse en junio de 2002, mientras que el resto del corpus se completará en el verano de 2003. El proyecto actual solo anota predicados verbales, dejando de lado nominalizaciones, adjetivos y preposiciones para una fase posterior. En un artículo separado (Kingsbury, Marcus \ & Palmer, de próxima publicación) discutimos las diferencias entre PropBank y recursos similares como Verbnet, Wordnet y Framenet. Al crear anotaciones para la estructura del argumento, se usa una combinación de factores sintácticos y semánticos, aunque las claves sintácticas son las más importantes. El método general es el siguiente: para cualquier predicado dado, se hace un estudio de los usos del predicado y los usos se dividen en sentidos principales si es necesario. Estos sentidos están divididos más por motivos sintácticos que semánticos, evitando así las divisiones detalladas y a menudo arbitrarias de, por ejemplo, WordNet. Los argumentos esperados de cada sentido se numeran secuencialmente desde Arg0 a Arg5. De acuerdo con las pautas establecidas por la comunidad ACE descritas anteriormente, no se intenta hacer que las etiquetas de los argumentos tengan el mismo "significado" de un sentido de un verbo a otro, por ejemplo, el "papel" que juega Arg2 en un sentido de un Arg3 puede jugar un predicado dado en otro sentido. Por otro lado, pretendemos que los predicados que pertenecen a la misma clase VerbNet compartan 2 BBN etiquetados de manera similar ya ha completado la anotación de co-referencia de pronombre en los mismos argumentos de datos, de acuerdo con la casi sinonimia de los predicados.

\paragraph{FrameNet}

FrameNet~\cite{framenet} es una base de datos léxica y un corpus anotado que modela los roles semánticos y las relaciones en una oración en lenguaje natural a través de estructuras conceptuales llamadas \textit{frames}. Los marcos representan conceptos de propósito general, o eventos, que definen las posibles relaciones semánticas en las que esos conceptos pueden realizarse en lenguaje natural.

FrameNet se basa en una teoría del significado llamada Frame Semantics, derivada del trabajo de Charles J. Fillmore y sus colegas (Fillmore 1976, 1977, 1982, 1985, Fillmore y Baker 2001, 2010). La idea básica es sencilla: que los significados de la mayoría de las palabras pueden entenderse mejor sobre la base de un marco semántico, una descripción de un tipo de evento, relación o entidad y los participantes en él. Por ejemplo, el concepto de cocinar generalmente involucra a una persona que cocina (Cocinar), la comida que se va a cocinar (Comida), algo para sostener la comida mientras se cocina (Recipiente) y una fuente de calor (Instrumento de calentamiento). En el proyecto FrameNet, esto se representa como un marco llamado Apply \ _heat, y Cook, Food, Heating \ _instrument y Container se denominan elementos de marco (FE). Las palabras que evocan este marco, como freír, hornear, hervir y asar, se denominan unidades léxicas (LU) del marco Apply \ _heat. Otros marcos son más complejos, como Revenge, que involucra más FE (Delincuente, Lesión, Lesionado \ _Party, Avenger y Castigo) y otros son más simples, como Colocar, con solo un Agente (o Causa), una cosa que es colocado (llamado Tema) y la ubicación en la que se coloca (Objetivo).

La entrada léxica para cada LU se deriva de tales anotaciones y especifica las formas en que los FE se realizan en estructuras sintácticas encabezadas por la palabra.

Muchos sustantivos comunes, como árbol, sombrero o torre, suelen servir como dependientes que encabezan FE, en lugar de evocar claramente sus propios marcos, por lo que hemos dedicado menos esfuerzo a anotarlos, ya que la información sobre ellos está disponible en otros léxicos, como WordNet (Miller et al. 1990). Sin embargo, reconocemos que dichos sustantivos también tienen una estructura de marco mínima propia y, de hecho, la base de datos FrameNet contiene un poco más de sustantivos que de verbos.

Formalmente, las anotaciones de FrameNet son conjuntos de triples que representan las realizaciones de EF para cada oración anotada, cada una de las cuales consta de un nombre de elemento de marco (por ejemplo, Comida), una función gramatical (por ejemplo, Objeto) y un tipo de frase (por ejemplo, frase nominal ( NOTARIO PÚBLICO)). Podemos pensar en estos tres tipos de anotaciones en cada FE como "capas", pero la función gramatical y las capas de tipo de frase no se muestran en el sistema de informes basado en la web, para evitar el desorden visual. La versión XML descargable de los datos incluye estas tres capas (y varias más que no se comentan aquí) para todas las oraciones anotadas, junto con descripciones completas de marco y FE, relaciones marco-marco y entradas léxicas para cada LU anotado. La mayoría de las anotaciones son de oraciones separadas anotadas para una sola LU, pero también hay una colección de textos en los que se han anotado todas las palabras que evocan marcos; los marcos superpuestos proporcionan una rica representación de gran parte del significado de todo el texto. El equipo de FrameNet ha definido más de 1.000 marcos semánticos y los ha vinculado mediante un sistema de relaciones de marcos, que relacionan marcos más generales con marcos más específicos y proporcionan una base para razonar sobre eventos y acciones intencionales.

Debido a que los marcos son básicamente semánticos, a menudo son similares en todos los idiomas; por ejemplo, los marcos sobre compra y venta involucran al comprador, vendedor, bienes y dinero de FE, independientemente del idioma en el que se expresen. Se están llevando a cabo varios proyectos para construir FrameNets paralelos al proyecto FrameNet en inglés para idiomas de todo el mundo, incluidos el español, alemán, chino y japonés, y se han llevado a cabo análisis y anotaciones semánticas de marcos en áreas especializadas, desde la terminología legal hasta el fútbol y el turismo.

\paragraph{VerbNet}

VerbNet~\cite{verbnet} es un léxico verbal que también define roles semánticos específicos para cada verbo. En VerbNet, los verbos se organizan en una jerarquía y se vinculan a través de diferentes roles temáticos, como agentes, causa, fuente o tema. Estos elementos permiten captar la representación semántica de oraciones.
Los roles semánticos de PropBank son similares a los roles temáticos definidos en VerbNet y los elementos de marco en FrameNet. Como tal, existen recursos que vinculan estas estructuras semánticas~\cite{semlink}.

VerbNet (VN) (Kipper-Schuler 2006) es la red en línea más grande de verbos en inglés que vincula sus patrones sintácticos y semánticos. Es un léxico jerárquico, independiente del dominio y de amplia cobertura del verbo con asignaciones a otros recursos léxicos, como WordNet (Miller, 1990; Fellbaum, 1998), PropBank (Kingsbury y Palmer, 2002) y FrameNet (Baker et al. , 1998). VerbNet está organizado en clases de verbos que amplían las clases de Levin (1993) mediante el refinamiento y la adición de subclases para lograr coherencia sintáctica y semántica entre los miembros de una clase. Cada clase de verbo en VN está completamente descrita por roles temáticos, preferencias de selección de los argumentos y marcos que consisten en una descripción sintáctica y una representación semántica con estructura de subeventos modelada en el Modelo de Evento Dinámico de Pustejovsky y Moszkowicz (2011) y Pustejovsky (2013) .

Cada clase VN contiene un conjunto de descripciones sintácticas, o marcos sintácticos, que describen las posibles realizaciones superficiales de la estructura del argumento para construcciones como transitivo, intransitivo, frases preposicionales, resultantes y un gran conjunto de alternancias de diátesis. Las restricciones semánticas (como animada, humana, organización) se utilizan para restringir los tipos de roles temáticos permitidos por los argumentos, y se pueden imponer restricciones adicionales para indicar la naturaleza sintáctica del constituyente que probablemente esté asociado con el rol temático. Los marcos sintácticos también pueden estar restringidos en términos de qué preposiciones están permitidas. Cada cuadro está asociado con información semántica explícita, expresada como una conjunción de predicados semánticos booleanos como "movimiento", "contacto" o "causa". Cada predicado semántico está asociado con una variable de evento E que permite a los predicados especificar cuándo en el evento el predicado es verdadero (inicio (E) para la etapa preparatoria, durante (E) para la etapa de culminación y final (E) para la etapa consecuente ). La Figura 1. muestra una entrada completa para un marco en la clase VerbNet Hit-18.1.

VerbNet se ha integrado recientemente con 57 nuevas clases de la extensión propuesta de Korhonen y Briscoe (2004) (K \ & B) a la clasificación original de Levin (Kipper et al., 2006). Este trabajo ha implicado asociar descripciones sintáctico-semánticas detalladas a las clases K \ & B, así como organizarlas apropiadamente en la taxonomía VN existente. También se ha incorporado a VN un conjunto adicional de 53 clases nuevas de Korhonen y Ryant (2005) (K \ & R). El resultado es un recurso de libre acceso que constituye la clasificación de verbos al estilo Levin más completa y versátil para inglés. Después de las dos extensiones, VN ahora también ha aumentado nuestra cobertura de tokens de PropBank (Palmer et. Al., 2005) del 78,45 \% al 90,86 \%, lo que hace factible la creación de un corpus de capacitación sustancial anotado con etiquetas de roles temáticos de VN y membresía de clase. asignaciones, que se publicará en 2007. Esto permitirá finalmente la experimentación a gran escala sobre la utilidad de las clases basadas en sintaxis para mejorar el rendimiento de analizadores sintácticos y etiquetadoras de roles semánticos en nuevos dominios.

A cada argumento verbal se le asigna un rol temático (generalmente único) dentro de la clase. Algunas excepciones a esta unicidad son las clases que contienen verbos con argumentos simétricos, como la clase Chitchat-37.6 o la clase ContiguousLocation-47.8. Estas clases tienen roles indexados como Actor1 y Actor2, como se explicó anteriormente.

\paragraph{AMR}

Una propuesta más reciente es Representación de significado abstracto ~ \ cite [ARM] {amr}. AMR constituye un esquema de representación semántica para oraciones en inglés que también intenta cubrir una amplia gama de relaciones semánticas con un modelo de propósito general.
AMR incluye roles semánticos de PropBank, así como resolución de correferencia dentro de la misma oración, entidades y tipos nombrados, negación y otros modificadores en una estructura gráfica que representa el significado de una oración en lenguaje natural.
Sin embargo, a pesar de que AMR captura el significado semántico completo de una oración, para el propósito del descubrimiento del conocimiento, sigue siendo considerablemente abstracto, y es necesario un procesamiento adicional para extraer estructuras concretas de conocimiento~\cite{rao2017biomedical}.

Nuestros principios básicos son:
Los AMR son gráficos enraizados y etiquetados que son fáciles de leer para las personas y fáciles de recorrer para los programas.
AMR tiene como objetivo abstraerse de las idiosincrasias sintácticas. Intentamos asignar el
mismo AMR a oraciones que tienen el mismo significado básico. Por ejemplo, a las frases “la describió como un genio”, “su descripción de ella: genio” y “ella era un genio, según su descripción” se les asigna el mismo AMR.
AMR hace un uso extensivo de los conjuntos de marcos PropBank (Kingsbury y Palmer, 2002; Palmer et al., 2005). Por ejemplo, representamos una frase como "inversor en bonos" utilizando el marco "invertir-01", aunque no aparecen verbos en la frase.
AMR es agnóstico sobre cómo podríamos querer derivar significados de cadenas, o viceversa. Al traducir oraciones a AMR, no dictamos una secuencia particular de aplicaciones de reglas ni proporcionamos alineaciones que reflejen dichas secuencias de reglas. Esto hace que el sembanking sea muy rápido y permite a los investigadores explorar sus propias ideas sobre cómo se relacionan las cadenas con los significados.
AMR está fuertemente sesgado hacia el inglés. No es un interlingua.

El modelo de anotación propuesto en esta investigación comparte similitudes con los modelos de anotación semántica de propósito general como AMR y PropBank.
En contraste con estos recursos, nuestro modelo no distingue entre diferentes tipos de acciones, que están vagamente relacionadas con los verbos, como se explica en la Sección~\ref{sec: model}. En cambio, definimos dos roles de propósito general, el agente que realiza una acción y el receptor de los efectos de la acción. Estos roles corresponden aproximadamente a \texttt{ARG0} y \texttt{ARG1} respectivamente en PropBank, aunque en casos específicos su significado semántico puede diferir.
Esta simplificación está dirigida a permitir la automatización del proceso de anotación con el uso de técnicas de aprendizaje automático.
Otra diferencia clave de nuestro modelo es la inclusión de relaciones taxonómicas de propósito general ~ (por ejemplo, \textit{hipernomía} / \textit{hiponomía} y \textit{merónimo} / \textit{holónimo}) que se infieren de la oración. Estas relaciones están dirigidas a facilitar la construcción automática de bases de conocimiento.

\subsection{Annotations models in the health domain}\label{sec:health}

Las tareas de descubrimiento de conocimientos en el ámbito de la salud suelen estar respaldadas por la construcción de corpus anotados manualmente.
Se han desarrollado varios modelos de anotaciones para tareas específicas con este fin. Un ejemplo es el {DrugSemantics} corpus~\cite{moreno2017drugsemantics} donde se anotan las características del producto, y {BARR2}~\cite{barr2} que se ocupa de las abreviaturas biomédicas.
Muchos corpus incluyen tipos específicos de entidades nombradas relevantes para el dominio médico, como {DDI}~\cite{ddi} que anota medicamentos y otras sustancias.
Otros ejemplos incluyen {i2b2}~\cite{i2b2} que anota medicamentos, dosis y otros detalles de la administración de medicamentos y {CLEF}~\cite{clef} que anota diferentes tipos de condiciones, dispositivos y sus resultados en casos clínicos específicos.
Dada la especificidad de los conceptos anotados, la mayoría de estos recursos son construidos por expertos en biomedicina.

Los ejemplos anteriores son corpus útiles para diseñar técnicas orientadas a tareas limitadas,
donde el modelo de anotación está diseñado específicamente para considerar solo partes del texto relevantes para los conceptos de intereses (es decir, entidades médicas, genes, etc.).
Un enfoque alternativo que intenta modelar una amplia gama de semánticas de un documento es {Bio-AMR}~\cite{bioamr}.
Este corpus contiene oraciones relacionadas con la salud anotadas con su estructura AMR, una representación semántica de propósito general del texto natural.
Otro recurso relevante es BioFrameNet~\cite{bioframenet}, una extensión de FrameNet con roles semánticos específicos para el dominio biomédico.
Una consecuencia positiva del uso de anotaciones semánticas de propósito general es que no necesariamente requiere expertos en áreas biomédicas para participar en el proceso de anotación.

El {eHealth-KD} corpus~\cite{martinez2018overview} intenta alcanzar un término medio al representar una amplia gama de conocimientos con un modelo de anotación simple basado en tripletes Sujeto-Acción-Objetivo y 4 relaciones semánticas adicionales.
Sin embargo, después del proceso de anotación se identificaron varias deficiencias.
Un ejemplo es la necesidad de incluir {causalidad} y {vinculación} como relaciones explícitas, en lugar de representarlas a través de acciones, dada la importancia de este tipo de afirmaciones en los textos médicos.
Asimismo, la anotación carece de la capacidad de representar correferencias (`` \textit{esto} '', `` \textit{eso} '') y, por esta razón, muchas oraciones no se pueden anotar completamente.
Además, las construcciones lingüísticas complejas que representan conceptos compuestos (por ejemplo, `` \textit{los pacientes que recibieron tratamiento} '') son difíciles de anotar, especialmente cuando participan en otras relaciones.
Este artículo amplía el modelo de anotación utilizado por el corpus eHealth-KD con elementos semánticos utilizados en modelos de anotación de propósito general, como AMR y PropBank.
Esta extensión permite resolver los problemas antes mencionados y aumenta su poder de representación sin agregar un conjunto demasiado complejo de nuevos roles y relaciones semánticas.

\section{Herramientas de Anotación}

Un elemento importante a considerar en la investigación del Descubrimiento del Conocimiento es la existencia de recursos e infraestructura computacionales que apoyan el desarrollo de nuevos enfoques.
La creación de recursos lingüísticos a menudo surge de un proceso de anotación manual por parte de expertos humanos, que requiere herramientas computacionales para la anotación real, así como mecanismos para fusionar anotaciones y acuerdo de computación, idealmente en un entorno colaborativo.
Una vez que se crean los recursos, es necesario distribuir el corpus, las líneas de base y las herramientas correspondientes entre la comunidad de investigación, a menudo a través de plataformas de intercambio de código fuente en línea.

En~\citet{annotation-tools} se proporciona un análisis extenso y una comparación de varias herramientas de anotación.
La tabla~\ref{tab:annotation-tools} resume las principales características que consideramos relevantes para esta investigación e identifica la herramienta de anotación más apropiada entre un subconjunto de alternativas populares.
Consideramos como requisitos herramientas de anotación de código abierto basadas en la web que permiten anotaciones de tramo de etiquetas múltiples, así como anotaciones de relación. El soporte para la anotación colaborativa, al menos parcialmente, también es muy deseable.
De las herramientas analizadas, identificamos Brat~\cite{brat} y WebAnno~\cite{webanno}, ya que cumplen con todos los requisitos antes mencionados. En nuestra investigación, preferimos Brat a WebAnno porque, aunque WebAnno ofrece más funciones, Brat permite una configuración más sencilla. No solo es más rápido iniciar un proyecto de anotación con esta herramienta, sino también capacitar a los anotadores para que utilicen su interfaz.

\begin{table}[htb]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{r|cccccccccccc}
        \textbf{Characteristics} & \rotatebox{90}{\textbf{GATE Teamware}} & \rotatebox{90}{\textbf{Knowtator}} & \rotatebox{90}{\textbf{WebAnno}} & \rotatebox{90}{\textbf{Brat}} & \rotatebox{90}{\textbf{BioQRator}} & \rotatebox{90}{\textbf{CATMA}} & \rotatebox{90}{\textbf{prodigy}} & \rotatebox{90}{\textbf{TextAE}} & \rotatebox{90}{\textbf{LightTag}} & \rotatebox{90}{\textbf{Djangology}} & \rotatebox{90}{\textbf{MyMiner}} & \rotatebox{90}{\textbf{WAT-SL}} \\ \midrule
        multi-label annotations &     &     & \ok & \ok &     & \ok &     &     & \ok & \ok &     &     \\ % F1
        relation annotations    &     & \ok & \ok & \ok & \ok &     &     & \ok & \ok &     & \ap &     \\ % F3
        allows custom model    & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok \\ %
        collaborative interface & \ok &     & \ap & \ap & \ap & \ap & \ap &     & \ok & \ok &     & \ap \\ % F10
        web-based interface     & \ok &     & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok \\ %
        can be self-hosted      & \ok & \ok & \ok & \ok &     & \ok & \ok & \ok &     & \ok &     & \ok \\ %
        open source license     & \ok & \ok & \ok & \ok &     & \ok &     & \ok &     & \ok & \ok & \ok \\ % T2
        citation                &\cite{gate}&\cite{knowtator}&\cite{webanno}&\cite{brat}&\cite{bioqrator}&\cite{catma}&\cite{prodigy}&\cite{textae}&\cite{lighttag}&\cite{djangology}&\cite{myminer}&\cite{watsl}\\
        \bottomrule
    \end{tabular}}
    \caption{Qualitative comparison of popular annotation tools. Adapted from Table 3 in~\citet{annotation-tools}, Table~3. A symbol~\ap~indicates that the corresponding feature is only partially supported.}
    \label{tab:annotation-tools}
\end{table}

\subsection{Descripción de las herramientas de anotación}

\begin{description}
\item[GATE Teamware] es una herramienta de anotación de texto de código abierto y una metodología para la implementación y el apoyo de proyectos de anotación complejos. Tiene una arquitectura basada en web, donde una serie de servicios web (por ejemplo, almacenamiento de documentos, anotación automática) están disponibles a través de HTTPS y los usuarios interactúan con las interfaces de anotación de texto a través de un navegador web estándar.

GATE Teamware se basa en GATE (Cunningham et al. 2011b), una plataforma de PNL de código abierto ampliable, robusta y ampliamente utilizada. GATE viene con numerosos componentes de procesamiento de texto reutilizables para muchos lenguajes naturales, junto con un entorno de desarrollo gráfico de PNL e interfaces de usuario para la visualización y edición de anotaciones lingüísticas, árboles de análisis, cadenas de co-referencia y ontologías. Sin embargo, GATE Teamware fue creado específicamente para ser utilizado por anotadores no expertos, así como para permitir proyectos de anotación de corpus metodológicamente sólidos, eficientes y rentables en la web.

Además de sus usos de investigación, GATE Teamware también se ha probado como un marco para servicios de anotación comercial rentables, suministrados como unidades internas o como actividades especializadas subcontratadas. Se han llevado a cabo varios proyectos de anotación de prueba en los dominios de la bioinformática y la inteligencia empresarial, con una formación mínima y produciendo corpus de alta calidad. Por ejemplo, Meurs et al. (2011) aplican GATE Teamware a la tarea de construir una base de datos de enzimas fúngicas para la investigación de biocombustibles. Sus resultados muestran que el uso de GATE Teamware para la preanotación automática y la corrección manual aumenta la velocidad con la que se pueden procesar los documentos para su inclusión en la base de datos en un factor de alrededor del 50 \%.

De manera similar a otro software del lado del servidor, la instalación de GATE Teamware es una tarea especializada, no trivial con costos asociados, en términos de tiempo significativo y experiencia del personal requerido. Para reducir esta barrera y proporcionar cero costos de inicio, hemos puesto a disposición máquinas virtuales GATE Teamware basadas en la nube, Footnote3, que se pueden encender y apagar según sea necesario. Además, la integración de GATECloud.net (Tablan et al. 2013) facilita la elección de un conjunto de documentos anotados automáticamente y enviarlos a una instancia de GATE Teamware. También hay una distribución de máquina virtual que se puede descargar y ejecutar localmente.

\item[Knowtator]

En Knowtator, un esquema de anotación se define con definiciones de clases, instancias, ranuras y facetas de Protégé utilizando la función de edición de la base de conocimientos de Protégé. El esquema de anotación definido se puede aplicar a una tarea de anotación de texto sin tener que escribir ninguna tarea.
software específico o editar archivos de configuración especializados. Los esquemas de anotación en Knowtator pueden modelar fenómenos sintácticos (por ejemplo, análisis sintácticos superficiales) y semánticos (por ejemplo, interacciones proteína-proteína).

Knowtator aborda la definición de un esquema de anotación como una tarea de ingeniería del conocimiento aprovechando las fortalezas de Protégé como editor de una base de conocimiento. Protégé tiene componentes de interfaz de usuario para definir marcos de clase, instancia, ranura y faceta.
Un esquema de anotación de Knowtator se crea definiendo marcos utilizando estos componentes de interfaz de usuario como lo haría un ingeniero de conocimiento al crear un modelo conceptual de algún dominio. Para Knowtator, las definiciones de marcos modelan los fenómenos que la tarea de anotación busca capturar.

Como ejemplo simple, la tarea de anotación de co-referencia que viene con Callisto se puede modelar en Protégé con dos definiciones de clase llamadas marcables y encadenadas. La clase de cadena tiene dos referencias de ranuras y primary\_reference que están restringidas por facetas para tener valores de tipo marcables. Este esquema de anotación simple ahora se puede usar para anotar fenómenos de co-referencia que ocurren en el texto usando Knowtator. Anotaciones en Knowtator creadas con este sencillo esquema de anotaciones.

Una fortaleza clave de Knowtator es su capacidad para relacionar anotaciones entre sí a través de las definiciones de ranuras de las clases anotadas correspondientes. En el ejemplo de co-referencia, las referencias de espacio de la cadena de clases relacionan las anotaciones marcables para las extensiones de texto "el gato" y "Él" con la anotación de la cadena. Las restricciones de las ranuras garantizan que las relaciones entre las anotaciones sean coherentes.

Protégé es capaz de representar modelos conceptuales mucho más sofisticados y complejos que pueden ser utilizados, a su vez, por Knowtator para la anotación de texto. Además, debido a que Protégé se usa a menudo para crear modelos conceptuales de dominios relacionados con disciplinas biomédicas, Knowtator es especialmente adecuado para capturar entidades con nombre y relaciones entre entidades con nombre para esos dominios.

\item[WebAnno]

WebAnno es la tercera versión importante de la herramienta de anotación basada en web WebAnno (Yimam et al., 2013; Yimam et al., 2014) que presenta nuevas funcionalidades que permiten la anotación de estructuras semánticas:

1. Las características de ranura permiten el modelado apropiado de estructuras de predicado-argumento para SRL. También admitimos los siguientes tipos de anotaciones semánticas adicionales: participantes y circunstancias para la anotación de eventos, relaciones n-arias para la extracción de relaciones y tareas de relleno de espacios para la extracción de información.

2. Las restricciones ayudan a los anotadores al realizar un filtrado sensible al contexto de los ricos conjuntos de etiquetas semánticas. Por ejemplo, el sentido de un predicado semántico determina los roles de argumento disponibles. Este filtrado es necesario para evitar perder un tiempo valioso al hacer que los anotadores busquen en una gran cantidad de etiquetas o que ingresen etiquetas manualmente. Las reglas de restricción se pueden definir manualmente o se pueden generar automáticamente, p. Ej. a partir de recursos léxicos legibles por máquina. Hasta donde sabemos, no existe ninguna otra herramienta de anotación basada en la web que ofrezca una funcionalidad comparable.

3. Una interfaz de anotación mejorada para un proceso de anotación optimizado utilizando una barra lateral permanentemente visible en lugar de un cuadro de diálogo emergente para editar anotaciones y sus características.

Estas nuevas funcionalidades se integran bien con las funcionalidades existentes en WebAnno 2, en particular su soporte para la anotación de estructuras sintácticas, permitiendo así la anotación semántica en coordinación con la anotación sintáctica. Hasta donde sabemos, WebAnno 3 es actualmente la única herramienta de anotación basada en la web y orientada a equipos que admite tanto la anotación de estructuras semánticas como sintácticas.

WebAnno 3 se desarrolló e implementó en estrecha coordinación con los usuarios en el contexto de un proyecto de anotación (cf. Mujdricza-Maydt et al. (2016)) para la desambiguación del sentido de las palabras (WSD) y SRL en textos alemanes e impulsado por sus requisitos prácticos. SRL es la tarea de identificar predicados semánticos, sus argumentos y asignar roles a estos argumentos. Es una tarea difícil que suelen realizar los expertos.
Ejemplos de esquemas de SRL bien conocidos motivados por diferentes teorías lingüísticas son FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) y VerbNet (Kipper Schuler, 2005). La anotación de SRL se basa típicamente en estructuras sintácticas obtenidas de bancos de árboles, como el Penn Treebank basado en constituyentes (para la anotación de PropBank), o el banco de árboles TIGER alemán para la anotación de estilo FrameNet (Burchardt et al., 2009). Un argumento se identifica típicamente por la extensión de su cabeza sintáctica o constituyente sintáctico. Para algunos esquemas de anotaciones (por ejemplo, FrameNet), la tarea también incluye WSD. En este caso, la etiqueta de sentido suele determinar los espacios de argumentos disponibles. El siguiente ejemplo muestra una anotación usando FrameNet; el predicado ask recibe la etiqueta de marco Cuestionando (correspondiente a su sentido de la palabra) y sus argumentos se anotan como Destinatario, Hablante, Mensaje e Iteraciones

\item[Brat]

BRAT se basa en nuestro visualizador de anotaciones de texto STAV de código abierto publicado anteriormente (Stenetorp et al., 2011b), que fue diseñado para ayudar a los usuarios a comprender las anotaciones complejas que involucran una gran cantidad de tipos semánticos diferentes, anotaciones de texto densas, parcialmente superpuestas y conjuntos no proyectivos de conexiones entre anotaciones. Ambas herramientas comparten un componente de visualización basado en gráficos vectoriales, que proporcionan detalles y renderización escalables. BRAT integra la funcionalidad de exportación de formato de imagen PDF y EPS para admitir el uso en, por ejemplo, cifras en publicaciones (Figura 1).

Ampliamos las capacidades de STAV implementando soporte para la edición de anotaciones. Esto se hizo agregando funcionalidad para reconocer gestos de interfaz de usuario estándar que son familiares de los editores de texto, software de presentación y muchas otras herramientas.
En BRAT, un tramo de texto se marca para anotación simplemente seleccionándolo con el mouse, “arrastrando” o haciendo doble clic en una palabra. De manera similar, las anotaciones se vinculan haciendo clic con el mouse en una anotación y arrastrando una conexión a la otra.

BRAT se basa en un navegador y se construye en su totalidad utilizando tecnologías web estándar. Por lo tanto, ofrece un entorno familiar para los anotadores y es posible comenzar a usar BRAT simplemente apuntando a una instalación con un navegador moderno que cumpla con los estándares. Por lo tanto, no es necesario instalar o distribuir ningún software de anotación adicional ni utilizar complementos del navegador. El uso de estándares web también hace posible que BRAT identifique de forma única cualquier anotación mediante los identificadores uniformes de recursos (URI), lo que permite vincular anotaciones individuales para discusiones en correo electrónico, documentos y páginas web, lo que facilita la comunicación con respecto a las anotaciones.

BRAT es completamente configurable y se puede configurar para admitir la mayoría de las tareas de anotación de texto. La primitiva de anotación más básica identifica un intervalo de texto y le asigna un tipo (o etiqueta o etiqueta), marcando p. Ej. Tokens, fragmentos o menciones de entidad etiquetados en POS (Figura 1 arriba). Estas anotaciones base se pueden conectar mediante relaciones binarias, dirigidas o no dirigidas, que se pueden configurar para, por ejemplo, extracción de relación simple, o anotación de marco de verbo (Figura 1 en el medio y en la parte inferior). También se admiten asociaciones n-arias de anotaciones, lo que permite la anotación de estructuras de eventos como las dirigidas en el MUC (Sundheim, 1996), ACE (Doddington et al., 2004) y BioNLP (Kim et al., 2011). Tareas de extracción (IE) (Figura 2). Los aspectos adicionales de las anotaciones se pueden marcar utilizando atributos, banderas binarias o de valores múltiples que se pueden agregar a otras anotaciones.

Finalmente, los anotadores pueden adjuntar notas de texto de forma libre a cualquier anotación. Además de las tareas de extracción de información, estas primitivas de anotación permiten configurar BRAT para su uso en varias otras tareas, como fragmentación (Abney, 1991), etiquetado de roles semánticos (Gildea y Jurafsky, 2002; Carreras y Márquez, 2005) y dependencia anotación (Nivre, 2003) (consulte la Figura 1 para ver ejemplos). Además, tanto el cliente BRAT como el servidor implementan soporte completo para el estándar Unicode, lo que permite que la herramienta admita la anotación de texto usando p. Ej. Caracteres chinos o devanagar ¯ ¯ı. BRAT se distribuye con ejemplos de más de 20 corpus para una variedad de tareas, que involucran textos en siete idiomas diferentes e incluyen ejemplos de corpus como los presentados para las tareas compartidas de CoNLL sobre el reconocimiento de entidades con nombre independientes del idioma (Tjong Kim Sang y De Meulder, 2003) y análisis de dependencia multilingüe (Buchholz y Marsi, 2006). BRAT también implementa un sistema completamente configurable para verificar restricciones detalladas en la semántica de la anotación, por ejemplo, especificando que un evento TRANSFER debe tomar exactamente uno de cada uno de los argumentos DADOR, RECIPIENTE y BENEFICIARIO, cada uno de los cuales debe tener uno de los tipos PERSON, ORGANIZATION o GEO -ENTIDAD POLÍTICA, así como un argumento DINERO de tipo DINERO, y opcionalmente puede tomar un argumento LUGAR de tipo UBICACIÓN (LDC, 2005). La verificación de restricciones está completamente integrada en la interfaz de anotaciones y la retroalimentación es inmediata, con efectos visuales claros que marcan las anotaciones incompletas o erróneas (Figura 3).

BRAT admite dos enfoques estándar para integrar los resultados de las herramientas de anotación completamente automáticas en un flujo de trabajo de anotación: las importaciones de anotaciones masivas se pueden realizar mediante herramientas de conversión de formato distribuidas con BRAT para muchos formatos estándar (como BIO en línea y con formato de columna), y herramientas que Proporcionar interfaces de servicios web estándar que se pueden configurar para que se invoquen desde la interfaz de usuario; sin embargo, los juicios humanos no se pueden reemplazar o basar en un análisis completamente automático sin algún riesgo de introducir sesgos y reducir la calidad de las anotaciones. Para abordar este problema, hemos estado estudiando formas de aumentar el proceso de anotación con información de métodos estadísticos y de aprendizaje automático para respaldar el proceso de anotación y, al mismo tiempo, implicar el juicio del anotador humano para cada anotación. Como una realización específica basada en este enfoque, hemos integrado un sistema de desambiguación de clases semánticas basado en el aprendizaje automático introducido recientemente, capaz de ofrecer múltiples salidas con estimaciones de probabilidad que se demostró que puede reducir la ambigüedad en promedio en más del 75 \% mientras conserva el clase correcta en una media del 99 \% de los casos en seis cuerpos (Stenetorp et al., 2011a). La sección 4 presenta una evaluación de la contribución de este componente a la productividad del anotador.

BRAT implementa un conjunto completo de funciones de búsqueda, lo que permite a los usuarios realizar búsquedas en columnas de documentos Figura 4: Las opciones del cuadro de diálogo de búsqueda de BRAT para anotaciones de texto, relaciones, estructuras de eventos o simplemente texto, con un rico conjunto de opciones de búsqueda definibles mediante un simple punto y -hacer clic
interfaz (Figura 4). Además, los resultados de la búsqueda pueden mostrarse opcionalmente usando la concordancia de palabras clave en el contexto y ordenados para navegar
utilizando cualquier aspecto de la anotación coincidente (por ejemplo, tipo, texto o contexto).

\item[BioQRator]

BioQRator (17) es una interfaz de usuario de propósito general para anotar bioentidades y relaciones. Se utilizan mensajes de red simples y mínimos para comunicarse entre BioQRator y los recursos de minería de texto. Esto le permite a uno crear fácilmente una interfaz personalizada para cualquier proyecto de bio-curación si la tarea involucrada es anotar entidades y / o relaciones. Un tema importante para los sistemas de conservación son los múltiples formatos diferentes que se utilizan. Para abordar este problema, adoptamos BioC (18, 19) como formato estándar de entrada y salida. Para la entrada, se pueden utilizar documentos con formato BioC o resúmenes de PubMed. Para la salida, los documentos anotados también se pueden guardar en formato BioC o CSV (valores separados por comas). Más importante aún, BioQRator proporciona una interfaz web interactiva fácil de usar. También es compatible con varios navegadores, incluidos Chrome, Firefox y Safari (parcialmente compatible con Internet Explorer).

La tarea interactiva de BioCreative (IAT) es una pista diseñada para explorar las interacciones usuario-sistema, promover el desarrollo de herramientas útiles de minería de texto y proporcionar un canal de comunicación para las comunidades de biocuración y minería de texto (20, 21). Para BioCreative IV, un equipo participante definió un tema y tareas para la evaluación de los curadores. Los voluntarios biocuradores fueron asignados a los equipos participantes en función de sus intereses en temas definidos. La tarea de BioCreative IAT es especialmente significativa para las comunidades de minería de textos porque ha habido poco esfuerzo para evaluar formalmente las herramientas de curación. Para demostrar la usabilidad de BioQRator en BioCreative IV, se definió una tarea PPI y se utilizó la búsqueda PIE (22, 23) para un recurso externo de minería de texto. Se utilizaron las bases de datos necesarias como PubMed, Entrez Gene y UniProt para proporcionar enlaces web para BioQRator.

\item[CATMA]

CATMA (Análisis y marcado de texto asistido por computadora), una herramienta desarrollada en la Universidad de Hamburgo y utilizada actualmente por más de 60 proyectos de investigación en todo el mundo. CATMA ofrece una combinación única de tres características principales que no se encuentran en ninguna otra herramienta de análisis de texto:

-CATMA admite la anotación y el análisis colaborativos: un texto o corpus de texto puede ser investigado individualmente, pero también de forma conjunta por un grupo de estudiantes o investigadores.

-CATMA apoya prácticas exploratorias no deterministas de anotación de texto: un enfoque discursivo y orientado al debate para la anotación de texto basado en las prácticas de investigación de las disciplinas hermenéuticas es el modelo conceptual subyacente.

-CATMA integra la anotación de texto y el análisis de texto en un entorno de trabajo basado en la web, lo que hace posible combinar la identificación de fenómenos textuales con su investigación de forma iterativa y sin fisuras.

Lo que distingue a CATMA de otros métodos de anotación digital es su enfoque 'no dogmático': el sistema no prescribe esquemas o reglas de anotación definidos, ni obliga al usuario a aplicar taxonomías rígidas de sí / no, correctas / incorrectas a los textos (aunque también permite esquemas más prescriptivos). Más bien, la lógica de CATMA invita a los usuarios a explorar la riqueza y las múltiples facetas de los fenómenos textuales de acuerdo con sus necesidades: los usuarios pueden crear, expandir y modificar continuamente sus propios conjuntos de etiquetas individuales, por lo que si un pasaje de texto invita a más de una interpretación, nada en el El sistema evita la asignación de anotaciones múltiples o incluso contradictorias. A pesar de toda esta flexibilidad, CATMA no produce anotaciones idiosincrásicas: todos los datos de marcado se pueden exportar en formato TEI / XML y reutilizar en otros contextos.

Dado que CATMA es una herramienta altamente intuitiva, también es adecuada para humanistas con poco conocimiento técnico: la GUI permite un inicio rápido y el generador de consultas de CATMA (un widget basado en diálogos paso a paso) ayuda a los usuarios a recuperar información compleja de textos sin tener que aprender un idioma de consulta. Otra ventaja en el lado fácil de usar es el hecho de que las funciones automatizadas de lectura a distancia de CATMA se mejoran y amplían continuamente: la versión actual 5.0 ya presenta una serie de rutinas de anotación automatizadas, entre otras, la identificación de características narrativas básicas en los textos.

\item[prodigy]

Prodigy es una herramienta de anotación moderna para crear datos de entrenamiento y evaluación para modelos de aprendizaje automático. También puede usar Prodigy para ayudarlo a inspeccionar y limpiar sus datos, realizar análisis de errores y desarrollar sistemas basados ​​en reglas para usar en combinación con sus modelos estadísticos.

La biblioteca de Python incluye una variedad de flujos de trabajo prediseñados y comandos de línea de comandos para varias tareas, y componentes bien documentados para implementar sus propios scripts de flujo de trabajo. Sus scripts pueden especificar cómo se cargan y guardan los datos, cambiar qué preguntas se hacen en la interfaz de anotaciones e incluso pueden definir HTML y JavaScript personalizados para cambiar el comportamiento de la interfaz. La aplicación web está optimizada para una anotación rápida, intuitiva y eficiente.

La misión de Prodigy es ayudarlo a hacer más de todos esos procesos manuales o semiautomáticos que todos sabemos que no hacemos lo suficiente. Para la mayoría de los científicos de datos, el consejo de dedicar más tiempo a analizar sus datos es como el consejo de usar hilo dental o dormir más: es fácil reconocer que es un buen consejo, pero no siempre es fácil de poner en práctica. Prodigy ayuda brindándole una herramienta práctica y flexible que se adapta fácilmente a su flujo de trabajo. Con pasos concretos a seguir en lugar de un objetivo vago, la anotación y la inspección de datos cambiarán de algo que debe hacer a algo que hará.

\item[TextAE]

TextAE de código abierto se desarrolla como un proyecto de código abierto. Publicado bajo la licencia MIT. Soporte Unicode Admite cualquier idioma compatible con UTF8. (Sin embargo, podríamos probar la función solo con un número limitado de idiomas. Si encuentra un problema con su idioma, háganoslo saber para que podamos solucionarlo). Instalación cero
Puede utilizar un editor TextAE listo para usar inmediatamente sin ningún proceso de instalación.
Editor de GUI con todas las funciones
Puede crear o editar varios tipos de anotaciones. anotación de entidad nombrada anotación de relación anotaciones sintácticas visor / editor predeterminado de PubAnnotation TextAE se desarrolla como un visor / editor predeterminado de PubAnnotation.
El cliente REST TextAE funciona como un cliente REST, lo que significa que puede obtener un archivo de anotaciones de la red y puede publicar un archivo de anotaciones en la red.

\item[LightTag]

Trabaje más rápido con nuestra interfaz optimizada
Atajos de teclado
Sin supuestos de tokenización
Soporte completo de Unicode
Anotaciones de subpalabras y frases
Idiomas RTL y CJK
Anotaciones de entidad, clasificación y relación

Controle la calidad de sus datos
El modo de revisión y la generación de informes de LightTag facilitan garantizar que sus datos sean perfectos y que sus anotadores estén funcionando al máximo.

Controle la calidad de sus datos
El modo de revisión y la generación de informes de LightTag facilitan garantizar que sus datos sean perfectos y que sus anotadores estén funcionando al máximo.

\item[Djangology]

La aplicación web de anotación Djangology se creó originalmente para satisfacer las necesidades de un proyecto de anotación colaborativo que involucra a más de 250 participantes internacionales. El objetivo del proyecto era crear un corpus estándar de oro que esté anotado con entidades nombradas del dominio de interés: estudios médicos de las condiciones de trauma, shock y sepsis. Resúmenes de una conferencia anual dedicada
al sujeto y alojado por la North American Shock Society 6 se utilizaron para identificar las entidades nombradas específicas del dominio a través de un proceso automatizado. Las anotaciones de entidades nombradas tuvieron que ser validadas por expertos en el dominio, los contribuyentes a la conferencia. El sistema Djangology
ha estado en uso durante dos años consecutivos (2008 y 2009), y ha logrado una tasa de respuesta media de los contribuyentes del 70 \%.

Las necesidades del proyecto llevaron a un conjunto de requisitos comunes a proyectos similares de anotación colaborativa altamente distribuida. Se necesitaba una interfaz de administración para gestionar documentos y usuarios, así como para la definición de esquemas de anotación. Las anotaciones creadas mediante un proceso automatizado debían cargarse en el sistema. Los participantes fueron notificados por correo electrónico y se les presentó un enlace a la interfaz basada en web. Después de iniciar sesión, los anotadores pudieron ver una lista de documentos asignados. Se necesitaba una interfaz de usuario intuitiva basada en la web para permitir a los participantes anotar documentos con un mínimo de texto instructivo. El acceso fácil y rápido a las anotaciones fue crucial para el éxito del proyecto. Como el tiempo de los expertos en dominios es bastante valioso, las complicadas instrucciones de instalación o anotación serían prohibitivas. El sistema también necesitaba mostrar estadísticas de acuerdos interannotadores, así como la evaluación

Djangology se puede implementar en cualquier servidor accesible desde la web y requiere una instalación de Python, una instalación de Django y conectividad a un servidor de base de datos 9. El código fuente y las instrucciones de instalación se pueden encontrar en el sitio web del proyecto http://djangology.sourceforge.net/. Estimamos que el tiempo de instalación y configuración de un extremo a otro para un desarrollador experto en Python y Django es de menos de una hora. Una vez implementada, se puede acceder a la aplicación desde cualquier navegador web; no se necesitan complementos de navegador, instalación de JVM o configuraciones de seguridad personalizadas, ya que la comunicación cliente-servidor se basa en solicitudes HTTP y Ajax estándar.

El esquema de la base de datos de la aplicación (Figura 1 (a)) y la interfaz de usuario se pueden ampliar y personalizar rápidamente. Por ejemplo, la creación de un nuevo campo para las cuentas de anotador podría lograrse sin esfuerzo simplemente agregando un nuevo atributo a la clase de modelo de Python correspondiente. El formulario web correspondiente y el esquema de la base de datos subyacente se actualizan de forma transparente mediante el marco de Django

La aplicación Djangology presenta a los administradores una interfaz para crear / modificar proyectos de anotaciones y administrar usuarios (Figura 2). Los administradores pueden importar documentos (documento único o modo por lotes) en un proyecto, definir el esquema de anotación del proyecto, crear cuentas de anotadores y asignar anotadores a proyectos específicos y a una lista de documentos. Las anotaciones y los documentos existentes también se pueden cargar fácilmente en el sistema a través de scripts Python personalizados (scripts Django independientes) o mediante una conexión directa a la base de datos Djangology. Djangology se ha utilizado para importar anotaciones creadas manualmente en formato Knowtator y desde BioScope Corpus (Szarvas et al., 2008), así como anotaciones creadas automáticamente por los marcos Gate y UIMA (Ferrucci y Lally, 2004) y el sistema Metamap c. de la Biblioteca Nacional de Medicina. En el flujo de trabajo del sistema, a los colaboradores se les suele enviar por correo electrónico la información de autenticación del sistema y se les presenta un enlace a la aplicación (Figura 3).

Una vez que hayan iniciado sesión, los anotadores pueden seleccionar uno de sus documentos asignados y continuar con la interfaz de anotación basada en la web. Una página web basada en Ajax permite a los colaboradores resaltar un fragmento de texto y asignarlo a uno de los tipos de anotaciones predefinidos (según el esquema de anotaciones del proyecto). El procedimiento para ingresar nuevas anotaciones y modificar las existentes es intuitivo y se basa en las convenciones de la interfaz de usuario: selección de texto / selección de menú del botón derecho. El sistema está diseñado específicamente para requerir una inversión mínima de tiempo por parte de los anotadores involucrados. No es necesario instalar, configurar o leer los manuales de usuario por parte de los colaboradores. Las anotaciones se guardan en la base de datos de backend a medida que se ingresan, lo que garantiza que no se pierda ningún trabajo. Para ahorrar el esfuerzo de los anotadores, una vez que se anota una frase, todas las apariciones de la frase en el documento se anotan automáticamente en el mismo tipo. Los usuarios también tienen la posibilidad de anular las anotaciones creadas automáticamente o cambiar el comportamiento predeterminado del sistema. Si lo desea, los contribuyentes también pueden marcar los documentos como completados para alertar al administrador del proyecto sobre el progreso de la anotación.

Una vez que se recopilan las anotaciones de varios contribuyentes, los administradores de proyectos tienen la capacidad de ver las estadísticas del acuerdo interannotador: una variedad de métricas basadas en proyectos y documentos por pares se calculan y presentan en la interfaz de usuario. Dado que el análisis de los desacuerdos entre los anotadores es una tarea común, también se proporciona una interfaz para una comparación lado a lado de las anotaciones de los documentos.

\item[MyMiner]

MyMiner es una aplicación web interactiva basada en un diseño modular con el propósito de ayudar a los usuarios en las tareas de biocuración y anotación de texto. La interfaz de MyMiner está diseñada para ser fácil de usar y no requiere la instalación de ningún software local. Cada módulo tiene una opción de exportación para guardar los resultados. El tiempo dedicado a procesar un documento se registra en el archivo exportado. Para mejorar la facilidad de uso, se ha adoptado y conservado un diseño de pantalla común entre los módulos de la aplicación. El área de análisis del documento de entrada se encuentra en la parte superior de la página; las opciones y herramientas se colocan debajo de la zona de selección principal. MyMiner combina PHP, JavaScript y AJAX para mejorar la interactividad del usuario. El núcleo del sistema MyMiner cubre cuatro módulos de aplicación que pueden usarse de forma independiente o combinarse siguiendo los pasos de una tubería de biocuración (Fig. S1 complementaria).

MyMiner maneja cualquier texto sin formato, incluidos resúmenes de artículos, oraciones de documentos, términos de ontología o descripciones de enfermedades.

El módulo "Etiquetado de archivos" es una interfaz de clasificación de texto manual fácil de usar que permite clasificar documentos, resúmenes, oraciones o términos, ofreciendo la posibilidad de ingresar etiquetas de clase especificadas por el usuario. Este módulo podría usarse, por ejemplo, para clasificar documentos como relevantes o no para un tema específico de una consulta de PubMed. Su propósito es cubrir la tarea de triaje (selección de artículos) que realizan los anotadores de la base de datos, pero también se puede utilizar para cualquier registro de clasificación manual. Los datos etiquetados que resultan de esta clasificación pueden servir como conjuntos de entrenamiento y prueba para sistemas de categorización de texto. Para reducir el tiempo de clasificación manual, ofrece la opción de establecer dinámicamente resaltados de texto positivos y negativos. Estas son expresiones que los usuarios pueden establecer en cualquier momento durante el proceso de etiquetado para resaltar el texto relevante (marcado en amarillo) o no relevante (marcado en rojo) para el tema de interés. El sistema ofrece la posibilidad de cargar las pautas de clasificación para que el anotador pueda consultarlas cuando sea necesario. Los usuarios pueden pausar y reanudar el proceso de conservación en cualquier momento guardando el documento clasificado. Para reanudar la clasificación, el archivo guardado se carga como archivo de entrada. Se registra el tiempo empleado por un usuario para seleccionar la etiqueta correspondiente. Esto puede resultar útil para estimar la eficiencia de los anotadores y la dificultad de la tarea.

El módulo "Comparar archivo" facilita la comparación directa de colecciones de elementos etiquetados generados por varios enfoques o personas. Además, es posible crear subconjuntos a partir de estas colecciones en función del acuerdo o desacuerdo de las etiquetas de anotación. Este módulo se puede utilizar para comparar y evaluar métodos de clasificación de documentos entre varias personas o software. Muestra un resumen global con información que cubre: (i) el número de documentos dentro de cada clase; (ii) el tiempo medio necesario para clasificar el texto; (iii) la correlación entre el tiempo de clasificación y la longitud del texto o (iv) el número de elementos etiquetados de manera diferente entre los anotadores. Este módulo permite extraer una colección de textos (conocido como corpus Gold Standard) que han sido etiquetados consistentemente por todos los anotadores. Alternativamente, el módulo también se puede utilizar para extraer los casos límite etiquetados de manera diferente. Las anotaciones apresuradas / inexactas se pueden detectar por desacuerdos entre los anotadores y / o una mala correlación entre el tamaño del documento y el tiempo de clasificación. Estos casos se pueden usar para refinar y mejorar las pautas de clasificación. El módulo Comparar archivo se ha utilizado para estimar la coherencia de las anotaciones manuales entre varios individuos y métodos (Sección 2 de datos suplementarios).

El propósito del módulo "Etiquetado de entidades" (reconocimiento de menciones de entidades) es detectar manualmente objetos conceptuales importantes dentro de un documento, un primer paso para una mayor identificación de eventos de anotación y relaciones para poblar bases de datos de conocimiento. Este módulo podría usarse para crear un corpus de menciones de genes y proteínas para probar y entrenar una herramienta de reconocimiento de entidades nombradas. Este módulo ofrece una interfaz interactiva que permite a los usuarios identificar semiautomáticamente varios tipos de entidades dentro de los documentos. Ha sido diseñado como un editor en línea WYSIWYG (What You See Is What You Get) que permite la adición de etiquetas especificadas por el usuario para nuevos tipos de entidades. Para la detección de bioentidades importantes, este módulo proporciona el reconocimiento automático de proteínas, ADN, ARN, líneas celulares y tipos de células mediante la integración del marcador ABNER (Settles, 2005). El sistema LINNAEUS se incorpora a MyMiner para identificar especies y organismos (Gerner et al., 2010). Además, las entidades definidas por el usuario se pueden detectar si se proporcionan diccionarios de términos y etiquetas. Para mejorar la precisión de las anotaciones, las etiquetas se pueden editar y las etiquetas generadas incorrectamente se pueden eliminar. Para definir relaciones simples entre entidades y términos, se agregó a este módulo una pantalla de casilla de verificación de matriz (Fig. Complementaria S14).

El módulo "Entity Linking" facilita la anotación manual de bioentidades mencionadas en un documento con identificadores estandarizados. Este módulo podría usarse para vincular manualmente artículos a identificadores de enfermedades y proteínas para crear un catálogo de proteínas involucradas en patologías. Los nombres de genes / proteínas se reconocen automáticamente y se muestran como una lista que se puede editar manualmente, y se pueden agregar nuevas entidades y eliminar las identificadas incorrectamente. Para cada nombre de gen / proteína, MyMiner sugiere una lista clasificada de identificadores UniProt que utilizan el mecanismo de puntuación de búsqueda UniProt (Arighi et al., 2011). Las menciones de especies se normalizan a los identificadores de taxón NCBI; Los identificadores OMIM están asociados a enfermedades y los términos de ontología están vinculados a identificadores de archivos de ontología enviados. Para este propósito, MyMiner lanza consultas asincrónicas a las respectivas bases de datos (UniProt, taxonomía NCBI, OMIM y archivo de ontología proporcionado por el usuario) utilizando solicitudes AJAX. Para organismos, proteínas, enfermedades y términos de ontología, se muestra una breve descripción para ayudar a validar posibles aciertos candidatos y para ayudar durante la desambiguación manual de identificadores de bases de datos potenciales. Las casillas de verificación permiten la selección de los identificadores más apropiados de la lista de candidatos. Si las especies se especifican antes de una búsqueda de identificadores de proteínas, se aplican restricciones específicas de especies para reducir el número de candidatos potenciales de UniProt.

\item[WAT-SL]

WAT-SL (Herramienta de anotación web para etiquetado de segmentos), una herramienta de anotación basada en la web de código abierto dedicada al etiquetado de segmentos.1 WAT-SL proporciona todas las funcionalidades para ejecutar y administrar proyectos de etiquetado de segmentos de manera eficiente. Su interfaz de anotación autodescriptiva solo requiere un navegador web, lo que la hace particularmente conveniente para los procesos de anotaciones remotas. La interfaz se puede adaptar fácilmente a los requisitos del proyecto utilizando tecnologías web estándar para centrarse en las etiquetas de segmento específicas en cuestión y cumplir con las expectativas de diseño de los anotadores. Al mismo tiempo, asegura que los textos a etiquetar permanezcan legibles durante todo el proceso de anotación. Este proceso se basa en el servidor y se puede interrumpir en cualquier momento. El progreso del anotador se puede monitorear constantemente, ya que todas las interacciones relevantes de los anotadores se registran en un formato de texto sin formato basado en valores clave.

WAT-SL es una herramienta de anotación basada en web lista para usar y fácilmente personalizable que se dedica al etiquetado de segmentos y que se centra en un uso fácil para todas las partes involucradas: anotadores, curadores de anotaciones y organizadores de proyectos de anotaciones. En WAT-SL, el proceso de anotación se divide en tareas, que generalmente corresponden a textos individuales. Juntas, estas tareas forman un proyecto.

Una vez que se etiqueta un segmento, su color de fondo cambia y el botón muestra una abreviatura de la etiqueta respectiva. Para ayudar a los anotadores a formar un modelo mental de la interfaz de anotaciones, los colores de fondo de los segmentos etiquetados coinciden con los colores de las etiquetas en el menú. Todas las etiquetas se guardan automáticamente, evitando cualquier pérdida de datos en caso de cortes de energía, problemas de conexión o similares.
En algunos casos, los textos pueden estar segmentados en exceso, por ejemplo, debido a una segmentación automática. Si este es el caso, WAT-SL permite a los anotadores marcar un segmento para continuar en el siguiente segmento. A continuación, la interfaz conectará visualmente estos segmentos (consulte los botones que muestran “->” en la Figura 2). Finalmente, la interfaz de anotaciones incluye un cuadro de texto para dejar comentarios a los organizadores del proyecto. Para simplificar la formulación de comentarios, cada segmento está numerado, y el número se muestra cuando se mueve el cursor del mouse sobre él.

Después de que se completa un proceso de anotación, generalmente sigue una fase de curación en la que las anotaciones de diferentes anotadores se consolidan en una. La interfaz de curación de WAT-SL permite una curación eficiente al imitar la interfaz de anotación con tres ajustes (Figura 3): Primero, los segmentos para los cuales la mayoría de los anotadores acordaron una etiqueta están preetiquetados en consecuencia. En segundo lugar, el menú muestra para cada etiqueta cuántos anotadores la eligieron. Y tercero, la descripción de la etiqueta muestra (anonimizada) qué anotador eligió la etiqueta, para que los curadores puedan interpretar cada etiqueta en su contexto. Se puede acceder a la curación bajo la misma URL que la anotación para permitir que los anotadores de algunas tareas sean curadores de otras tareas.

WAT-SL es una aplicación Java independiente de plataforma y fácil de implementar, con pocas configuraciones almacenadas en un simple archivo "clave = valor". Entre otros, los anotadores se gestionan en este archivo asignando un nombre de usuario, una contraseña y un conjunto de tareas a cada uno de ellos. Para cada tarea, el organizador de un proyecto de anotación crea un directorio (ver más abajo). WAT-SL utiliza el nombre del directorio como nombre de la tarea en todas las ocasiones. Una vez que se ejecuta el archivo Java que proporcionamos, lee todas las configuraciones e inicia un servidor. El servidor está inmediatamente listo para aceptar solicitudes de los anotadores.

\end{description}

La distribución pública de corpus anotados y recursos relacionados, por ejemplo, líneas de base, scripts de evaluación, scripts de carga y formato, etc., a menudo se habilita a través de plataformas de intercambio de código fuente abierto.
Podría decirse que las opciones más populares son Github \ footnote {\url{https://github.com}} y Gitlab \ footnote {\url{https://gitlab.com}}, que proporcionan características similares a pesar de pequeñas diferencias en su núcleo. modelos de negocio.
También es posible compartir los recursos correspondientes a través de plataformas de hospedaje institucional u otras soluciones ad-hoc. Esto podría ser conveniente en el caso de requisitos legales, licencias complejas que son incompatibles con idiosincrasias de código abierto o cualquier otra consideración que no permita el intercambio público completo.
En nuestro caso, todos los recursos están disponibles públicamente en una colección de repositorios de Gitlab \ footnote {\url{https://ehealthkd.github.io}}.

\section{Anotación Semi-Automática}

Machine learning, and specifically supervised learning, is one of the most effective tools for automating complex cognitive tasks, such as recognizing objects in images or understanding natural language text.
One of the main bottlenecks of supervised learning is the need for high-quality datasets of labeled samples on which statistical models can be trained.
These datasets are usually built by human experts in a lengthy and costly manual process.
Active learning~\cite{Cohn2010ActiveL} is an alternative paradigm to conventional supervised learning that has been proposed to reduce the costs involved in manual annotation .

The key idea underlying active learning is that a learning algorithm can perform better with less training examples if it is allowed to actively select which examples to learn from~\cite{survey}.
In the supervised learning context, this paradigm changes the role of the human expert.
In conventional supervised learning contexts, the human expert guides the learning process by providing a large dataset of labeled examples. However, in active learning the active role is shifted to the algorithm and the human expert becomes an oracle, participating in a labeling-training-query loop.
In the active paradigm,  a model is incrementally built by training on a partial collection of samples and then selecting one or more unlabeled samples to query the human oracle for labels and increase the training set.
This approach introduces the new problem of how to best select the query samples so as to maximize the model's performance while minimizing the effort of the human participant.

The simplest active learning scenario consists of  the classification of independent elements $x_i$ drawn from a pool of unlabeled samples.
Examples range from image classification~\cite{Gal2017DeepBA} to sentiment mining~\cite{Kranjc2015ActiveLF},  in which the minimal level of sampling (e.g., an image or text document) corresponds to the minimal level of decision. i.e, a single label is assigned to each $x_i$. More complex scenarios arise when the decision level is more fine-grained than the sampling level. In the domain of text mining, an interesting scenario is the task of entity and relation extraction from natural language text~\cite{zhang2012unified}.
In this scenario the sampling level is a sentence, but the minimal level of decision involves each token or pair of tokens in the sentence, and furthermore, these decisions are in general not independent within the same sentence.
In this case, it is not trivial to estimate how informative an unlabeled sample will be, since each sample has several sources of uncertainty.

This section reviews some of the most relevant research related with active learning in general, and specifically focused on entity detection and relation extraction.
One of the most important design decisions in active learning is how to intelligently select the novel unlabeled samples in the most efficient way. The underlying assumption is that we want to train a
model to the highest possible performance~(measured in precision, $F_1$, etc.) while minimizing the human cost (measured in time, number of samples manually labeled, or any other suitable metric).
This requirement is often framed as the selection of the \textit{most informative} unlabeled samples, and formalized in terms of a query strategy~\cite{survey}.
The most common query strategies for general-purpose active learning can be grouped into the following categories:

\begin{description}
\item[(i) Uncertainty sampling:] The most informative samples are considered those with the highest degree of uncertainty, given some measure of uncertainty for each sample~\cite{Lewis1994148}.

\item[(ii) Query by committee:] The most informative samples are considered those with the highest disagreement among a committee of either different models or different hypotheses from the same underlying model~\cite{seungquery}.

\item[(iii) Expected model change:] The most informative samples are considered those that produce the highest change in the model's hypothesis if they were included in the training set~\cite{NIPS2007_3252}.
In recent years, researchers in fields such as machine learning, knowledge discovery, data mining and
natural language processing, among others, have produced many approaches and techniques to
leverage the large amount of information in the Internet for a variety of tasks, from
building search~\cite{google} and recommender systems~\cite{youtube}
to improving medical diagnostics~\cite{watson}.

Among the different approaches relevant to knowledge discovery, we can recognize a
continuous spectrum of techniques, based on how much expert knowledge is used.
Heavily knowledge-based techniques are based
on rules defined in knowledge bases handcrafted by domain experts~\cite{chandrasekaran1986generic}.
These approaches have a great degree
of reliability and precision, and generally allow for more complexity in the extracted knowledge,
but are difficult to scale to large amounts of data.
In contrast, the statistical approaches consist of techniques based on pattern recognition with statistical
and probabilistic models~\cite{kevin2012machine}. These techniques scale better with large amounts of data~\cite{le2013building},
providing better recall, but often are limited to extracting simple models of knowledge,
and can be more sensitive to noisy, fake or biased information~\cite{bolukbasi2016man}.

Given these mutually complementary characteristics, several hybrid approaches have been proposed.
Recently, research areas such as ontology learning~\cite{cimiano2009ontology},
learning by reading~\cite{barker2007learning} or entity embedding~\cite{hu2015entity} have arisen.
In these areas, researchers combine techniques from machine learning, natural language
processing and knowledge representation to solve more complex problems that cannot
be dealt with using only the classical tools.

Many machine learning systems are designed to solve a domain-specific task, such as
assigning a class to an element from a predefined set of labels. These systems,
when trained with data for a particular domain, are often not applicable to other domains
or to scenarios where several different domains must be used together. Moreover,
often systems are designed to be trained once from a corpus, and don't allow for
a continuous improvement of the knowledge learned.
Recently there are attempts to build general-purpose learning systems that are always
improving while obtaining new knowledge, re-evaluating the old knowledge and refining their
own confidence~\cite{mitchell2015never}.

Additionally, it is interesting to design non-monolithic learning systems, but instead
built as a set of modular components that can be combined in different ways.
This composability would allow a continuous learning system not only to improve the
quality of the extracted knowledge, but also to learn how to tune its own internal
parameters to perform a better knowledge extraction in the future. It is conceivable
that such a system could gradually learn which types of basic processes (i.e., entity recognition, POS-tagging, etc.)
are most useful for a given domain or for a particular corpus. Likewise, such a system could
learn which types of probabilistic models provide the best results in a particular dataset.
\item[(iv) Variance and error reduction:] The most informative samples are those which produce the highest reduction in the model's generalization error or, as a proxy, its variance~\cite{roy2001toward}.
\end{description}

Expected model change (iii) and variance/error reduction (iv) strategies are heavily dependent on the specific learning model used.
In contrast, uncertainty sampling (i) and query by committee (ii) are  applicable in general with a high degree of model agnosticism.
Furthermore, relevant subsets of both strategies can be formalized under a single framework if we define the uncertainty as a measure of the entropy of the model's predicted output.
In this framework, query-by-committee can be implemented via weighted voting, thereby assigning empirical probabilities to the possible outputs.

Weighted density is a complimentary strategy in which the most informative samples are weighted by how representative they are of the input space, for example, by measuring their similarity to the remaining samples~\cite{settles2008analysis}.
This approach attempts to counter-balance a noticeable tendency to select outliers as the most informative samples ---a problem associated with other query strategies--- since outliers are often the samples that create the highest amount of uncertainty, disagreement or hypothesis change.

Recent advseplnances in natural language processing have produced an increased interest in active learning to alleviate the requirement for large annotated corpora~\cite{Olsson2009ALS, Tchoua2019ActiveLY}.
\citet{settles2008analysis} compare several strategies for active learning in sequence labeling scenarios, concluding that query strategies based on measures of sequence entropy combined with weighted sampling outperform other variants.
\citet{Meduri2020ACB} propose a comprehensive benchmark to evaluate different active learning strategies for entity matching.
In the task of named entity recognition, CRF models have been used to select query samples
\citep{Claveau2017StrategiesTS, Lin2019AlpacaTagAA}.
The task of relation extraction also benefits from active learning approaches, both in general-purpose settings~\cite{fu2013efficient} and in domain-specific settings~\cite{zhang2012unified}.
However, despite the growing body of research, it is still a challenge to apply active learning in joint entity recognition and relation extraction, especially in scenarios with low resources~\cite{Gao2019ActiveER}.

  \section{Recursos Lingüísticos}

  Different semantic relations have been established  in the state of the art, many of these giving rise to the construction of corpora. We focus on two approaches: corpora or annotation models to represent knowledge in many domains as well as those specifically about health.
  The table~\ref{tab:corpora}  presents the seven characteristics relevant to our corpus and indicates which of them are present in a sample of corpora from the state-of-the-art.
  These characteristics can be understood in the following terms:
  \begin{enumerate}
  \item \textit{general-purpose annotation:} applicability of the underlying annotation model to any domain;
  \item \textit{independence of syntax:} capturing semantic aspects rather than syntactic relations in sentences;
  \item \textit{ontological knowledge:} supporting inheritance and composition between concepts;
  \item \textit{composite concepts:} allowing the annotation of concepts that involve other sub-concepts;
  \item \textit{attributes:} modeling attributes for each annotated entity such as quantifiers~(e.g., number of occurrences) or qualifiers~(e.g., degree of certainty);
  \item \textit{contextual relations:} modeling relations that only occur when conditioned by a specific context; and,
  \item \textit{causality / entailment:} including relations for representing causality and/or entailment.
  \end{enumerate}

  \begin{table}[htb]
      \centering
      \begin{tabular}{ll|c|c|c|c|c|c|c|c}
          & \textbf{Characteristics} & \rotatebox{90}{\textbf{Ixa MedGS}~\cite{ORONOZ2015318}} & \rotatebox{90}{\textbf{DrugSemantics}~\cite{moreno2017drugsemantics}} & \rotatebox{90}{\textbf{DDI}~\cite{herrero2013ddi}} &
          \rotatebox{90}{\textbf{Bio AMR}~\cite{bioamr}} &
          \rotatebox{90}{\textbf{YAGO}~\cite{suchanek2007yago}} & \rotatebox{90}{\textbf{ConceptNet}~\cite{speer2017conceptnet}} & \rotatebox{90}{\textbf{eHealth-KD v1}~\cite{ehealth}} &
          \rotatebox{90}{\textbf{eHealth-KD v2}} \\ \midrule
          1 & general-purpose annotation &     &     &     & \ok & \ok & \ok & \ok & \ok \\
          2 & independence of syntax      & \ok & \ok & \ok &     & \ok & \ok & \ok & \ok \\
          3 & ontological knowledge      &     &     &     & \ok & \ok & \ok & \ok & \ok \\
          4 & composite concepts  &     &     &     & \ok &     &     & \ok & \ok \\
          5 & attributes        &     & \ok &     & \ok & \ok &     & \ok & \ok \\
          6 & contextual relations       &     &     &     & \ok &     &     &     & \ok \\
          7 & causality / entailment     & \ok &     &     & \ok &     & \ok &     & \ok \\
          \bottomrule
      \end{tabular}
      \caption{Comparison between the \textit{eHealth-KD v2} corpus and other corpora with respect to
      the characteristics that define our proposal.}
      \label{tab:corpora}
  \end{table}

  \begin{table}[h!]\centering
    \footnotesize{
    \begin{tabularx}{\hsize}{X|XXXXX}
        \hline
        \hline
      \textbf{Corpus} & \textbf{Drug Semantic} & \textbf{Ixa MedGS} & \textbf{CLEF} & \textbf{DDI} & {\textbf{BARR2}} \\
        \hline \\
      \textbf{Doc. Type} & Product summaries & Discharge summaries & Clinical documents & Abstracts & Clinical case studies \\
        \textbf{Annotation Type} & Manual & Auto/Manual check & Manual &  Auto/Manual check & Manual \\
        \textbf{Annotators} & Experts & Experts & Experts \& Non-experts & Expert & Experts \\
        \textbf{Schema} & Medical entities & Medical entities & Medical entities & Medical entities & Medical abbreviations \\
        \textbf{Language} & Spanish & Spanish & English & English & Spanish \\
        \textbf{Documents} & 5~(16\%) & 75~(0.01\%) & 150~(0.27\%) & 1025~(100\%) & 648(20\%) \\
        \textbf{Origin} & AEMPS & Galdacao-Usansolo Hospital & Royal Madersen Hospital & Medline, Drug Bank & PubMed, IBCECS \& SciELO \\
        \hline
        \hline
    \end{tabularx}
    } %%% CHANGE add BARR2 corpus
    \caption{Summary of related corpora annotated with domain-specific entities for the health domain.
            {Percentage values for \textbf{Documents} indicate how many of the original documents were actually annotated, as reported by the original authors.} %%%%CHANGE explain percentages
            \label{tab:stateofart}}
    \end{table}


    \begin{table}[h!]\centering
    \footnotesize{
    \begin{tabularx}{\hsize}{X|XXXX}
        \hline
        \hline
      \textbf{Corpus} & \textbf{Bio AMR} & \textbf{Yago} & \textbf{Emotinet} & \textbf{eHealth-KD} \\
        \hline \\
      \textbf{Doc. Type} & Sentences & Sentences & Posts & Sentences\\
        \textbf{Annotation Type} & Manual & Automatic & Manual & Manual \\
        \textbf{Annotators} & Non-experts & Non-experts & Non-experts & Non-experts \\
        \textbf{Schema} & AMR & {SPO} & {SAOE} & {SAT+R} \\ %%%% CHANGE add basic semantic relations
        \textbf{Language} & English & English & Spanish \& English \& Italian & Spanish \\
        \textbf{Documents}  & 6542 &  ----- &  & 1173(11.8\%) \\
        \textbf{Origin} & PubMed & Wikipedia, WordNet & Blog & Medline Spanish XML\\
        \hline
        \hline
    \end{tabularx}
    }
    \caption{Summary of related corpora annotated with a general-purpose schema, or not specific to the health domain.
    {Percentage values for \textbf{Documents} indicate how many of the original documents were actually annotated, as reported by the original authors. SPO: Subject, Predicate, Object triplets; SAT+R: Subject, Action, Target triplets and additional Relations (see section~\ref{sec:problem}); SAOE:  Subject, Action, Object, Emotion tuples.}%%%%CHANGE explain percentages and abbreviations
    \label{tab:stateofart2}}
    \end{table}

    \subsection{Descripción de los recursos}

    \begin{description}
      \item[Ixa MedGS]
      \item[DrugSemantics]
      \item[DDI]
      \item[Bio AMR]
      \item[YAGO]
      \item[ConceptNet]
      \item[CLEF]
      \item[BARR2]
      \item[Emotinet]
    \end{description}

    Most health-related corpora are annotated using self-defined health
    related entities relevant to the task at hand.
    Of these, arguably one of the most used is the CLEF
    corpus~\cite{kelly2016overview}.
    This corpus contains 150 English clinical documents, manually annotated by a
    team of experts (clinical and biologists) and non-experts.
    In contrast, the DDI corpus~\cite{herrero2013ddi}, which contains 1025 English
    documents from Medline was pre-annotated automatically, and then manually
    checked by domain experts (Pharmacists).
    Similar corpora in Spanish language exist.
    The Drug Semantic corpus~\cite{moreno2017drugsemantics} is an example, where
    domain experts (Registered Nurses and students) manually annotated Spanish
    summaries of product characteristics.
    {Likewise, the BARR2}~\cite{barr2} {corpus contains manually annotated abbreviation-definition
    pairs in Spanish clinical papers extracted from bibliographic databases.}
    %%% CHANGE add BARR2 corpus
    On the other hand, the Ixa MedGS corpus~\cite{oronoz2015creation} was
    pre-annotated automatically and then manually checked by domain experts in
    Pharmacology.
    An interesting alternative is the Bio AMR corpus~\cite{bioamr}, which contains
    AMR annotations of several medical documents, hence combining a general purpose
    annotation schema in a specific domain.

    In the context of general domain knowledge, one of the most relevant resources
    for our research is YAGO~\cite{fabian2007yago}.
    It consists of a large knowledge base automatically extracted from Wikipedia,
    WordNet, and other sources.
    Since YAGO is intended to represent general domain knowledge, its semantic
    structure is defined in terms of fact triples, in the spirit of RDF and other
    ontological representations.
    In contrast, the Emotinet knowledge base~\cite{emotinet} is oriented towards a
    specific domain (emotions), and is built from the manual annotation of blog
    entries, using a general semantic structure that links entities, actions, and
    emotions.
    Although Emotinet is designed for a particular domain, its structure is rather
    general, in the sense that it can readily represent any type of event or action
    performed by entities.

    As Table \ref{tab:stateofart} shows, the type of documents used is highly
    variable, which provokes large differences in terms of the length of documents,
    structure of discourse and vocabulary.
    An interesting characteristic is the type of annotation, either manual,
    pre-automated with expert review, or fully automated.
    Although recent research shows an increasing tendency towards pre-automated
    or fully automated annotation, manual annotation is still regarded as
    more reliable.

    Health related corpora are usually annotated by experts with a domain-specific
    semantic structure, such as entities related to diseases, drugs, genes, or
    treatments.
    Given the complexity of the concepts in the medical domain, annotators usually include
    medical doctors or other specialists of the medical domain.
    In these resources, very few general-purpose natural language features are used.
    This provides a greater detail of semantic information, since the entities and
    relations are relevant for the domain at hand.
    However, in the same sense, it might discard important information in the text
    which cannot be represented with the structure defined.
    This may or may not be an issue for a specific line of research.
    In our case, we consider it important to extract as much knowledge as
    possible from each source.
    In contrast, general purpose corpora or knowledge bases are usually annotated
    by non-experts with a semantic structure designed to represent as much
    knowledge as possible.
    This strategy tends to increase recall (a larger amount of facts is extracted)
    but it might extract irrelevant or incorrect facts.
    In these cases, the annotation schema relies largely on natural language semantics, such as Subject-Predicate-Object triples.

    The trend of representing knowledge with a general structure has been aided by recent
    advances in Teleologies~\cite{teleologies} that provide a theoretical framework for
    representing general purpose facts using a small set of concepts (objects,
    actions and functions).
    In contrast with Abstract Meaning Representation (AMR), the Teleologies framework is not
    specifically aimed at natural language understanding, but at representing
    the semantics of a general knowledge domain. This type of framework is less dependent
    on the linguistic characteristics of a specific language.
    The Subject-Action-Target structure defined in this paper is based on a simplification
    of the Teleologies conceptualization, applied to the domain of medical texts.
    However, inspired by general purpose knowledge bases, we also include a few specific
    semantic relations that are broadly used in general purpose ontologies and semantic networks.
    This combination (i.e. SAT+R, see Section~\ref{sec:problem}) makes the annotation schema used in eHealth-KD novel.

    \subsection{Características generales}

  \paragraph{General-purpose annotation}
  General-purpose annotation models are often used in corpora extracted from encyclopedic sources, such as \textit{YAGO}~\cite{suchanek2007yago} and \textit{ConceptNet}~\cite{speer2017conceptnet}, both of which contain facts automatically extracted from Wikipedia~(among other sources). In contrast, domain-specific annotation models are usually employed when the source is more restricted to a specific domain. Examples include \textit{Ixa MedGS}~\cite{ORONOZ2015318}, which contains health related concepts for diseases, causes and medications; \textit{DrugSemantics}~\cite{moreno2017drugsemantics}, which annotates health entities, drugs and procedures; and, \textit{DDI}~\cite{herrero2013ddi}, which annotates drug-drug interactions. A middle ground is the \textit{Bio AMR}~\cite{bioamr} corpus, which applies a general purpose annotation model~(AMR)~\cite{banarescu2013abstract} to health documents. The \textit{eHealth-KD v2} corpus is similar to the latter in this respect, since the annotation model defined is general, but it is applied specifically to health sentences in this research.
  The \textit{eHealth-KD v2} corpus constitutes the result of the evolution of the \textit{eHealth-KD v1}~\cite{ehealth} corpus.

  Most of the aforementioned resources are focused on capturing the semantics of sentences, in the sense that very different sentences with the same facts are likely to be similarly annotated. We consider \textit{BioAMR} less independent of syntax because even though AMR is a semantic annotation model---far more abstract than dependency parsing, for example---, it still relies heavily on sentence grammatical structure. Hence, a significant change in the sentence structure is likely to change the annotation, even if the underlying semantic message remains unchanged. For example, since AMR uses PropBank~\cite{propbank} roles, changing a word for a semantically similar word, including a synonym, will probably change the corresponding annotation and thereby the available roles.
  This also makes AMR and similar resources language-dependent, not only in practice given their dependence on the existence
  of word banks, but also in nature. While attempting to apply AMR in Spanish, \citet{migueles2018annotating} show that even though AMR is theoretically language-agnostic,
  the existing annotation guidelines are biased towards English and must be adapted to capture linguistic phenomena
  that don't exist in English.
  The annotation model designed in this research for the \textit{eHealth-KD v2} corpus, attempts to achieve a higher level of syntactic independence, in part by using a smaller set of entities, relations and roles than AMR. More specifically, our annotation model does not distinguish semantic roles for each possible \texttt{Action}, instead relying on general purpose roles~(i.e., \texttt{subject} and \texttt{target}, see Section~\ref{subsec:model}).

  \paragraph{Ontological knowledge}
  General-purpose annotation models often allow ontological knowledge to be represented in the form of inheritance and composition between concepts. In this context, we consider the ability to recognize and annotate  these ontological relations in the source text. Health-related annotation models do not usually deal with this problem, mainly because the entities and relations to annotate form a predefined ontology where composition and hierarchy, if any exist, are already conceived in the annotation model itself. However, general purpose annotations often include relations like \textit{ConceptNet}'s \texttt{is-a} or \texttt{part-of} that directly represent these ontological concepts, and are thus able to extract ontological representations from natural text.

  \paragraph{Composite concepts}
  The model designed for the \textit{eHealth-KD v2} corpus also includes relations specifically for this purpose, mostly inspired by \textit{ConceptNet} and \textit{YAGO}.
  Composite concepts, in contrast, refer to the ability to annotate concepts that are formed by a fine-grained combination of other entities, in the same sentence. For example, take the sentence: ``\textit{the doctors that work the night shift get paid extra hours}''. \textit{AMR} allows for the representation of the concept that not all doctors, but only those that work the night shift, are the ones who get paid extra hours. Our proposal also includes several annotation patterns to deal with this type of scenario.

  \paragraph{Attributes}
  Attributes are often used to further refine the meaning of annotated entities. Examples include quantifiers in \textit{AMR}, or modifiers that specify a degree of uncertainty, or a negation of a concept. Our proposal includes four general-purpose attributes that model uncertainty, negation and qualifiers for expressing emphasis.

  \paragraph{Contextual relations}
  Contextual relations, as defined in the \textit{eHealth-KD v2} corpus, allow  facts that only occur
  under certain conditions to be represented, for example, in a specific time frame or location or under certain assumptions. This allows for a finer-grained semantic annotation. \textit{BioAMR} inherits this ability from \textit{AMR}, which allows modifiers for expressing \textit{how}, \textit{when}, \textit{where} or \textit{why} some event occurs. In our proposal, we provide contextual relations that specify time and location, and an additional general-purpose relation for other conditions.

  \paragraph{Causality and entailment}
  Causality and entailment are general-purpose relations that allow some level of inference or reasoning. The \textit{Ixa MedGS} corpus defines a \texttt{causes} relation, since it is relevant in the domain the corpus is modeling. Likewise, \textit{AMR} and \textit{ConceptNet} include similar relations. Our proposal includes both causality and entailment as two different relations with well-defined semantic meanings.

  \section{Sistemas de Aprendizaje Automático para el Descubrimiento de Conocimiento}

  In recent years, researchers in fields such as machine learning, knowledge discovery, data mining and
natural language processing, among others, have produced many approaches and techniques to
leverage the large amount of information in the Internet for a variety of tasks, from
building search~\cite{google} and recommender systems~\cite{youtube}
to improving medical diagnostics~\cite{watson}.

Among the different approaches relevant to knowledge discovery, we can recognize a
continuous spectrum of techniques, based on how much expert knowledge is used.
Heavily knowledge-based techniques are based
on rules defined in knowledge bases handcrafted by domain experts~\cite{chandrasekaran1986generic}.
These approaches have a great degree
of reliability and precision, and generally allow for more complexity in the extracted knowledge,
but are difficult to scale to large amounts of data.
In contrast, the statistical approaches consist of techniques based on pattern recognition with statistical
and probabilistic models~\cite{kevin2012machine}. These techniques scale better with large amounts of data~\cite{le2013building},
providing better recall, but often are limited to extracting simple models of knowledge,
and can be more sensitive to noisy, fake or biased information~\cite{bolukbasi2016man}.

Given these mutually complementary characteristics, several hybrid approaches have been proposed.
Recently, research areas such as ontology learning~\cite{cimiano2009ontology},
learning by reading~\cite{barker2007learning} or entity embedding~\cite{hu2015entity} have arisen.
In these areas, researchers combine techniques from machine learning, natural language
processing and knowledge representation to solve more complex problems that cannot
be dealt with using only the classical tools.

Many machine learning systems are designed to solve a domain-specific task, such as
assigning a class to an element from a predefined set of labels. These systems,
when trained with data for a particular domain, are often not applicable to other domains
or to scenarios where several different domains must be used together. Moreover,
often systems are designed to be trained once from a corpus, and don't allow for
a continuous improvement of the knowledge learned.
Recently there are attempts to build general-purpose learning systems that are always
improving while obtaining new knowledge, re-evaluating the old knowledge and refining their
own confidence~\cite{mitchell2015never}.

Additionally, it is interesting to design non-monolithic learning systems, but instead
built as a set of modular components that can be combined in different ways.
This composability would allow a continuous learning system not only to improve the
quality of the extracted knowledge, but also to learn how to tune its own internal
parameters to perform a better knowledge extraction in the future. It is conceivable
that such a system could gradually learn which types of basic processes (i.e., entity recognition, POS-tagging, etc.)
are most useful for a given domain or for a particular corpus. Likewise, such a system could
learn which types of probabilistic models provide the best results in a particular dataset.

  The speed and volume at which information is produced has increased exponentially in the last decade,
   mainly due to the rise of the social networks and the mobile technology. In order to cope with this volume of information,
   it is necessary to be able to process massive amounts of data continuously.
    The field of machine learning provides tools for the automatic extraction of information and knowledge from different
    sources of data.
    Machine learning not only allows to automate processes and tasks of knowledge discovery or text mining, but also provides a large
    improvement in the scalability of these processes~\cite{wu2014data}. By using mass computing resources, it is possible to process millions
    of raw documents in a reasonable time, far exceeding what can be done by domain experts.
    Recent improvements in computing capabilities and access to larger datasets have given rise
    to the field of deep learning, which has improved the state of the art in
    several of the classic machine learning tasks~\cite{lecun2015deep}.

	Arguably, the two most common approaches in machine learning are supervised and unsupervised learning~\cite{kevin2012machine}.
    Supervised learning can be used for recognizing specific elements of knowledge in a source of data. For example, tagging
    pieces of text to indicate that they define an entity~\cite{nadeau2007survey} (e.g., a person, organization, or place), recognizing relations between
    said entities, or assigning a sentiment or opinion score~\cite{liu2012sentiment} to a fragment of text. On the other hand, unsupervised learning
    can help with finding relevant structure in a large set of elements. Clustering algorithms can be used to detect similar
    concepts or to extract abstract concepts from groups of more concrete elements. Other techniques can be used for reducing
    the amount of information, for example, to remove noisy, uncertain or irrelevant pieces of
    information~\cite{bingham2001random}.

    In general, most machine learning algorithms are not designed to represent the learned knowledge in complex structures,
    such as those defined by human domain experts (i.e., ontologies). In turn, the representations often have a simple structure, such
 	as a probability distribution or a correlation matrix~\cite{bengio2013representation}.
    When applying these algorithms to a real problem, an domain-specific
    interpretation of those representations has to be made.
    % Esta es una debilidad imporante, la no explicación
    Furthermore, many of the most powerful machine learning models are difficult to explain, in the sense
    that when the system produces an answer, a human expert cannot easily understand and reproduce the inference steps
    that the system performs~\cite{olden2002illuminating}.
    % creo mejor decir que si la representacion es tan importante las ontologias son una forma de representar conocimiento
    % y entonces meter el gancho de embeddings y entity embeddings
    Choosing an appropriate representation is decisive for the success of most machine learning techniques~\cite{bengio2012deep}.
    In recent years, there has been an increased interest in the problem of automatically learning relevant representations.
    Word embeddings~\cite{mikolov} and more general entity embeddings~\cite{hu2015entity} represent the first steps towards powering
    deep learning approaches with more explainable internal representations.
    Since ontologies are, by definition, representations of a given conceptualization, it is conceivable that using ontologies
    as seeds for the representation of a given domain, the performance of data mining processes based on machine learning can
    be improved.

  \section{Entornos de Evaluación Competitivos}

  A strategy often used to encourage research on a specific task is the organization of a shared
  evaluation campaign. In contrast with regular research, evaluation campaigns often have a fixed
  time frame, and evaluation resources are not fully disclosed~(e.g., gold annotations for
  test sets are hidden) to allow a fair comparison in a friendly competitive environment.
  In this section, we analyze relevant efforts for organizing evaluation campaigns for both the biomedical domain or for dealing with entity and relation extraction.

  Several online services allow researchers to organize machine learning challenges and competitions, providing automatic grading, user management, and other useful features.
  Kaggle\footnote{\url{https://kaggle.com}} is arguably the most popular choice, its main limitation for our purposes being that to host a challenge, organizers must contact the service providers. Possible alternatives are AIcrowd\footnote{\url{https://www.aicrowd.com}} and
  Codalab\footnote{\url{https://codalab.org}} which provide free options for challenge organizers.

  The CLEF eHealth Evaluation Lab has proposed several challenges in the biomedical domain, including
  named entity recognition~\cite{clef2013} and information extraction~\cite{clef2014} in English,
  and later editions in French documents~\cite{clef2015, clef2016}.
  In these challenges, medical reports from MEDLINE, EMEA and similar sources are annotated with disorders, medical terms, acronyms and abbreviations, which provide evaluation scenarios
  for several NLP tasks, including entity recognition, normalization and disambiguation.
  Another relevant task is proposed by~\citet{semeval2017-task9} in Semeval 2017, focused on AMR parsing and generation from biomedical
  sentences in English. Applying a general-purpose conceptualization, such as AMR, to specific domains encouraged participants to bridge the gap between developing generalizable techniques and applying domain-specific heuristics.
  However, AMR parsing is already a complex problem in itself, which can negatively impact on researcher participation in these challenges if they are not specialized in AMR.
  Simpler, general-purpose models can encourage a greater degree of participation given the easier entry curve.
  An example of the latter is the Semeval 2017 Task 10~\cite{semeval2017-task10},
  a challenge regarding keyphrase and relation extraction from scientific documents,
  with a simple model based on three entity classes and two general-purpose relations.
  This task received a much larger number of submissions than the former, even though both challenges where hosted on the same venue and aimed at similar audiences.

  As can be expected, English is the most prominently used language in NER-related challenges, given the larger
  number of available corpora and resources. However, important efforts have been devoted to fostering research
  in less prominent languages. Relevant to our discussion are the IberLEF campaigns that focus on Iberian languages,
  such as Spanish, Portuguese, Catalan, and other regional variations.
  Two examples of recent NER-related tasks are the Portuguese Named Entity~\cite{glauber2019iberlef} challenge and the MEDDOCAN~\cite{marimon2019automatic} document anonymization challenge.
  The first proposes entity recognition and relation extraction in the general domain, in Portuguese.
  The second proposes the identification of privacy-sensitive entity mentions in medical documents, e.g., names, addresses,
  dates, ages, etc.

  Outside the frame of a competition, open, long-running evaluation systems allow
  researchers to evaluate their approaches with official evaluation metrics.
  This can also provide a centralized repository of the state-of-the-art, where existing approaches are
  summarized and linked to existing papers.
  In this regard, this research proposes an online evaluation system that allows a comparison of
  new approaches with officially published results at any time. Based on this infrastructure, official evaluation campaigns with a more competitive design are organized in scheduled time-frames.

  \section{Representación del Conocimiento}

  There are many resources used in the task of
natural language processing. Among the most common ones, ontologies, as representations
of concepts, data types and their relations in an explicit computational structure.
Some of the most popular ontologies used in NLP are WordNet~\cite{} and DBpedia~\cite{}.
For their ubiquity and usefulness, research in ontologies has increased in the last years,
particularly regarding the automatic creation of these structures,
giving birth to the field of Ontology Learning.
Ontology Learning has the potential of reducing the cost of creating
and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.

Ontology Learning deals with the problem of automatically creating an
ontology from several resources. Examples include mining analysis, mostly
the study of text in natural language~/cite{}. In this
particular task, several approaches have been presented, from syntactic and
semantic analysis of corpora~\cite{} to statistical and machine learning based
approaches~\cite{}.

Several tools and systems have been proposed for this task.
Text2Onto~\cite{cimiano2005text2onto} is a framework for
data-driven change discovery which employs a probabilistic
ontology model (POM).
OntoLT~\cite{buitelaar2004ontolt} extracts concepts and
relations automatically from linguistically annotated text collections,
by means of rule set, which maps linguistic classes to ontology classes.
A newer approach is OntoGain~\cite{drymonas2010unsupervised},
which uses an unsupervised approach,
and exploits the inherent multi-word term's lexical information
in order to extract higher level concepts.

One of the issues with these approaches is the amount of spurious
information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
In general, there will be many unimportant
or redundant pieces of information in the analyzed corpora. A naive
approach that doesn't take this issue into consideration will create
immense ontologies with very little useful information. To tackle
this problem, OntoGain proposes a hierarchical clustering scheme
that attempts to identify general concepts and relations.

In general these resources are focused in the extraction of knowledge and
pay much less attention to the task of find relevant knowledge.
An ontology is interesting not just because of its size, but
because of how is such information selected, processed and stored.
In this problem we focus our work, attempting not only
to recognize information, but also to reduce this information and obtain
information with more relevance.

-----------

The problem of discovering, storing and using knowledge in a computationally effective
form has been widely studied~\cite{mitchell2015never, ROSPOCHER2016132, cimiano2009ontology}.
This problem has been treated from two different but complementary research areas:
the fields of knowledge representation and machine learning.
The knowledge representation community provides means for
computationally representing and operating with stored knowledge in forms that
can ensure some degree of logical consistency.
Conversely, the machine learning community provides
tools for obtaining useful knowledge from large collections of structured and unstructured data.
Recenlty, a new discipline named ontology learning has arisen, which draws ideas
and techniques from both the knowledge representation and the machine learning fields.
This field deals with the problem of automatically building ontological representations
of knowledge from a variety of data sources. As such, the theory of ontology learning
is relevant in the design of the framework presented in this paper.
In this section we present a brief review of the relevant concepts and technologies
in each of these three domains, as well as some approaches similar to our proposal.

	\subsection{Knowledge Representation and Reasoning}

    Since the dawn of computer science, one of the problems that has attracted wide attention
    is that of representing knowledge in a computational format, such that automatic reasoning
    can be performed to discover new, previously unknown truths~\cite{sowa2000knowledge}.
    Arguably, the most popular knowledge representation technology in use are
    ontologies~\cite{guarino1995formal}, which have
    become the \emph{de facto} standard.
    %Definición
	Ontologies can been defined as a formal specification of a conceptualization~\cite{asuncion2003}.
    This represents concepts, relations between these concepts, instances of these concepts and inference rules
    for deriving new relations.

    As such, ontologies can be considered as a combination of two predominant approaches
    for knowledge representation: those based on formal logic~\cite{brachman1992knowledge}
    and those based on graphs of semantic relations~\cite{chein2008graph}.
    In logic-based approaches the facts are represented as logic predicates or functions
    and reasoning is enabled through the application of formal inference rules.
    In contrast, graph-based representations express facts as nodes~(objects)
    and edges~(relations) and reasoning is built on top of graph traversing methods.
    However, in ontologies objects and their attributes and relations are represented in a graph of concepts,
    which can also be interpreted as a set of predicates and functions on these objects.
    On top of this layer, inference rules can be added,
    which enable logical reasoning methods to be used for deriving new attributes and
    relations between existing concepts.

	Relations in an ontology can be of a specific domain, but often some general domain
    relations are represented, such
    as \textit{is-a} and \textit{part-of}. These types of relations allow representing more abstract
    or complex concepts out of the composition of more concrete or simple concepts.
    Hence many ontologies contain some kind of taxonomy of increasingly abstract concepts,
    that are also interconnected
    with each other using other semantical relations which can be domain specific.
    % Ventajas
    These resources allow representing complex frameworks of knowledge, down to a degree of specificity which
    enables the design of fully automated reasoning tools which use this knowledge for
    a variety of computational tasks.
    % Dificultades o desventajas
  	Due to the high complexity of the concepts and relations that are represented,
    and the experience needed to recognize the most relevant concepts of a domain,
    ontologies are usually manually constructed by domain experts~\cite{wong2012ontology}.
    Thus, building an ontology is a process that requires a long time and a large number of experts to
    define and populate it with relevant instances that refer to objects and relations.
    This makes really difficult build hand crafted ontologies and ensure their maintainability, due to day a day huge quantity of new and valuable information, desirable to convert into knowledge, appear in the World Wide Web.
    Another important outcome of this process is that experts usually represent only facts that are absolutely
    true in the domain. Although existing ontology formats can be extended to deal with fuzzy~\cite{fuzzyontology} or
    vague data~\cite{bobillo2011fuzzy}, manually assigning a degree of belief to a specific fact is a complex task.

%     % Ejemplos
    It is possible to distinguish between two types of ontologies: general domain (or upper ontologies) and domain-specific (or simply domain ontologies).
    Domain-specific ontologies are those which deal with the concepts and relations of the particular knowledge domain.
    As examples, we can cite ontologies in the medical sciences~\cite{rector2003opengalen,gene2004gene},
    or the software engineering field~\cite{4641930}.
    Other ontologies are more general since they can be used in different domains, or they are used for general
    purpose tasks which are employed in many areas.
    WordNet~\cite{miller1995wordnet} is a general purpose ontology that contains most of the words of the English language,
    and syntactic and semantic relations between them.
    It is used in many tasks in natural language processing and text mining.
    DBPedia~\cite{mendes2012dbpedia} is an encyclopedic ontology that contains part of the knowledge present in the
    Wikipedia\footnote{http://www.wikipedia.org}.
    It relates people, historical events, facts, locations, and other concepts, in a structured and queryable format.
    Since ontologies have a unified form of representing a single fact, concept or relation using Uniform Resource Locator~(URL), it is
    possible and very common for different ontologies to link each other. For example, many domain specific ontologies
    have entities which are linked to the corresponding entry in DBPedia. The approach of linking and referencing to other
  	widely known ontologies, known as \textit{linked data}~\cite{bizer2009linked},
    enables the standarization of the representation of shared knowledge and eases the tasks of querying and analyzing it.

    Ontologies are an effective tool for representing knowledge in a wide
    variety of domains and scenarios~\cite{staab2010handbook}.
    They are flexibly enough to adapt to a particular domain and powerful enough to represent complex concepts.
    However, one of the most complex task in this sense is maintaining
    an ontology up-to-date with respect to the massive amount of unstructured data that is generated and published every day.
	Therefore, the need arises for computational tools to build ontologies with automated or semi-automated processes.

    \subsection{Ontology Learning}

    In the intersection of machine learning and knowledge representation,
      the field of ontology learning has arisen to deal with the
      complexity of manually maintaining and updating ontologies.
      This field draws techniques and tools from both communities,
      to automate part of the process of creating and maintaining ontologies.
    Ontology learning has the potential of reducing the cost of creating
    and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.
      This problem is also addressed in
      learning by reading~\cite{barker2007learning}, a field which draws techniques
    the natural language processing and knowledge representation and reasoning communities.
    The purpose is to build a formal representation of some particular field given unrestricted
    textual data related to the field. This representation must also allow fully automatic
    reasoning.
      Learning by reading can be considered as a particular case of ontology learning,
      even though it is only concerned with textual input, and the
      output is not necessarily formated as an ontology.

      In the field of ontology learning, two general high level tasks can be distinguished:
      ontology population and ontology enrichment~\cite{petasis2011ontology}.
      Ontology population deals with the
      sub-problem of finding new instances for an already defined ontology, while
      ontology enrichment deals with adding new concepts and relations to an existing
      ontology. There is an overlap between these tasks, and most of the existing
      approaches cannot be classified purely in these terms.
      In this field, several tools have been proposed, which
      combine different approaches and solve different subsets of the ontology
      learning tasks. A brief review of these systems can help defining the
      main characteristics that our framework should have.
      % No sé si decir algo como a continuación se presentan varias de ellas
      % atendiendo a sus características mas relevantes. Esto nos permite conocer
      %la diversidad de enfoques y las potensialidades que debemos tener en cuenta
      %para crear nuestro sistema

      % general (early approach)
      Early approaches, such as SYNDIKATE~\cite{syndikate}, deal only with populating a
      knowledge base, with a predefined ontological structure~(classes and relations).
      % web approaches
      Since the Web is a rich source of information, several approaches have focused on extracting knowledge
      from it, exploiting the semi-structured format of web resources.
      Some systems like ARTEQUAKT~\cite{artequakt} and SOBA~\cite{soba} are domain-specific,
      respectively focusing on the art and the sports domains.
      Other systems, like WEB-$>$KB~\cite{webkb} attempt to build general domain knowledge bases from the
      web, exploiting also the structure of links between pages to identify relations.
      % from structured data
      Another example is the VIKEF~\cite{vikef} system, which uses product catalogs as sources of data, hence exploiting the
      inherent structure present in this type of data.
      % bootstraping with human knowledge
      Even though most systems attempt fully-automatic extraction, some examples like ADAPTATIVA~\cite{adaptativa} include
      a bootstrapping strategy, where human experts provide feedback about the extracted knowledge.

      In order to extract relevant knowledge from unrestricted text, NLP techniques have
      been introduced in systems such as OPTIMA~\cite{optima} and ISODLE~\cite{isolde}.
      % rule-based
      The use of natural language features can be used build rule-based systems,
      like the OntoLT~\cite{buitelaar2004ontolt} proposal, that extract concepts and
    relations via a mapping of linguistic classes to ontology classes.
      % based on statistical models
      An alternative approach is to use statistical or probabilistic models,
      exemplified by systems such LEILA~\cite{leila} or Text2Onto~\cite{cimiano2005text2onto}.
    % relevance
      Another example is KnowItAll~\cite{knowitall}, which introduces a point-wise mutual information
      metric to select relevant instances.

    Once instances of entities are relations are extracted from text, a natural question is
      wether more abstract knowledge can be inferred from these examples.
      The systems who address this issue often use unsupervised techniques to attempt
      to discover inherent structures. Two relevant examples of this approach
      are OntoGain~\cite{drymonas2010unsupervised} and ASIUM~\cite{asium},
      which attempt to automatically build a hierarchy of concepts using clustering techniques.
      % multimedia
      The BOEMIE~\cite{boemie} system is another interesting example, since it
      attempts to automatically infer abstract concepts from the concrete instances found,
      but focuses not only on text, but also on multimedia sources such as images, and videos.
    % never-ending learning
      Most of the mentioned systems usually focus on one iteration of the extraction process.
      However, more recent approaches, like NELL~\cite{mitchell2015never}, attempt
      to learn continuosly from a stream of web data, and increase over time
      both the amount and the quality of the knowledge discovered.

    One of the issues with many of these approaches is the amount of spurious
    information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
    In general, there will be many unimportant
    or redundant pieces of information in the analyzed corpora. A naive
    approach that doesn't take this issue into consideration will create
    immense ontologies with very little useful information. To tackle
    this problem, OntoGain proposes a hierarchical clustering scheme
    that attempts to identify general concepts and relations.

    In general, these tools are focused on the extraction of knowledge and
    on the task of finding relevant knowledge.
      When the extracting knowledge from a trustworthy source, even if a natural
      language source, it makes sense to
      focus on optimizing recall, i.e., obtaining as much information as possible.
      If the input source is a set of medical papers, or the main
      web page of an institution, there is a high chance that most of the information
      present in those documents is correct. Hence, an ontology extraction procedure
      that maximizes recall will obtain good results.

      However, when the input source is of a lesser quality, such as blogs or
      social media posts, there is a greater chance that some, or even most,
      of the information is fake or incorrect.
      If we consider also the so called phenomena of the \textit{post-truth},
      and acknowledge that some authors are deliberately sharing fake news
      or facts, the problem becomes much harder, and pressing.
      Even if deliberate lies were not an issue, most of the information shared
      in social media and similar sources is irrelevant in a long term.
      In this context, the problem of extracting a useful ontology from a large
      corpora of Internet sources becomes less a problem of recognizing the pieces
      of information lying in the corpus, and more a problem of filtering and selecting
      the relevant information, once extracted.

      Despite the existence of some general purpose systems,
      there is no proposal of an architecture for a computational framework
      that can simultaneously and continuosly learn from the most varied sources of information online.
      Another challenge in this aspect is to obtain a computationally convenient representation
      of this knowledge, independent of the domain, source and format of the input data.
      Furthermore, such system has to explicitly deal with the large amount of spurious,
      irrelevant, or deliberately fake information spread through web sources.

    \subsection{Quality Metrics}\label{sec:evaluation}

% 	Once a computational system is implemented following the guidelines of the framework
%     presented in this paper, it is necessary to perform some evaluation on its output.
%     However, being a complex system, with many interrelated components, evaluating it
%     is not as simple as comparing the actual output with the expected output.
    In this section we present a methodology for evaluating such a framework and obtaining
    interesting metrics that can validate its performance across the large range of
    tasks the framework is intended to enable.

    In a computational system or software framework, there are interesting
    software engineering metrics to evaluate. Such a system should be highly modular and
    extensible, so that it can be easily adapted to new input formats, or new algorithms
    can be easily plugged in and integrated into the whole pipeline.
    The modular design of the framework can help in achieving a high
    degree of extensibility.

	Besides these high level metrics, each of the tasks performed by the framework can be
    evaluated separately. Most of these tasks have a definite performance metric that can
    be used to rate the degree of correctness of such task. For many of the tasks described in the previous
    sections we can find standard performance metrics in the literature that can used to
    evaluate each particular process.

%     For example, a sensor for
%     extracting entities can be evaluated individually by running it on a tagged corpus
%     and computing precision and recall.

    An aggregate metric of these individual performances could provide a high level
    overview of the performance of the whole framework.
    However, designing an aggregate metric that provides a practical, interpretable,
    measure of the quality of the framework performance, can be very complex.
    Each of the different tasks performed by the framework can have a very different
    baseline performance. A 90\% precision can be a very good result
    in some complex tasks, such as dependency parsing~\cite{AlbertiABCGKKMO17}, but mediocre in other tasks, such
    as image classification~\cite{Russakovsky2015}. Moreover, this baseline number can
    vary not only across tasks, but in the same task, according to which test suite (or corpus)
    is used.

	Going up one level of abstraction, for the general problem of ontology learning there are also
    several evaluation metrics and methodologies available, such as OntoRand~\cite{ontorand}
    and OntoMetric~\cite{ontometric}. However, most of these
    methodologies are designed for evaluating a single ontology that is either created or
    modified using techniques from ontology learning. Once more, extending these methodologies
    to a collection of ontologies is not as straightforward as aggregating or averaging
    the individual results. On the other hand, when dealing with a collection of ontologies,
    other concerns can arise, such as intra-ontology consistency, which are not usually
    considered when evaluating a single ontology. As a final consideration, the framework
    by design, is expected to maintain a degree of internal inconsistency in order
    to better cover multiple and possibly overlapping domains.
    We present some of the most commonly described approaches in the literature for
    evaluating ontologies~\cite{petasis2011ontology}
    and describe how they can be used in our context.

    \paragraph{M1- Comparison with a gold standard.}
	This approach consists in comparing a learned ontology with a baseline ontology
    for the same domain~\cite{corcoglioniti2016frame}. The baseline ontology is
    assumed to be both correct and largely representative of the domain. This method
    provides a great trade-off of speed versus accuracy, since both ontologies
    can be automatically compared in a number of metrics without human intervention,
    and the results have a high reliability because the baseline ontology is created
    by experts. Some disadvantages do exist, for example, it is not always easy to
    find a good baseline ontology for a given domain, especially if the domain is
    not very well defined or is very novel. On the other hand, even two ontologies
    extracted from the same domain by experts can have wide differences with respect to
    the structure, and particularly to the names the classes and relations are
    assigned. This requires some form of normalization and mapping between both ontologies
    prior to comparison. This metric is difficult to use, particularly if
    we're creating new characteristics.

    \paragraph{M2- Expert evaluation.}
    An alternative middle ground to the previous approach is having a domain
    expert (or several) to simply look at the resulting ontology and evaluate
    it according to some predefined metrics~\cite{ROSPOCHER2016132}.
    This is arguably the most reliable method, in the sense that it provides
    the highest degree of validation one could aspire. However, the clear
    disadvantage lies in the limited amount of information that a human can
    process in a reasonable time. This disadvantage is worsened in the case
    where an ontology is created from a very large corpus of data, as is
    the purpose of our framework. In this case, a small subset of the data
    could be analyzed, and results extrapolated from there, but this idea
    adds the complexity of determining a subset that is relevant enough but
    remains of a manageable size.
    This metric is usually expensive or difficult to use.

    \paragraph{M3- Evaluation through an application.}
    A more practical approach consists in finding an interesting application and
    evaluating if the use of a learned ontology provides an improvement in that
    application~\cite{gurevych2003semantic}. For example, using a learned ontology
    about human feelings and related phrases to improve the performance of a
    standard opinion mining problem. If using the knowledge represented in the
    ontology provides a boost to performance, as measured by the standard
    approach in the given application, we obtain a reliable validation that
    the process for learning ontology, at least, has a measurable practical benefit.
    In some sense, this is one of the most valuable evaluations to perform,
    because it provides an immediate comparison baseline for a practical problem.
    The previous methods which only evaluate the ontology internally do not
    necessarily guarantee that its content will be useful, even if it's correct by
    all metrics. Another upside is that the evaluation process can be completely
    automated, and scaled to match the complexity and size of the target application.
    As a downside, validating a use case is not necessarily a metric for the overall
    quality of the learned knowledge, and it's not clear if those results will
    replicate in different domains and applications.
    This metric indeed seem more simple but in many cases not need
    improvement a task, but a wide set.

    \paragraph{M4- Data driven evaluation.}
    Finally, a data driven evaluation can be performed, by comparing the entities
    and relations in an ontology to a corpus of data, not used during the
    construction of the ontology, but representative of the same
    domain~\cite{brank2005survey}. The ontology can be evaluated by counting
    the number of overlapping entities present in it against those found in the corpus.
    Care must be taken to allow for some variation in the corpus with respect to
    the ontology, for example, using some form of query expansion.
    This approach has been used to relatively compare different expert-made
    ontologies against the same corpus and deciding which ontology provides
    the best ``fit'' with the corpus~\cite{brewster2004data}.
    However, obtaining an absolute metric of fit between an ontology and a
    corpus is harder, mainly because it is not known beforehand what is the
    value of fit one should expect to achieve.
    Another possible issue of this approach, in the particular case of
    ontologies that have been learned from the text, is to inadvertently
    introduce a bias in the evaluation. If the methods used to compare the
    ontology and the text corpus are correlated with those used to build the ontology,
    then the results will be of a dubious validity.
    For example, if one uses a NER algorithm during the building of the ontology,
    and the same algorithm is used in the corpus to recognize relevant entities;
    or if some co-occurrence metric is used for detecting relations in both cases.
    This metric is complex to define and on many occasions is unrepresentative.

    \vspace{1em}

    Evaluating a single ontology learning method is a complex task, as shown by the multiple approaches
    proposed in the community. Hence, it's is very unlikely we can find a single automated metric
    to measure the overall
    performance of a framework as the one proposed. The best approach seems to be using a combination of the
    existing methods, adapted to our scenario, with the added complexity of dealing with multiple ontologies
    at the same time. In some cases a gold standard can be found and used to obtain a baseline comparison. In
    other cases, provided a suitable interface for easily querying the knowledge is added, a domain expert can
    interact with the framework and give a qualitative evaluation for the domain of interest.
    From a pragmatical point of the view, the most interesting and valuable evaluation seems to be finding
    relevant practical problems that can be solved, or improved, when using our framework.

    %The phenomenon of the post-truth era brings additional challenges. In this context, we believe
    %that for the problem of knowledge discovery the task of evaluating the relevance of the knowledge
    %extracted becomes more important than ever before.
----------
    \subsection{Learning tasks disscusion}

In the ontology learning community, several frameworks have been developed which attack problems
similar to the ones we present. Some of the approaches found in the literature concentrate on one
particular task, i.e., ontology creation, population, or enrichment, among others. For example,
frameworks like KnowItAll~\cite{knowitall}, Artequakt~\cite{artequakt} and SOBA~\cite{soba} are oriented mainly towards the task of ontology
population. Others, such as ASIUM~\cite{asium}, VIKEF~\cite{vikef} and SYNDICATE~\cite{syndikate} are oriented mainly towards ontology enrichment.
However, many of the tasks or subproblems that need to be solved in either of these domains are
very similar and can be reused. Hence, more general frameworks such as Text2Onto~\cite{cimiano2005text2onto} or BOEMIE~\cite{boemie} have
arisen which deal with a combination of these tasks.
Our framework is designed to handle the common ontology learning tasks,
including ontology creation, population, enrichment, merging and mapping.
Each of these tasks is performed in one or more of the framework's main module,
not as independent or isolated features,
but as an integral part of the framework's learning process.

The module for processing unstructured data can be interpreted as an ontology
creation and population pipeline, because the input is raw unstructured data and the output is
a brand new ontology where everything from classes and relations to specific instances is
created automatically from scratch.
On the other hand, once these newly created ontologies are delivered to the module for knowledge
discovery, several processes are performed which can be interpreted as forms of ontology
enrichment, since new relations can be discovered and inference rules can be automatically defined.
Finally, in the module for processing structured data a process of ontology merging and mapping is
performed, to normalize the incoming ontologies against the knowledge already stored.

\subsection{Quantity and complexity}

Regarding the quantity and complexity of concepts recognized, the existing solutions can
be divided into those that only extract entities (e.g., KnowItAll), those who only extract
relations (e.g., ADAPTATIVA~\cite{adaptativa}, LEILA~\cite{leila}), and those which attempt to extract both (e.g., Artequakt, Web->KB~\cite{webkb}, BOEMIE).
Our approach corresponds to this last category, with the addition that it attempts to extract also
more abstract concepts via techniques such as hierarchical clustering.
The discovery of inference rules is another relevant task that we intend to integrate
in the module for knowledge discovery, as part of the process for generating new knowledge.
This process enriches the ontologies already built by adding higher-level knowledge, in the form
of logical predicates or axioms. These can in turn be used later for discovering missing instances
or relations, or for detecting outliers and mistakes.

\subsection{Use of machine learning}

Most systems employ some form of machine learning tools for most of the
tasks. In particular, many employ NLP tools to process natural text and extract pieces of knowledge,
and statistical techniques to detect clusters. However, in general, the architecture of these
systems generally follows a very strictly designed pipeline, where components are carefully
connected to each other. Our proposal is different in the sense that we pretend our system
to be fully learnable in the future. All components, algorithms, and parts, can be measured and evaluated
automatically, if the knowledge of how these parts interact is described in an organizational
ontology inside the framework.

% NUEVO
The organizational ontology, once defined, should provide a description of the framework's
modules, down to a degree of detail that allows performing automated reasoning about which
components can be connected. This ontology could be used in an automatic pipeline
designer, i.e., a module that learns how to combine specific components
(algorithms, data sources, etc.) for a particular learning task. By self-evaluating
its own performance on each particular task (given a combination of the evaluation
metrics presented in section~\ref{sec:evaluation}), this module could learn, for example,
if for a particular data source is more convenient applying entity extraction or removing
stopwords, or which clustering algorithm performs better.
As an illustrative example, Figure~\ref{fig:pipeline} shows several possible
learning pipelines built from common components. A relevant evaluation metric for each
of these pipelines could allow the framework to automatically discover the best combinations.
This opens the door to the possibility of, in time, the framework
itself learning which algorithms and approaches work better in each task, or in each domain.
% ---

\subsection{Human input}

Most of the frameworks and solutions existing require a certain degree of human interaction.
In most cases, a domain expert is expected to interact with a computational tool to either validate or
refine the output of learning pipeline.
As explained before, our approach proposes to be completely automated,
in the sense that the final knowledge stored may never be revised by a human. The whole framework is thus
designed with the purpose of complete automation. All the decisions that determine which fragments
of the information extracted get stored are based on computational parameters that can
be automatically tunned.
This does not deny some cases where a domain expert could interact with the framework,
through some form of natural language interface, in order to access the stored knowledge.
However, this possibility of interaction is a result of the framework, not a necessity.

\subsection{Generalization}

With respect to domain restrictions, we can classify existing solutions into those which are
completely independent of domain (e.g., KnowItAll, LEILA, ISOLDE~\cite{isolde}) and those which are tailored
to particular domains (e.g., SOBA, Artequakt). The domain independent solutions are usually designed
so that there is no particular dependence tied to a domain, hence, the are reusable in several domains.
However, this means one system can be used in one domain or another, but it doesn't necessarily
means that the same system can learn a piece of knowledge of two different
domains \emph{simultaneously}.

If a system is simply designed to be domain-independent, and used to learn from two very different
domains, the expected result is some sort of combined ontology that represents both domains with
an approximation of the union of the concepts (entities and relations) therein.
This might not be the ideal representation, specially when extending this to many different domains.
Trying to build a single ontology that encompasses all the knowledge that can be mined from
several different domains (possible in the order of hundreds or thousands) can be significantly
more difficult than a simple summation or union of each of the single domains.

Our approach to multi-domain coverage is different. Even though the case study presented only involves
one ontology, our framework is designed to learn a new ontology every time
a new domain is discovered. This new ontology can be potentially merged into with
existing ontologies, but it can also simply be stored as a brand new piece of usable knowledge.
In time, the system would accumulate different ontologies for different domains.
When the need arises to use such knowledge %(e.g. to perform automated question answering),
the framework would decide which ontologies are relevant in a particular task. % (i.e., a particular query).
This in turn requires a more complex processing since an ontology merging approach could be
necessary to compute a final output for a task that involves multiple domains. However, we believe this increased
complexity is an affordable cost compared to the expressive power gained by representing as much knowledge
as possible without the restrictions of having a single taxonomy, or a single set of inference rules.

This belief is in part inspired by how the expert knowledge works in humans. It is true that
humans have a basic set of skills which could be considered domain-independent, such as our innate
abilities for pattern matching. These basic skills are used across many of the tasks we encounter daily.
The computational analogy is a system with a single general purpose learning algorithm that could
perform, for example, tasks as dissimilar as  speech recognition, image classification and translation
with the same process.
This is the preferred approach for a part of the community of researchers in machine learning~\cite{kaiser2017one},
who expect to build a general purpose intelligence out of a single general purpose learning algorithm
and a single general purpose internal representation.

However, for really complex tasks, we argue that humans use specialized representations,
learning algorithms, and inference techniques. A human expert in a highly complex domain (such as mathematics),
doesn't use the same techniques for inference, than those used in real time tasks such
as object and speech recognition. We acknowledge that inference in highly complex domains cannot be
performed with intuition-based tools. Building an intelligent
system that can perform inference in a number of highly complex domains simultaneously will
require using specialized representations and techniques in each domain. It is towards this
principle that we guide our design decisions.

  \section{Discusión}
