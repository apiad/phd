\chapter{Estado de la Cuestión}\label{Chap:SOTA}
\markboth{\MakeUppercase{Estado de la Cuestión}}{}

Intro del estado del arte

- el descubrimiento de conocimiento en lenguaje natural

Humans consume the ambient information and transform
it into independent concepts, that can be related to each other. In general,
we only store a small part of all the information we process every day.
This information consists of the most important concepts and relations,
that can be useful in future situations.
Thus, we learn basically by separating what could be
considered noise from the information that we think will be useful in the future.
In general we said two types of memories: one for recent knowledge and 
other where we stored the knowledge for the long time.

The design of complex machine learning algorithms and techniques necessarily
to mimic human behaviors have been there since the beginning of the artificial intelligent.
This work presents and initial approach to the problem translate the 
humans experience in the reading process whit the objective to
execute the learning, to the machine. 
For this we are facing the challenge to process text for detect elements
and interactions between ours. Moreover, it is  necessary research the most better 
proposal of artificial intelligent to transform the information in knowledge
and them represent this knowledge whit semantics.    
In particular we analyze the process of
creating a representation of knowledge (in the form of an ontology)
obtained. 

Nowadays, one of the biggest information sources in Internet
is text in natural language, which appears in news, opinions, messages, encyclopedias, 
and many other forms of digital communication.
This is not accidental, since words~(computationally represented as text) 
are one of the most important communication channels throughout the human history.
One of the most basic concepts recognizable in natural language that provide
useful information are actions. These are generally associated with two
distinctive concepts, the subject and the target. The first is who performs
the action, and the later is who receives the consequences of said action.
Correctly identifying actions, and their corresponding subjects and targets
is one of the most basic and fundamental challenges in order to understand
the information.

--------------------------

The accelerated growth of Internet has produced a surplus of information (news, email,
social media, blogs) that greatly exceeds our capacity for processing and consuming data.
Thus, building automatic systems that can extract knowledge from this flow of information has
become of the most active research fields in Computer Science. This phenomena has been
dubbed Big Data~\cite{bigdata}, and its study has attracted the attention of different 
research communities 
as business intelligence~\cite{chen2012business}, engineering including physical and 
biological~\cite{wu2014data} and social media~\cite{shah2015big}.

In recent years, researchers in fields such as machine learning, knowledge discovery, data mining and
natural language processing, among others, have produced many approaches and techniques to
leverage the large amount of information in the Internet for a variety of tasks, from
building search~\cite{google} and recommender systems~\cite{youtube} 
to improving medical diagnostics~\cite{watson}.

Among the different approaches relevant to knowledge discovery, we can recognize a
continuous spectrum of techniques, based on how much expert knowledge is used. 
Heavily knowledge-based techniques are based 
on rules defined in knowledge bases handcrafted by domain experts~\cite{chandrasekaran1986generic}. 
These approaches have a great degree
of reliability and precision, and generally allow for more complexity in the extracted knowledge,
but are difficult to scale to large amounts of data.
In contrast, the statistical approaches consist of techniques based on pattern recognition with statistical
and probabilistic models~\cite{kevin2012machine}. These techniques scale better with large amounts of data~\cite{le2013building}, 
providing better recall, but often are limited to extracting simple models of knowledge,
and can be more sensitive to noisy, fake or biased information~\cite{bolukbasi2016man}.

Given these mutually complementary characteristics, several hybrid approaches have been proposed.
Recently, research areas such as ontology learning~\cite{cimiano2009ontology},
learning by reading~\cite{barker2007learning} or entity embedding~\cite{hu2015entity} have arisen.
In these areas, researchers combine techniques from machine learning, natural language
processing and knowledge representation to solve more complex problems that cannot
be dealt with using only the classical tools.

Many machine learning systems are designed to solve a domain-specific task, such as
assigning a class to an element from a predefined set of labels. These systems,
when trained with data for a particular domain, are often not applicable to other domains
or to scenarios where several different domains must be used together. Moreover,
often systems are designed to be trained once from a corpus, and don't allow for
a continuous improvement of the knowledge learned.
Recently there are attempts to build general-purpose learning systems that are always
improving while obtaining new knowledge, re-evaluating the old knowledge and refining their
own confidence~\cite{mitchell2015never}.

Additionally, it is interesting to design non-monolithic learning systems, but instead
built as a set of modular components that can be combined in different ways.
This composability would allow a continuous learning system not only to improve the
quality of the extracted knowledge, but also to learn how to tune its own internal
parameters to perform a better knowledge extraction in the future. It is conceivable
that such a system could gradually learn which types of basic processes (i.e., entity recognition, POS-tagging, etc.)
are most useful for a given domain or for a particular corpus. Likewise, such a system could
learn which types of probabilistic models provide the best results in a particular dataset.


- importancia de la semántica

% PAPER ICAII

Humans try to represent the world we live in and facts and events that happen using language. Language enables ideas to be structured so that they can be shared them with other people.
Generally, one of the most common ways of representing arbitrary pieces of knowledge is through the notion of concepts and actions~\cite{teleologies}.
By understanding the world as composed of concepts interacting with each other through actions, humans can communicate a large variety and
complexity of information and knowledge.
The simplest way of combining concepts and actions is through binary relations, where one concept acts as the subject and another as the target.
This simple structure is very common in human languages.
More than 75\% of languages have a syntactic structure that follows either the SVO (subject-verb-object) or SOV (subject-object-verb) order~\cite{cambridge}.
Also, Subject-Verb-Object is the most common order in Creole languages, which has been suggested as an indication that is the most natural order
for human psychology.~\cite{diamond2013rise}.
As an example, from the sentence ``Arnold drives a car'' and we obtain an Subject-Verb-Object triplet \verb|(Arnold, drives, car)|.

This structure is ubiquitous in many different problems in the field of Natural Language Processing.
Different authors have employed a Subject-Verb-Object for similar structure or representing knowledge extracted from natural text, in a variety of domains~\cite{never-ending-learner,emotinet}.
Thus, a representation based on Subject-Verb-Object triplet is suitable for the generic representation of knowledge, independently of the domain.

Ontologies are one of the most common representations of knowledge in a digital format.
Ontologies, knowledge bases, semantic nets, and similar computational representations, are often built on the concept of the Subject-Verb-Object triplet, even if the individual components are referred to by using different terms (i.e., relations in RDF triplets or predicates in logic knowledge bases).

The rise of the semantic web has encouraged the creation of large scale ontologies and knowledge bases in several domains.
Some ontologies represent general domains, such as DBPedia, and collect a large proportion of common human knowledge.
Others, such as Ivanovic and Budimac~(\cite{IVANOVIC20145158}) operate in a more specific domain but incorporate more detailed facts about the domain.
One of the biggest obstacles to the development of the semantic web is the amount of effort and time it takes for human experts to build ontologies by hand~\cite{gomez2006ontological, petasis2011ontology}.
In this sense, the field of ontology learning~\cite{buitelaar2005ontology} studies the techniques and methodologies that enable the automatic or semi-automatic extraction of ontologies from unstructured sources of information, such as natural text~\cite{mitchell2015never,emotinet}.

One of the most important issues in ontology learning from natural text is how to recognize which knowledge is relevant in a domain. Usually, in a corpus of natural text, a large portion of the information will be spurious or unimportant~\cite{Kanya2009InformationE}.
Humans have an innate ability to forget the spurious or unimportant facts that are presented to us daily, and only store for long term that piece of knowledge that is relevant for a particular purpose.
Several relevance metrics have been proposed~\cite{manning2008introduction, brank2005survey}.
In general, the most relevant knowledge can be related to the actions and concepts that appear most often in a domain.

% PAPER JBI

The exponential growth of the Internet in the last decades has produced a massive surplus of textual information in all areas of human endeavor. This scenario presents both an opportunity and a challenge for researchers. On the one hand, a growing body of scientific literature is readily available, where potential solutions for critical problems could be found by linking partial results published in distinct documents. On the other hand, the extent of the information available cannot be processed by humans alone in a reasonable time frame. Hence,  efforts have recently been directed towards designing automatic techniques that can discover relevant pieces of information in large corpora, make logical connections, and synthesize useful knowledge.
The first step in many of these techniques involves the collection, processing and annotation of data that can be used to train machine learning algorithms or build expert systems through the use of natural language processing techniques.

The digital health sector is of great interest to the research community given the potential social benefits derived from applying automatic knowledge discovery technologies. The research community has produced a large number of annotated corpora in different sub-domains of this sector, from specific (e.g., drug-disease~\cite{goldberg1996drug} or gene-protein interactions~\cite{tanabe2005genetag}) to broad in scope and domain (e.g., clinical trial reports~\cite{nye2018corpus}).
Domain-specific corpora and technologies are of critical importance in high-precision medicine.
However, systems built for very specific domains are arguably harder to generalize and extend than systems built on general-purpose conceptualizations.
As such, there is a growing interest in designing annotation models and corpora with general-purpose semantics that can be used in a variety of domains or as a component in more specialized systems.

Besides domain, language is another dimension that has been the focus of recent research.
Most of the largest linguistic resources are based on English sources, motivated in part by the abundance of available raw material~(e.g., online encyclopedias, research papers), which is not surprising given that English is the most predominant language in science, technology and communications.
However, English-based resources are not always directly applicable to other languages.
Even though automatic translation has reached impressive accuracy in open domains, it is still a challenge to create cross-language resources, such as with Spanish, which is less predominant in technical domains~\cite{villegas2018mespen}.
Instead of focusing on specific niche languages, one possible line of research is designing resources that are
language-agnostic, in the sense that they can be generalized to multiple languages with little effort, by virtue
of being based on underlying common characteristics shared by many languages.

Designing annotation models that can generalize to multiple domains requires deciding on a basic representation of language that covers a broad range of semantics.
Moreover, these representations should be as independent of syntax and grammatical rules as possible, if they are
expected to generalize to multiple languages.
Recent work~\cite{estevez2018gathering} suggests that Subject-Action-Target triplets can be used to detect a large number of semantic interactions in natural language, independent of domain and relatively independent of language, since
more than 75\% of human languages employ some variation of the Subject-Verb-Object grammatical structure~\cite{crystal2004cambridge}.
Likewise, several ontological representations often agree in a number of general-purpose relations, (e.g., \textit{is-a} hyponyms, \textit{part-of} holonyms) that are useful in any domain.
Other conceptualizations allow the capture of semantics closer to natural language, such as Abstract Meaning Representation, AMR~\cite{banarescu2013abstract}.
The construction of corpora annotated with general-purpose semantic structures like Subject-Action-Target and high-level ontological relations is the first step in the design of systems that can discover knowledge automatically in a variety of domains and scenarios.

Research in knowledge discovery requires not only linguistic resources~(e.g., annotated corpora) but also computational resources and infrastructures that enable researchers to systematically evaluate their results and compare them objectively with alternative approaches.
This involves the formal definition of tasks and the design of objective evaluation metrics that ensure fair comparison is possible.
Even better is a publicly available evaluation system where researchers can submit their results, guaranteeing the same evaluation criteria is applied and freeing researchers from reproducing the evaluation environment. Such a system would also guarantee a more transparent and reproducible research process, and would provide a centralized repository of existing approaches, helping new researchers to update on the state-of-the-art.

% RESUMEN DEL PROCESO

- explicar el proceso y las tareas
  - definir un esquema de anotacion, modelo semántico, que sea bueno
  - herramientas con las que hacer la anotacion
  - anotación asistida y utilidades para anotar mejor
  - anotar un corpus con metricas de calidad y merging, hablar de la metodologia de anotacion
  - entrenar sistemas de machine learning para la extraccion automática
  - diseñar entornos de evaluación (challenges) para comparar sistemas
  - construir ontologías

  \section{Modelos Semánticos de Anotación}

  Knowledge discovery is a field of computer science that shows an accelerated growth in the past three decades.
  Advances in this area have been applied in many domains, from databases~\cite{fayyad1996data, knowledgeDatabase} to
  images~\cite{lu2016visual} and natural language text~\cite{carlson2010toward}.
  Specifically in natural language text, this field is highly relevant in the biomedical and health domains,
  where it is used for performing tasks such as
  Named Entity Recognition~(NER), Relationship Extraction and Hypothesis Generation, among others.~\cite{simpson2012biomedical}.
  These tasks generally use annotated corpora for learning the characteristics that appear in the text and mapping them to knowledge structures.
  For each task, specific annotation models have been designed that focus on specific elements of the text.
  For example, in NER tasks is more important to focus on nominal phrases than other grammatical constructions.

  Despite that these domain-specific tasks are different, most of them share common characteristics. For example, most tasks deal with the detection of relevant entities and their relations. Hence, promoting general-purpose annotation models would allow the design of reusable and cross-domain knowledge discovery techniques.
  In this line, several domain-independent semantic representations have been developed~(e.g., AMR~\cite{amr}, PropBank~\cite{propbank}, FrameNet~\cite{framenet}).
  However, these representations rely heavily on fine-grained lexicons that define specific semantic roles for each word meaning. Therefore, developing knowledge discovery systems with this level of detail supposes great challenges. Using more coarse-grained semantic representation, even with the loss of some representational capacity, would simplify the creation of automatic techniques based on machine learning.
  This representation could also be used as the first stage in a pipeline for a domain-specific task, thus reusing resources and techniques in domains with few available resources.
  In this section we present a review of relevant annotation models from which we draw inspiration.
  We focus general-purpose annotation models~\ref{sec:general} as well as on annotation models that have been applied to the health domain~\ref{sec:health}.

  \subsection{General-purpose annotation models}\label{sec:general}

  Several general-purpose semantic annotation models have been developed, that attempt to represent the semantics of a sentence beyond the syntactic structure.
  These models are loosely based on the Subject-Verb-Object grammatical structure that is pervasive in human language.

  PropBank~\cite{propbank} proposes a general purpose annotation schema, based on annotating predicates (verbs) as the main semantic constituents of a sentence. ProbBank's annotation schema is able to represent several semantic relations, including the agent that causes an action, the receiver of the effects of an action, time and location modifiers, and causal relationships.
  One key characteristic of PropBank is that every predicate defines custom semantic roles, i.e., the predicate ``\textit{accept}'' defines roles for the agent who accepts~(\texttt{ARG0}), the object that is accepted~(\texttt{ARG1}), and the agent from whom that object is accepted.

  FrameNet~\cite{framenet} is a lexical database and an annotated corpus that models the semantic roles and relations in a natural language sentence through conceptual structures named \textit{frames}. Frames represent general-purpose concepts, or events, that define the possible semantic relations in which those concepts can be realized in natural language.

  VerbNet~\cite{verbnet} is a verb lexicon that also defines specific semantic roles for each verb. In VerbNet, verbs are organized in a hierarchy, and linked through different thematic roles, such as agents, cause, source, or topic. These elements allow to capture the semantic representation of sentences.

  PropBank semantic roles are similar to the thematic roles defined in VerbNet and frame elements in FrameNet. As such, there are resources that link these semantic structures~\cite{semlink}.

  A more recent proposal is Abstract Meaning Representation~\cite[ARM]{amr}. AMR constitutes a semantic representation schema for English sentences that also attempts to cover a wide range of semantic relations with a general-purpose model.
  AMR includes PropBank semantic roles, as well as coreference resolution within the same sentence, named entities and types, negation, and other modifiers in a graph structure that represents the meaning of a natural language sentence.
  However, even though AMR captures the full semantic meaning of a sentence, for the purpose of knowledge discovery it is still considerably abstract, and additional processing is necessary to extract concrete structures of knowledge~\cite{rao2017biomedical}.

  The annotation model proposed in this research shares similarities from general-purpose semantic annotation models such as AMR and PropBank.
  In contrast to these resources, our model makes no distinction between different types of actions, which are loosely related to verbs, as explained in Section~\ref{sec:model}. Instead, we define two general-purpose roles, the agent that performs and action, and the receiver of the effects of the action. These roles roughly correspond to \texttt{ARG0} and \texttt{ARG1} respectively in PropBank, although in specific cases their semantic meaning might differ.
  This simplification is directed towards enabling the automation of the annotation process with the use of machine learning techniques.
  Another key difference of our model is the inclusion of general-purpose taxonomic relations~(e.g, \textit{hypernomy}/\textit{hyponomy} and \textit{meronym}/\textit{holonym}) that are inferred from the sentence. These relations are directed towards easing the automatic construction of knowledge bases.

  \subsection{Annotations models in the health domain}\label{sec:health}

  Knowledge discovery tasks in the health domain are often supported by the construction of manually-annotated corpora.
  Several task-specific annotation models have been developed for this purpose. One example is the  {DrugSemantics} corpus~\cite{moreno2017drugsemantics} where product characteristics are annotated, and  {BARR2}~\cite{barr2} which is concerned with biomedical abbreviations.
  Many corpora include specific types of named entities relevant to the medical domain, such as {DDI}~\cite{ddi} which annotates drugs and other substances.
  Other examples include {i2b2}~\cite{i2b2} which annotates medications, dosages and other details of drug administration and  {CLEF}~\cite{clef} which annotate different types of conditions, devices and their results in specific clinical cases.
  Given the specificity of the annotated concepts, most of these resources are built by biomedical experts.

  The previous examples are corpora helpful in designing techniques oriented towards narrow tasks,
  where the annotation model is specifically designed to only consider portions of the text relevant to the concepts of interests (i.e., medical entities, genes, etc.).
  An alternative approach that attempts to model a wide range of the semantics of a document is {Bio-AMR}~\cite{bioamr}.
  This corpus contains health-related sentences annotated with their AMR structure, a general-purpose semantic representation of natural text.
  Another relevant resource is BioFrameNet~\cite{bioframenet}, an extension to FrameNet with specific semantic roles for the biomedical domain.
  A positive consequence of using general-purpose semantic annotations is that it doesn't necessarily require experts in biomedical areas to participate in the annotation process.

  The {eHealth-KD} corpus~\cite{martinez2018overview} attempts to achieve a middle ground by representing a broad range of knowledge with a simple annotation model based on Subject-Action-Target triplets and 4 additional semantic relations.
  However, after the annotation process several shortcomings were identified.
  One example is the necessity for including {causality} and {entailment} as explicit relations, rather than representing them through actions, given the importance of this type of assertions in medical texts.
  Likewise, the annotation lacks the ability to represent coreferences (``\textit{this}'', ``\textit{that}''), and for this reason many sentences cannot be fully annotated.
  Also, complex linguistic constructions that represent composite concepts (e.g., ``\textit{the patients that received treatment}'') are difficult to annotate, especially when they participate in other relations.
  This paper extends the annotation model used by the eHealth-KD corpus with semantic elements used in general-purpose annotation models, such as AMR and PropBank.
  This extension allows solving the aforementioned issues and increases its representational power without adding an overly complex set of new semantic roles and relations.

  \section{Herramientas de Anotación}

  An important element to consider in Knowledge Discovery research is the existence of computational
  resources and infrastructure that supports the development of new approaches.
  The creation of linguistic resources often stems from a process of manual annotation by human
  experts, which requires computational tools for the actual annotation as well as mechanisms for merging
  annotations and computing agreement, ideally in a collaborative environment.
  Once the resources are created, it is necessary to distribute the corresponding corpus, baselines, and tools among the research community, often through online source code sharing platforms.

  An extensive analysis and comparison of several annotation tools is provided in~\citet{annotation-tools}.
  Table~\ref{tab:annotation-tools} summarizes the main characteristics we considered relevant for this research and identifies the most appropriate annotation tool among  a subset of  popular alternatives.
  We consider as requisites web-based, open source annotation tools that allow multi-label span annotations as well as relation annotations. Support for collaborative annotation, at least partially, is also highly desirable.
  Of the analyzed tools, we identified Brat~\cite{brat} and WebAnno~\cite{webanno}, as they comply with all the aforementioned requisites. In our research, we preferred Brat to WebAnno because, even though WebAnno provides more features, Brat allows an easier setup. It is not only faster to start an annotation project using this tool, but also to train annotators to use its interface.

  \begin{table}[htb]
      \centering
      \resizebox{\textwidth}{!}{
      \begin{tabular}{r|cccccccccccc}
          \textbf{Characteristics} & \rotatebox{90}{\textbf{GATE Teamware}} & \rotatebox{90}{\textbf{Knowtator}} & \rotatebox{90}{\textbf{WebAnno}} & \rotatebox{90}{\textbf{Brat}} & \rotatebox{90}{\textbf{BioQRator}} & \rotatebox{90}{\textbf{CATMA}} & \rotatebox{90}{\textbf{prodigy}} & \rotatebox{90}{\textbf{TextAE}} & \rotatebox{90}{\textbf{LightTag}} & \rotatebox{90}{\textbf{Djangology}} & \rotatebox{90}{\textbf{MyMiner}} & \rotatebox{90}{\textbf{WAT-SL}} \\ \midrule
          multi-label annotations &     &     & \ok & \ok &     & \ok &     &     & \ok & \ok &     &     \\ % F1
          relation annotations    &     & \ok & \ok & \ok & \ok &     &     & \ok & \ok &     & \ap &     \\ % F3
          allows custom model    & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok \\ %
          collaborative interface & \ok &     & \ap & \ap & \ap & \ap & \ap &     & \ok & \ok &     & \ap \\ % F10
          web-based interface     & \ok &     & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok \\ %
          can be self-hosted      & \ok & \ok & \ok & \ok &     & \ok & \ok & \ok &     & \ok &     & \ok \\ %
          open source license     & \ok & \ok & \ok & \ok &     & \ok &     & \ok &     & \ok & \ok & \ok \\ % T2
          citation                &\cite{gate}&\cite{knowtator}&\cite{webanno}&\cite{brat}&\cite{bioqrator}&\cite{catma}&\cite{prodigy}&\cite{textae}&\cite{lighttag}&\cite{djangology}&\cite{myminer}&\cite{watsl}\\
          \bottomrule
      \end{tabular}}
      \caption{Qualitative comparison of popular annotation tools. Adapted from Table 3 in~\citet{annotation-tools}, Table~3. A symbol~\ap~indicates that the corresponding feature is only partially supported.}
      \label{tab:annotation-tools}
  \end{table}

  The public distribution of annotated corpora and related resources, e.g., baselines, evaluation scripts,
  loading and formatting scripts, etc., is often enabled via open source code sharing platforms.
  Arguably the most popular options are Github\footnote{\url{https://github.com}} and
  Gitlab\footnote{\url{https://gitlab.com}}, which provide similar features despite minor
  differences in their core business models.
  It is also possible to share the corresponding resources via institutional hosting platforms or
  other ad-hoc solutions. This could be convenient in the case of legal requirements, complex licenses that are incompatible with open source idiosyncrasies or any other consideration
  that disallows full public sharing.
  In our case, all resources are publicly available in a collection of Gitlab repositories\footnote{\url{https://ehealthkd.gitlab.io}}.

  \section{Anotación Semi-Automática}

Machine learning, and specifically supervised learning, is one of the most effective tools for automating complex cognitive tasks, such as recognizing objects in images or understanding natural language text.
One of the main bottlenecks of supervised learning is the need for high-quality datasets of labeled samples on which statistical models can be trained.
These datasets are usually built by human experts in a lengthy and costly manual process.
Active learning~\cite{Cohn2010ActiveL} is an alternative paradigm to conventional supervised learning that has been proposed to reduce the costs involved in manual annotation .

The key idea underlying active learning is that a learning algorithm can perform better with less training examples if it is allowed to actively select which examples to learn from~\cite{survey}.
In the supervised learning context, this paradigm changes the role of the human expert.
In conventional supervised learning contexts, the human expert guides the learning process by providing a large dataset of labeled examples. However, in active learning the active role is shifted to the algorithm and the human expert becomes an oracle, participating in a labeling-training-query loop.
In the active paradigm,  a model is incrementally built by training on a partial collection of samples and then selecting one or more unlabeled samples to query the human oracle for labels and increase the training set.
This approach introduces the new problem of how to best select the query samples so as to maximize the model's performance while minimizing the effort of the human participant.

The simplest active learning scenario consists of  the classification of independent elements $x_i$ drawn from a pool of unlabeled samples.
Examples range from image classification~\cite{Gal2017DeepBA} to sentiment mining~\cite{Kranjc2015ActiveLF},  in which the minimal level of sampling (e.g., an image or text document) corresponds to the minimal level of decision. i.e, a single label is assigned to each $x_i$. More complex scenarios arise when the decision level is more fine-grained than the sampling level. In the domain of text mining, an interesting scenario is the task of entity and relation extraction from natural language text~\cite{zhang2012unified}.
In this scenario the sampling level is a sentence, but the minimal level of decision involves each token or pair of tokens in the sentence, and furthermore, these decisions are in general not independent within the same sentence.
In this case, it is not trivial to estimate how informative an unlabeled sample will be, since each sample has several sources of uncertainty.

This section reviews some of the most relevant research related with active learning in general, and specifically focused on entity detection and relation extraction.
One of the most important design decisions in active learning is how to intelligently select the novel unlabeled samples in the most efficient way. The underlying assumption is that we want to train a
model to the highest possible performance~(measured in precision, $F_1$, etc.) while minimizing the human cost (measured in time, number of samples manually labeled, or any other suitable metric).
This requirement is often framed as the selection of the \textit{most informative} unlabeled samples, and formalized in terms of a query strategy~\cite{survey}.
The most common query strategies for general-purpose active learning can be grouped into the following categories:

\begin{description}
\item[(i) Uncertainty sampling:] The most informative samples are considered those with the highest degree of uncertainty, given some measure of uncertainty for each sample~\cite{Lewis1994148}.

\item[(ii) Query by committee:] The most informative samples are considered those with the highest disagreement among a committee of either different models or different hypotheses from the same underlying model~\cite{seungquery}.

\item[(iii) Expected model change:] The most informative samples are considered those that produce the highest change in the model's hypothesis if they were included in the training set~\cite{NIPS2007_3252}.

\item[(iv) Variance and error reduction:] The most informative samples are those which produce the highest reduction in the model's generalization error or, as a proxy, its variance~\cite{roy2001toward}.
\end{description}

Expected model change (iii) and variance/error reduction (iv) strategies are heavily dependent on the specific learning model used.
In contrast, uncertainty sampling (i) and query by committee (ii) are  applicable in general with a high degree of model agnosticism.
Furthermore, relevant subsets of both strategies can be formalized under a single framework if we define the uncertainty as a measure of the entropy of the model's predicted output.
In this framework, query-by-committee can be implemented via weighted voting, thereby assigning empirical probabilities to the possible outputs.

Weighted density is a complimentary strategy in which the most informative samples are weighted by how representative they are of the input space, for example, by measuring their similarity to the remaining samples~\cite{settles2008analysis}.
This approach attempts to counter-balance a noticeable tendency to select outliers as the most informative samples ---a problem associated with other query strategies--- since outliers are often the samples that create the highest amount of uncertainty, disagreement or hypothesis change.

Recent advseplnances in natural language processing have produced an increased interest in active learning to alleviate the requirement for large annotated corpora~\cite{Olsson2009ALS, Tchoua2019ActiveLY}.
\citet{settles2008analysis} compare several strategies for active learning in sequence labeling scenarios, concluding that query strategies based on measures of sequence entropy combined with weighted sampling outperform other variants.
\citet{Meduri2020ACB} propose a comprehensive benchmark to evaluate different active learning strategies for entity matching.
In the task of named entity recognition, CRF models have been used to select query samples
\citep{Claveau2017StrategiesTS, Lin2019AlpacaTagAA}.
The task of relation extraction also benefits from active learning approaches, both in general-purpose settings~\cite{fu2013efficient} and in domain-specific settings~\cite{zhang2012unified}.
However, despite the growing body of research, it is still a challenge to apply active learning in joint entity recognition and relation extraction, especially in scenarios with low resources~\cite{Gao2019ActiveER}.

  \section{Recursos Lingüísticos}

  Different semantic relations have been established  in the state of the art, many of these giving rise to the construction of corpora. We focus on two approaches: corpora or annotation models to represent knowledge in many domains as well as those specifically about health.
  The table~\ref{tab:corpora}  presents the seven characteristics relevant to our corpus and indicates which of them are present in a sample of corpora from the state-of-the-art.
  These characteristics can be understood in the following terms:
  \begin{enumerate}
  \item \textit{general-purpose annotation:} applicability of the underlying annotation model to any domain;
  \item \textit{independence of syntax:} capturing semantic aspects rather than syntactic relations in sentences;
  \item \textit{ontological knowledge:} supporting inheritance and composition between concepts;
  \item \textit{composite concepts:} allowing the annotation of concepts that involve other sub-concepts;
  \item \textit{attributes:} modeling attributes for each annotated entity such as quantifiers~(e.g., number of occurrences) or qualifiers~(e.g., degree of certainty);
  \item \textit{contextual relations:} modeling relations that only occur when conditioned by a specific context; and,
  \item \textit{causality / entailment:} including relations for representing causality and/or entailment.
  \end{enumerate}

  \begin{table}[htb]
      \centering
      \begin{tabular}{ll|c|c|c|c|c|c|c|c}
          & \textbf{Characteristics} & \rotatebox{90}{\textbf{Ixa MedGS}~\cite{ORONOZ2015318}} & \rotatebox{90}{\textbf{DrugSemantics}~\cite{moreno2017drugsemantics}} & \rotatebox{90}{\textbf{DDI}~\cite{herrero2013ddi}} &
          \rotatebox{90}{\textbf{Bio AMR}~\cite{bioamr}} &
          \rotatebox{90}{\textbf{YAGO}~\cite{suchanek2007yago}} & \rotatebox{90}{\textbf{ConceptNet}~\cite{speer2017conceptnet}} & \rotatebox{90}{\textbf{eHealth-KD v1}~\cite{ehealth}} &
          \rotatebox{90}{\textbf{eHealth-KD v2}} \\ \midrule
          1 & general-purpose annotation &     &     &     & \ok & \ok & \ok & \ok & \ok \\
          2 & independence of syntax      & \ok & \ok & \ok &     & \ok & \ok & \ok & \ok \\
          3 & ontological knowledge      &     &     &     & \ok & \ok & \ok & \ok & \ok \\
          4 & composite concepts  &     &     &     & \ok &     &     & \ok & \ok \\
          5 & attributes        &     & \ok &     & \ok & \ok &     & \ok & \ok \\
          6 & contextual relations       &     &     &     & \ok &     &     &     & \ok \\
          7 & causality / entailment     & \ok &     &     & \ok &     & \ok &     & \ok \\
          \bottomrule
      \end{tabular}
      \caption{Comparison between the \textit{eHealth-KD v2} corpus and other corpora with respect to
      the characteristics that define our proposal.}
      \label{tab:corpora}
  \end{table}

  \paragraph{General-purpose annotation}
  General-purpose annotation models are often used in corpora extracted from encyclopedic sources, such as \textit{YAGO}~\cite{suchanek2007yago} and \textit{ConceptNet}~\cite{speer2017conceptnet}, both of which contain facts automatically extracted from Wikipedia~(among other sources). In contrast, domain-specific annotation models are usually employed when the source is more restricted to a specific domain. Examples include \textit{Ixa MedGS}~\cite{ORONOZ2015318}, which contains health related concepts for diseases, causes and medications; \textit{DrugSemantics}~\cite{moreno2017drugsemantics}, which annotates health entities, drugs and procedures; and, \textit{DDI}~\cite{herrero2013ddi}, which annotates drug-drug interactions. A middle ground is the \textit{Bio AMR}~\cite{bioamr} corpus, which applies a general purpose annotation model~(AMR)~\cite{banarescu2013abstract} to health documents. The \textit{eHealth-KD v2} corpus is similar to the latter in this respect, since the annotation model defined is general, but it is applied specifically to health sentences in this research.
  The \textit{eHealth-KD v2} corpus constitutes the result of the evolution of the \textit{eHealth-KD v1}~\cite{ehealth} corpus.

  Most of the aforementioned resources are focused on capturing the semantics of sentences, in the sense that very different sentences with the same facts are likely to be similarly annotated. We consider \textit{BioAMR} less independent of syntax because even though AMR is a semantic annotation model---far more abstract than dependency parsing, for example---, it still relies heavily on sentence grammatical structure. Hence, a significant change in the sentence structure is likely to change the annotation, even if the underlying semantic message remains unchanged. For example, since AMR uses PropBank~\cite{propbank} roles, changing a word for a semantically similar word, including a synonym, will probably change the corresponding annotation and thereby the available roles.
  This also makes AMR and similar resources language-dependent, not only in practice given their dependence on the existence
  of word banks, but also in nature. While attempting to apply AMR in Spanish, \citet{migueles2018annotating} show that even though AMR is theoretically language-agnostic,
  the existing annotation guidelines are biased towards English and must be adapted to capture linguistic phenomena
  that don't exist in English.
  The annotation model designed in this research for the \textit{eHealth-KD v2} corpus, attempts to achieve a higher level of syntactic independence, in part by using a smaller set of entities, relations and roles than AMR. More specifically, our annotation model does not distinguish semantic roles for each possible \texttt{Action}, instead relying on general purpose roles~(i.e., \texttt{subject} and \texttt{target}, see Section~\ref{subsec:model}).

  \paragraph{Ontological knowledge}
  General-purpose annotation models often allow ontological knowledge to be represented in the form of inheritance and composition between concepts. In this context, we consider the ability to recognize and annotate  these ontological relations in the source text. Health-related annotation models do not usually deal with this problem, mainly because the entities and relations to annotate form a predefined ontology where composition and hierarchy, if any exist, are already conceived in the annotation model itself. However, general purpose annotations often include relations like \textit{ConceptNet}'s \texttt{is-a} or \texttt{part-of} that directly represent these ontological concepts, and are thus able to extract ontological representations from natural text.

  \paragraph{Composite concepts}
  The model designed for the \textit{eHealth-KD v2} corpus also includes relations specifically for this purpose, mostly inspired by \textit{ConceptNet} and \textit{YAGO}.
  Composite concepts, in contrast, refer to the ability to annotate concepts that are formed by a fine-grained combination of other entities, in the same sentence. For example, take the sentence: ``\textit{the doctors that work the night shift get paid extra hours}''. \textit{AMR} allows for the representation of the concept that not all doctors, but only those that work the night shift, are the ones who get paid extra hours. Our proposal also includes several annotation patterns to deal with this type of scenario.

  \paragraph{Attributes}
  Attributes are often used to further refine the meaning of annotated entities. Examples include quantifiers in \textit{AMR}, or modifiers that specify a degree of uncertainty, or a negation of a concept. Our proposal includes four general-purpose attributes that model uncertainty, negation and qualifiers for expressing emphasis.

  \paragraph{Contextual relations}
  Contextual relations, as defined in the \textit{eHealth-KD v2} corpus, allow  facts that only occur
  under certain conditions to be represented, for example, in a specific time frame or location or under certain assumptions. This allows for a finer-grained semantic annotation. \textit{BioAMR} inherits this ability from \textit{AMR}, which allows modifiers for expressing \textit{how}, \textit{when}, \textit{where} or \textit{why} some event occurs. In our proposal, we provide contextual relations that specify time and location, and an additional general-purpose relation for other conditions.

  \paragraph{Causality and entailment}
  Causality and entailment are general-purpose relations that allow some level of inference or reasoning. The \textit{Ixa MedGS} corpus defines a \texttt{causes} relation, since it is relevant in the domain the corpus is modeling. Likewise, \textit{AMR} and \textit{ConceptNet} include similar relations. Our proposal includes both causality and entailment as two different relations with well-defined semantic meanings.

% PAPER EHEALTHKD 2019

  The accelerated growth of the Internet has resulted in a massive collection of scientific texts that are available online.
Several bibliographical databases exist, grouping academic texts from different domains, such \texttt{Arxiv.org}\footnote{https://arxiv.org} and
\texttt{Medline}\footnote{https://medline.gov},
which are two of the largest repositories, containing a vast amount of information that can be used by the scientific community.
However, its large size makes it impossible for human researchers to efficiently find useful results, definitions, or facts.
Even with the use of specialized search engines (such as Google Scholar), it is complicated to find relevant information in domain-specific documents.
This is due in part to the lack of a unified semantic structure in these documents, which are written in natural language.

To provide more fine-grained search results, documents can be processed to extract the relevant semantic entities and facts mentioned.
The task of automatically discovering semantic knowledge from text is covered by research areas
such as ontology learning~\cite{cimiano2009ontology} and {learning by reading}~\cite{barker2007learning},
whose purpose is to build semantic networks that {capture} the knowledge present in large collections of text.
These semantic networks enable the use of search engines that provide an analysis beyond the textual content's relevance, by exploiting the semantic structure of the network.
In this context, processing health textual contents has attracted {great interest}~\cite{gonzalez2017capturing}, motivated by the large number of medical documents published yearly.

Several approaches exist for building semantic representations of knowledge.
In many cases, these representations use a domain-specific conceptualization.
Although this provides a more specialized representation, it makes these approaches harder to apply to a broad range of domains.
Alternatively, a general purpose conceptualization could be used, which is able to represent entities and facts from multiple knowledge domains.
Such conceptualization should be general enough so as to accommodate many different domains, but still to provide a degree of expressiveness necessary for knowledge mining tasks.
One possible conceptualization is using
{Subject-Action-Target triplets}~\cite{suilan2018}.
This structure has proven to be useful for representing knowledge in both specific
domains such as movie reviews~\cite{suilan2018} or sentiment
mining~\cite{emotinet} and in {general domain ontology learning}~\cite{mitchell2018never}. %%%%CHANGE fix cite info
Furthermore, Subject-Action-Target triplets automatically extracted from text can be
later linked to domain-specific relations through the use of semantic networks.
As an example, the \textit{SemRep} system~\cite{semrep} extracts Subject-Predicate-Object
triplets from natural eHealth texts. The predicates are linked to specific relations
in the UMLS~\cite{umls} semantic network.

Recent work in the development of Teleologies~\cite{teleologies} suggests that Action-Subject-Target triplets can be the base for general purpose conceptualizations across many different domains, since this triplet allows the capture of interactions between objects through the actions they perform on each other.
A small set of semantic relations, such as \textit{hyponomy} and
\textit{holonomy} can provide additional semantic structure to theAMR
representation. These ``general'' relations are common in most knowledge bases,
regardless of domain, such as WordNet~\cite{miller1998wordnet},
DBPedia~\cite{lehmann2015dbpedia}, and ConceptNet~\cite{conceptnet}.
Other possible conceptualizations allow the capture of semantics of natural language,
such as Abstract Meaning Representation~(AMR)~\cite{amr}. Despite the superior representational
power of AMR over simple structures such as Action-Subject-Target triplets {and basic semantic
relations}, the annotation process for AMR is considerably more complex both for humans and automated techniques.

Building corpora annotated with the Action-Subject-Target structure is the first step towards the design of systems that can automatically extract these annotations. Several corpora exist in the literature, annotated with a variety of different schemes, such as CLEF~\cite{kelly2016overview}, Yago~\cite{fabian2007yago} and Emotinet~\cite{emotinet}.
However, most of these resources are annotated with domain-specific conceptualizations that are difficult to extend to different knowledge domains.
This paper presents a general purpose conceptualization and an example corpus\footnote{https://github.com/knowledge-learning/ehealth-kd}
annotated with such conceptualization, which demonstrates its ability to represent a wide variety of topics in a semantically rich structure.
Furthermore, a set of baseline implementations of machine learning techniques for automatically annotating similar sentences are presented\footnote{https://github.com/knowledge-learning/ehealth-kd/tree/master/baseline}.
Based on these resources, an ongoing online evaluation is available for researchers\footnote{https://competitions.codalab.org/competitions/18188}.

  \section{Sistemas de Aprendizaje Automático para el Descubrimiento de Conocimiento}

  \section{Entornos de Evaluación Competitivos}

  A strategy often used to encourage research on a specific task is the organization of a shared
  evaluation campaign. In contrast with regular research, evaluation campaigns often have a fixed
  time frame, and evaluation resources are not fully disclosed~(e.g., gold annotations for
  test sets are hidden) to allow a fair comparison in a friendly competitive environment.
  In this section, we analyze relevant efforts for organizing evaluation campaigns for both the biomedical domain or for dealing with entity and relation extraction.

  Several online services allow researchers to organize machine learning challenges and competitions, providing automatic grading, user management, and other useful features.
  Kaggle\footnote{\url{https://kaggle.com}} is arguably the most popular choice, its main limitation for our purposes being that to host a challenge, organizers must contact the service providers. Possible alternatives are AIcrowd\footnote{\url{https://www.aicrowd.com}} and
  Codalab\footnote{\url{https://codalab.org}} which provide free options for challenge organizers.

  The CLEF eHealth Evaluation Lab has proposed several challenges in the biomedical domain, including
  named entity recognition~\cite{clef2013} and information extraction~\cite{clef2014} in English,
  and later editions in French documents~\cite{clef2015, clef2016}.
  In these challenges, medical reports from MEDLINE, EMEA and similar sources are annotated with disorders, medical terms, acronyms and abbreviations, which provide evaluation scenarios
  for several NLP tasks, including entity recognition, normalization and disambiguation.
  Another relevant task is proposed by~\citet{semeval2017-task9} in Semeval 2017, focused on AMR parsing and generation from biomedical
  sentences in English. Applying a general-purpose conceptualization, such as AMR, to specific domains encouraged participants to bridge the gap between developing generalizable techniques and applying domain-specific heuristics.
  However, AMR parsing is already a complex problem in itself, which can negatively impact on researcher participation in these challenges if they are not specialized in AMR.
  Simpler, general-purpose models can encourage a greater degree of participation given the easier entry curve.
  An example of the latter is the Semeval 2017 Task 10~\cite{semeval2017-task10},
  a challenge regarding keyphrase and relation extraction from scientific documents,
  with a simple model based on three entity classes and two general-purpose relations.
  This task received a much larger number of submissions than the former, even though both challenges where hosted on the same venue and aimed at similar audiences.

  As can be expected, English is the most prominently used language in NER-related challenges, given the larger
  number of available corpora and resources. However, important efforts have been devoted to fostering research
  in less prominent languages. Relevant to our discussion are the IberLEF campaigns that focus on Iberian languages,
  such as Spanish, Portuguese, Catalan, and other regional variations.
  Two examples of recent NER-related tasks are the Portuguese Named Entity~\cite{glauber2019iberlef} challenge and the MEDDOCAN~\cite{marimon2019automatic} document anonymization challenge.
  The first proposes entity recognition and relation extraction in the general domain, in Portuguese.
  The second proposes the identification of privacy-sensitive entity mentions in medical documents, e.g., names, addresses,
  dates, ages, etc.

  Outside the frame of a competition, open, long-running evaluation systems allow
  researchers to evaluate their approaches with official evaluation metrics.
  This can also provide a centralized repository of the state-of-the-art, where existing approaches are
  summarized and linked to existing papers.
  In this regard, this research proposes an online evaluation system that allows a comparison of
  new approaches with officially published results at any time. Based on this infrastructure, official evaluation campaigns with a more competitive design are organized in scheduled time-frames.

  \section{Representación del Conocimiento}

  There are many resources used in the task of
natural language processing. Among the most common ones, ontologies, as representations
of concepts, data types and their relations in an explicit computational structure.
Some of the most popular ontologies used in NLP are WordNet~\cite{} and DBpedia~\cite{}.
For their ubiquity and usefulness, research in ontologies has increased in the last years,
particularly regarding the automatic creation of these structures,
giving birth to the field of Ontology Learning.
Ontology Learning has the potential of reducing the cost of creating
and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.

Ontology Learning deals with the problem of automatically creating an
ontology from several resources. Examples include mining analysis, mostly
the study of text in natural language~/cite{}. In this
particular task, several approaches have been presented, from syntactic and
semantic analysis of corpora~\cite{} to statistical and machine learning based
approaches~\cite{}. 

Several tools and systems have been proposed for this task. 
Text2Onto~\cite{cimiano2005text2onto} is a framework for
data-driven change discovery which employs a probabilistic 
ontology model (POM).
OntoLT~\cite{buitelaar2004ontolt} extracts concepts and
relations automatically from linguistically annotated text collections,
by means of rule set, which maps linguistic classes to ontology classes.
A newer approach is OntoGain~\cite{drymonas2010unsupervised}, 
which uses an unsupervised approach,
and exploits the inherent multi-word term's lexical information
in order to extract higher level concepts.

One of the issues with these approaches is the amount of spurious
information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
In general, there will be many unimportant
or redundant pieces of information in the analyzed corpora. A naive
approach that doesn't take this issue into consideration will create
immense ontologies with very little useful information. To tackle
this problem, OntoGain proposes a hierarchical clustering scheme
that attempts to identify general concepts and relations.

In general these resources are focused in the extraction of knowledge and
pay much less attention to the task of find relevant knowledge. 
An ontology is interesting not just because of its size, but 
because of how is such information selected, processed and stored.
In this problem we focus our work, attempting not only 
to recognize information, but also to reduce this information and obtain
information with more relevance.

-----------

The problem of discovering, storing and using knowledge in a computationally effective
form has been widely studied~\cite{mitchell2015never, ROSPOCHER2016132, cimiano2009ontology}. 
This problem has been treated from two different but complementary research areas:
the fields of knowledge representation and machine learning.
The knowledge representation community provides means for
computationally representing and operating with stored knowledge in forms that
can ensure some degree of logical consistency. 
Conversely, the machine learning community provides
tools for obtaining useful knowledge from large collections of structured and unstructured data.
Recenlty, a new discipline named ontology learning has arisen, which draws ideas
and techniques from both the knowledge representation and the machine learning fields.
This field deals with the problem of automatically building ontological representations
of knowledge from a variety of data sources. As such, the theory of ontology learning
is relevant in the design of the framework presented in this paper.
In this section we present a brief review of the relevant concepts and technologies
in each of these three domains, as well as some approaches similar to our proposal.

	\subsection{Knowledge Representation and Reasoning}

    Since the dawn of computer science, one of the problems that has attracted wide attention
    is that of representing knowledge in a computational format, such that automatic reasoning
    can be performed to discover new, previously unknown truths~\cite{sowa2000knowledge}.
    Arguably, the most popular knowledge representation technology in use are
    ontologies~\cite{guarino1995formal}, which have
    become the \emph{de facto} standard.
    %Definición
	Ontologies can been defined as a formal specification of a conceptualization~\cite{asuncion2003}.
    This represents concepts, relations between these concepts, instances of these concepts and inference rules
    for deriving new relations. 

    As such, ontologies can be considered as a combination of two predominant approaches 
    for knowledge representation: those based on formal logic~\cite{brachman1992knowledge}
    and those based on graphs of semantic relations~\cite{chein2008graph}.
    In logic-based approaches the facts are represented as logic predicates or functions 
    and reasoning is enabled through the application of formal inference rules.
    In contrast, graph-based representations express facts as nodes~(objects)
    and edges~(relations) and reasoning is built on top of graph traversing methods.
    However, in ontologies objects and their attributes and relations are represented in a graph of concepts, 
    which can also be interpreted as a set of predicates and functions on these objects. 
    On top of this layer, inference rules can be added,
    which enable logical reasoning methods to be used for deriving new attributes and 
    relations between existing concepts.
    
	Relations in an ontology can be of a specific domain, but often some general domain 
    relations are represented, such
    as \textit{is-a} and \textit{part-of}. These types of relations allow representing more abstract
    or complex concepts out of the composition of more concrete or simple concepts.
    Hence many ontologies contain some kind of taxonomy of increasingly abstract concepts, 
    that are also interconnected
    with each other using other semantical relations which can be domain specific.
    % Ventajas
    These resources allow representing complex frameworks of knowledge, down to a degree of specificity which
    enables the design of fully automated reasoning tools which use this knowledge for 
    a variety of computational tasks.
    % Dificultades o desventajas
  	Due to the high complexity of the concepts and relations that are represented,
    and the experience needed to recognize the most relevant concepts of a domain,
    ontologies are usually manually constructed by domain experts~\cite{wong2012ontology}.
    Thus, building an ontology is a process that requires a long time and a large number of experts to
    define and populate it with relevant instances that refer to objects and relations.
    This makes really difficult build hand crafted ontologies and ensure their maintainability, due to day a day huge quantity of new and valuable information, desirable to convert into knowledge, appear in the World Wide Web.
    Another important outcome of this process is that experts usually represent only facts that are absolutely
    true in the domain. Although existing ontology formats can be extended to deal with fuzzy~\cite{fuzzyontology} or
    vague data~\cite{bobillo2011fuzzy}, manually assigning a degree of belief to a specific fact is a complex task.

%     % Ejemplos
%     It is possible to distinguish between two types of ontologies: general domain (or upper ontologies) and domain-specific (or simply domain ontologies).
%     Domain-specific ontologies are those which deal with the concepts and relations of the particular knowledge domain.
%     As examples, we can cite ontologies in the medical sciences~\cite{rector2003opengalen,gene2004gene},
%     or the software engineering field~\cite{4641930}.
%     Other ontologies are more general since they can be used in different domains, or they are used for general
%     purpose tasks which are employed in many areas.
%     WordNet~\cite{miller1995wordnet} is a general purpose ontology that contains most of the words of the English language,
%     and syntactic and semantic relations between them.
%     It is used in many tasks in natural language processing and text mining.
%     DBPedia~\cite{mendes2012dbpedia} is an encyclopedic ontology that contains part of the knowledge present in the
%     Wikipedia\footnote{http://www.wikipedia.org}.
%     It relates people, historical events, facts, locations, and other concepts, in a structured and queryable format.
%     Since ontologies have a unified form of representing a single fact, concept or relation using Uniform Resource Locator~(URL), it is
%     possible and very common for different ontologies to link each other. For example, many domain specific ontologies
%     have entities which are linked to the corresponding entry in DBPedia. The approach of linking and referencing to other
%   	widely known ontologies, known as \textit{linked data}~\cite{bizer2009linked},
%     enables the standarization of the representation of shared knowledge and eases the tasks of querying and analyzing it.

    Ontologies are an effective tool for representing knowledge in a wide
    variety of domains and scenarios~\cite{staab2010handbook}.
    They are flexibly enough to adapt to a particular domain and powerful enough to represent complex concepts.
    However, one of the most complex task in this sense is maintaining
    an ontology up-to-date with respect to the massive amount of unstructured data that is generated and published every day.
	Therefore, the need arises for computational tools to build ontologies with automated or semi-automated processes.

	\subsection{Machine Learning}

%	The speed and volume at which information is produced has increased exponentially in the last decade,
%    mainly due to the rise of the social networks and the mobile technology. In order to cope with this volume of information,
%    it is necessary to be able to process massive amounts of data continuously.
    The field of machine learning provides tools for the automatic extraction of information and knowledge from different
    sources of data. 
    Machine learning not only allows to automate processes and tasks of knowledge discovery or text mining, but also provides a large
    improvement in the scalability of these processes~\cite{wu2014data}. By using mass computing resources, it is possible to process millions
    of raw documents in a reasonable time, far exceeding what can be done by domain experts.
    Recent improvements in computing capabilities and access to larger datasets have given rise
    to the field of deep learning, which has improved the state of the art in
    several of the classic machine learning tasks~\cite{lecun2015deep}.

	Arguably, the two most common approaches in machine learning are supervised and unsupervised learning~\cite{kevin2012machine}.
    Supervised learning can be used for recognizing specific elements of knowledge in a source of data. For example, tagging
    pieces of text to indicate that they define an entity~\cite{nadeau2007survey} (e.g., a person, organization, or place), recognizing relations between
    said entities, or assigning a sentiment or opinion score~\cite{liu2012sentiment} to a fragment of text. On the other hand, unsupervised learning
    can help with finding relevant structure in a large set of elements. Clustering algorithms can be used to detect similar
    concepts or to extract abstract concepts from groups of more concrete elements. Other techniques can be used for reducing
    the amount of information, for example, to remove noisy, uncertain or irrelevant pieces of
    information~\cite{bingham2001random}.

    In general, most machine learning algorithms are not designed to represent the learned knowledge in complex structures,
    such as those defined by human domain experts (i.e., ontologies). In turn, the representations often have a simple structure, such
 	as a probability distribution or a correlation matrix~\cite{bengio2013representation}.
    When applying these algorithms to a real problem, an domain-specific
    interpretation of those representations has to be made. 
    % Esta es una debilidad imporante, la no explicación
    Furthermore, many of the most powerful machine learning models are difficult to explain, in the sense
    that when the system produces an answer, a human expert cannot easily understand and reproduce the inference steps
    that the system performs~\cite{olden2002illuminating}.
    % creo mejor decir que si la representacion es tan importante las ontologias son una forma de representar conocimiento
    % y entonces meter el gancho de embeddings y entity embeddings
    Choosing an appropriate representation is decisive for the success of most machine learning techniques~\cite{bengio2012deep}.
    In recent years, there has been an increased interest in the problem of automatically learning relevant representations.
    Word embeddings~\cite{mikolov} and more general entity embeddings~\cite{hu2015entity} represent the first steps towards powering
    deep learning approaches with more explainable internal representations.
    Since ontologies are, by definition, representations of a given conceptualization, it is conceivable that using ontologies
    as seeds for the representation of a given domain, the performance of data mining processes based on machine learning can
    be improved.
    \subsection{Ontology Learning}

    In the intersection of machine learning and knowledge representation,
      the field of ontology learning has arisen to deal with the 
      complexity of manually maintaining and updating ontologies.
      This field draws techniques and tools from both communities, 
      to automate part of the process of creating and maintaining ontologies.
    Ontology learning has the potential of reducing the cost of creating
    and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.
      This problem is also addressed in 
      learning by reading~\cite{barker2007learning}, a field which draws techniques
    the natural language processing and knowledge representation and reasoning communities.
    The purpose is to build a formal representation of some particular field given unrestricted
    textual data related to the field. This representation must also allow fully automatic
    reasoning.    
      Learning by reading can be considered as a particular case of ontology learning,
      even though it is only concerned with textual input, and the
      output is not necessarily formated as an ontology.
      
      In the field of ontology learning, two general high level tasks can be distinguished:
      ontology population and ontology enrichment~\cite{petasis2011ontology}.
      Ontology population deals with the
      sub-problem of finding new instances for an already defined ontology, while
      ontology enrichment deals with adding new concepts and relations to an existing
      ontology. There is an overlap between these tasks, and most of the existing
      approaches cannot be classified purely in these terms.  
      In this field, several tools have been proposed, which
      combine different approaches and solve different subsets of the ontology
      learning tasks. A brief review of these systems can help defining the 
      main characteristics that our framework should have.
      % No sé si decir algo como a continuación se presentan varias de ellas 
      % atendiendo a sus características mas relevantes. Esto nos permite conocer 
      %la diversidad de enfoques y las potensialidades que debemos tener en cuenta
      %para crear nuestro sistema
  
      % general (early approach)
      Early approaches, such as SYNDIKATE~\cite{syndikate}, deal only with populating a 
      knowledge base, with a predefined ontological structure~(classes and relations).
      % web approaches
      Since the Web is a rich source of information, several approaches have focused on extracting knowledge
      from it, exploiting the semi-structured format of web resources.
      Some systems like ARTEQUAKT~\cite{artequakt} and SOBA~\cite{soba} are domain-specific,
      respectively focusing on the art and the sports domains.
      Other systems, like WEB-$>$KB~\cite{webkb} attempt to build general domain knowledge bases from the 
      web, exploiting also the structure of links between pages to identify relations.
      % from structured data
      Another example is the VIKEF~\cite{vikef} system, which uses product catalogs as sources of data, hence exploiting the
      inherent structure present in this type of data.
      % bootstraping with human knowledge
      Even though most systems attempt fully-automatic extraction, some examples like ADAPTATIVA~\cite{adaptativa} include
      a bootstrapping strategy, where human experts provide feedback about the extracted knowledge.
  
      In order to extract relevant knowledge from unrestricted text, NLP techniques have
      been introduced in systems such as OPTIMA~\cite{optima} and ISODLE~\cite{isolde}.
      % rule-based
      The use of natural language features can be used build rule-based systems,
      like the OntoLT~\cite{buitelaar2004ontolt} proposal, that extract concepts and
    relations via a mapping of linguistic classes to ontology classes.
      % based on statistical models
      An alternative approach is to use statistical or probabilistic models,
      exemplified by systems such LEILA~\cite{leila} or Text2Onto~\cite{cimiano2005text2onto}.    
    % relevance
      Another example is KnowItAll~\cite{knowitall}, which introduces a point-wise mutual information
      metric to select relevant instances.
    
    Once instances of entities are relations are extracted from text, a natural question is
      wether more abstract knowledge can be inferred from these examples.
      The systems who address this issue often use unsupervised techniques to attempt
      to discover inherent structures. Two relevant examples of this approach
      are OntoGain~\cite{drymonas2010unsupervised} and ASIUM~\cite{asium}, 
      which attempt to automatically build a hierarchy of concepts using clustering techniques.
      % multimedia
      The BOEMIE~\cite{boemie} system is another interesting example, since it 
      attempts to automatically infer abstract concepts from the concrete instances found,
      but focuses not only on text, but also on multimedia sources such as images, and videos.
    % never-ending learning
      Most of the mentioned systems usually focus on one iteration of the extraction process.
      However, more recent approaches, like NELL~\cite{mitchell2015never}, attempt
      to learn continuosly from a stream of web data, and increase over time
      both the amount and the quality of the knowledge discovered.
      
    One of the issues with many of these approaches is the amount of spurious
    information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
    In general, there will be many unimportant
    or redundant pieces of information in the analyzed corpora. A naive
    approach that doesn't take this issue into consideration will create
    immense ontologies with very little useful information. To tackle
    this problem, OntoGain proposes a hierarchical clustering scheme
    that attempts to identify general concepts and relations.
  
    In general, these tools are focused on the extraction of knowledge and
    on the task of finding relevant knowledge.
      When the extracting knowledge from a trustworthy source, even if a natural
      language source, it makes sense to
      focus on optimizing recall, i.e., obtaining as much information as possible.
      If the input source is a set of medical papers, or the main
      web page of an institution, there is a high chance that most of the information
      present in those documents is correct. Hence, an ontology extraction procedure
      that maximizes recall will obtain good results.
      
      However, when the input source is of a lesser quality, such as blogs or
      social media posts, there is a greater chance that some, or even most,
      of the information is fake or incorrect.
      If we consider also the so called phenomena of the \textit{post-truth},
      and acknowledge that some authors are deliberately sharing fake news
      or facts, the problem becomes much harder, and pressing.
      Even if deliberate lies were not an issue, most of the information shared
      in social media and similar sources is irrelevant in a long term.
      In this context, the problem of extracting a useful ontology from a large
      corpora of Internet sources becomes less a problem of recognizing the pieces
      of information lying in the corpus, and more a problem of filtering and selecting
      the relevant information, once extracted.
  
      Despite the existence of some general purpose systems, 
      there is no proposal of an architecture for a computational framework
      that can simultaneously and continuosly learn from the most varied sources of information online.
      Another challenge in this aspect is to obtain a computationally convenient representation
      of this knowledge, independent of the domain, source and format of the input data.
      Furthermore, such system has to explicitly deal with the large amount of spurious,
      irrelevant, or deliberately fake information spread through web sources.
  
    \subsection{Quality Metrics}\label{sec:evaluation}

% 	Once a computational system is implemented following the guidelines of the framework
%     presented in this paper, it is necessary to perform some evaluation on its output.
%     However, being a complex system, with many interrelated components, evaluating it
%     is not as simple as comparing the actual output with the expected output.
    In this section we present a methodology for evaluating such a framework and obtaining
    interesting metrics that can validate its performance across the large range of
    tasks the framework is intended to enable.

    In a computational system or software framework, there are interesting
    software engineering metrics to evaluate. Such a system should be highly modular and
    extensible, so that it can be easily adapted to new input formats, or new algorithms
    can be easily plugged in and integrated into the whole pipeline. 
    The modular design of the framework can help in achieving a high
    degree of extensibility. 

	Besides these high level metrics, each of the tasks performed by the framework can be
    evaluated separately. Most of these tasks have a definite performance metric that can
    be used to rate the degree of correctness of such task. For many of the tasks described in the previous
    sections we can find standard performance metrics in the literature that can used to
    evaluate each particular process.
    
%     For example, a sensor for
%     extracting entities can be evaluated individually by running it on a tagged corpus
%     and computing precision and recall. 

    An aggregate metric of these individual performances could provide a high level
    overview of the performance of the whole framework.
    However, designing an aggregate metric that provides a practical, interpretable,
    measure of the quality of the framework performance, can be very complex.
    Each of the different tasks performed by the framework can have a very different
    baseline performance. A 90\% precision can be a very good result
    in some complex tasks, such as dependency parsing~\cite{AlbertiABCGKKMO17}, but mediocre in other tasks, such
    as image classification~\cite{Russakovsky2015}. Moreover, this baseline number can
    vary not only across tasks, but in the same task, according to which test suite (or corpus)
    is used.

    \begin{figure}[htb]
    	\begin{center}
        	\includegraphics[width=0.6\columnwidth]{Graphics/evaluation.pdf}
            \caption{Schematic representation of the process for evaluate knowledge.}
            \label{fig:evaluation}
        \end{center}
    \end{figure}
    
	Going up one level of abstraction, for the general problem of ontology learning there are also
    several evaluation metrics and methodologies available, such as OntoRand~\cite{ontorand} 
    and OntoMetric~\cite{ontometric}. However, most of these
    methodologies are designed for evaluating a single ontology that is either created or
    modified using techniques from ontology learning. Once more, extending these methodologies
    to a collection of ontologies is not as straightforward as aggregating or averaging
    the individual results. On the other hand, when dealing with a collection of ontologies,
    other concerns can arise, such as intra-ontology consistency, which are not usually
    considered when evaluating a single ontology. As a final consideration, the framework
    by design, is expected to maintain a degree of internal inconsistency in order
    to better cover multiple and possibly overlapping domains.
    We present some of the most commonly described approaches in the literature for 
    evaluating ontologies~\cite{petasis2011ontology}
    and describe how they can be used in our context.

    \paragraph{M1- Comparison with a gold standard.}
	This approach consists in comparing a learned ontology with a baseline ontology
    for the same domain~\cite{corcoglioniti2016frame}. The baseline ontology is
    assumed to be both correct and largely representative of the domain. This method
    provides a great trade-off of speed versus accuracy, since both ontologies
    can be automatically compared in a number of metrics without human intervention,
    and the results have a high reliability because the baseline ontology is created
    by experts. Some disadvantages do exist, for example, it is not always easy to
    find a good baseline ontology for a given domain, especially if the domain is
    not very well defined or is very novel. On the other hand, even two ontologies
    extracted from the same domain by experts can have wide differences with respect to
    the structure, and particularly to the names the classes and relations are
    assigned. This requires some form of normalization and mapping between both ontologies
    prior to comparison. This metric is difficult to use, particularly if
    we're creating new characteristics.

    \paragraph{M2- Expert evaluation.}
    An alternative middle ground to the previous approach is having a domain
    expert (or several) to simply look at the resulting ontology and evaluate
    it according to some predefined metrics~\cite{ROSPOCHER2016132}.
    This is arguably the most reliable method, in the sense that it provides
    the highest degree of validation one could aspire. However, the clear
    disadvantage lies in the limited amount of information that a human can
    process in a reasonable time. This disadvantage is worsened in the case
    where an ontology is created from a very large corpus of data, as is
    the purpose of our framework. In this case, a small subset of the data
    could be analyzed, and results extrapolated from there, but this idea
    adds the complexity of determining a subset that is relevant enough but
    remains of a manageable size.
    This metric is usually expensive or difficult to use.

    \paragraph{M3- Evaluation through an application.}
    A more practical approach consists in finding an interesting application and
    evaluating if the use of a learned ontology provides an improvement in that
    application~\cite{gurevych2003semantic}. For example, using a learned ontology
    about human feelings and related phrases to improve the performance of a
    standard opinion mining problem. If using the knowledge represented in the
    ontology provides a boost to performance, as measured by the standard
    approach in the given application, we obtain a reliable validation that
    the process for learning ontology, at least, has a measurable practical benefit.
    In some sense, this is one of the most valuable evaluations to perform,
    because it provides an immediate comparison baseline for a practical problem.
    The previous methods which only evaluate the ontology internally do not
    necessarily guarantee that its content will be useful, even if it's correct by
    all metrics. Another upside is that the evaluation process can be completely
    automated, and scaled to match the complexity and size of the target application.
    As a downside, validating a use case is not necessarily a metric for the overall
    quality of the learned knowledge, and it's not clear if those results will
    replicate in different domains and applications.
    This metric indeed seem more simple but in many cases not need
    improvement a task, but a wide set.

    \paragraph{M4- Data driven evaluation.}
    Finally, a data driven evaluation can be performed, by comparing the entities
    and relations in an ontology to a corpus of data, not used during the
    construction of the ontology, but representative of the same
    domain~\cite{brank2005survey}. The ontology can be evaluated by counting
    the number of overlapping entities present in it against those found in the corpus.
    Care must be taken to allow for some variation in the corpus with respect to
    the ontology, for example, using some form of query expansion.
    This approach has been used to relatively compare different expert-made
    ontologies against the same corpus and deciding which ontology provides
    the best ``fit'' with the corpus~\cite{brewster2004data}.
    However, obtaining an absolute metric of fit between an ontology and a
    corpus is harder, mainly because it is not known beforehand what is the
    value of fit one should expect to achieve.
    Another possible issue of this approach, in the particular case of
    ontologies that have been learned from the text, is to inadvertently
    introduce a bias in the evaluation. If the methods used to compare the
    ontology and the text corpus are correlated with those used to build the ontology,
    then the results will be of a dubious validity.
    For example, if one uses a NER algorithm during the building of the ontology,
    and the same algorithm is used in the corpus to recognize relevant entities;
    or if some co-occurrence metric is used for detecting relations in both cases.
    This metric is complex to define and on many occasions is unrepresentative.

    \vspace{1em}

    Evaluating a single ontology learning method is a complex task, as shown by the multiple approaches
    proposed in the community. Hence, it's is very unlikely we can find a single automated metric
    to measure the overall
    performance of a framework as the one proposed. The best approach seems to be using a combination of the
    existing methods, adapted to our scenario, with the added complexity of dealing with multiple ontologies
    at the same time. In some cases a gold standard can be found and used to obtain a baseline comparison. In
    other cases, provided a suitable interface for easily querying the knowledge is added, a domain expert can
    interact with the framework and give a qualitative evaluation for the domain of interest.
    From a pragmatical point of the view, the most interesting and valuable evaluation seems to be finding
    relevant practical problems that can be solved, or improved, when using our framework.

    %The phenomenon of the post-truth era brings additional challenges. In this context, we believe
    %that for the problem of knowledge discovery the task of evaluating the relevance of the knowledge
    %extracted becomes more important than ever before.
----------
    \subsection{Learning tasks disscusion}

In the ontology learning community, several frameworks have been developed which attack problems
similar to the ones we present. Some of the approaches found in the literature concentrate on one
particular task, i.e., ontology creation, population, or enrichment, among others. For example,
frameworks like KnowItAll~\cite{knowitall}, Artequakt~\cite{artequakt} and SOBA~\cite{soba} are oriented mainly towards the task of ontology
population. Others, such as ASIUM~\cite{asium}, VIKEF~\cite{vikef} and SYNDICATE~\cite{syndikate} are oriented mainly towards ontology enrichment.
However, many of the tasks or subproblems that need to be solved in either of these domains are
very similar and can be reused. Hence, more general frameworks such as Text2Onto~\cite{cimiano2005text2onto} or BOEMIE~\cite{boemie} have
arisen which deal with a combination of these tasks.
Our framework is designed to handle the common ontology learning tasks,
including ontology creation, population, enrichment, merging and mapping.
Each of these tasks is performed in one or more of the framework's main module,
not as independent or isolated features,
but as an integral part of the framework's learning process.

The module for processing unstructured data can be interpreted as an ontology
creation and population pipeline, because the input is raw unstructured data and the output is
a brand new ontology where everything from classes and relations to specific instances is
created automatically from scratch.
On the other hand, once these newly created ontologies are delivered to the module for knowledge
discovery, several processes are performed which can be interpreted as forms of ontology
enrichment, since new relations can be discovered and inference rules can be automatically defined.
Finally, in the module for processing structured data a process of ontology merging and mapping is
performed, to normalize the incoming ontologies against the knowledge already stored.

\subsection{Quantity and complexity}

Regarding the quantity and complexity of concepts recognized, the existing solutions can
be divided into those that only extract entities (e.g., KnowItAll), those who only extract
relations (e.g., ADAPTATIVA~\cite{adaptativa}, LEILA~\cite{leila}), and those which attempt to extract both (e.g., Artequakt, Web->KB~\cite{webkb}, BOEMIE).
Our approach corresponds to this last category, with the addition that it attempts to extract also
more abstract concepts via techniques such as hierarchical clustering.
The discovery of inference rules is another relevant task that we intend to integrate
in the module for knowledge discovery, as part of the process for generating new knowledge.
This process enriches the ontologies already built by adding higher-level knowledge, in the form
of logical predicates or axioms. These can in turn be used later for discovering missing instances
or relations, or for detecting outliers and mistakes.

\subsection{Use of machine learning}

Most systems employ some form of machine learning tools for most of the
tasks. In particular, many employ NLP tools to process natural text and extract pieces of knowledge,
and statistical techniques to detect clusters. However, in general, the architecture of these
systems generally follows a very strictly designed pipeline, where components are carefully
connected to each other. Our proposal is different in the sense that we pretend our system
to be fully learnable in the future. All components, algorithms, and parts, can be measured and evaluated
automatically, if the knowledge of how these parts interact is described in an organizational
ontology inside the framework.

% NUEVO
The organizational ontology, once defined, should provide a description of the framework's
modules, down to a degree of detail that allows performing automated reasoning about which
components can be connected. This ontology could be used in an automatic pipeline
designer, i.e., a module that learns how to combine specific components
(algorithms, data sources, etc.) for a particular learning task. By self-evaluating
its own performance on each particular task (given a combination of the evaluation
metrics presented in section~\ref{sec:evaluation}), this module could learn, for example,
if for a particular data source is more convenient applying entity extraction or removing
stopwords, or which clustering algorithm performs better.
As an illustrative example, Figure~\ref{fig:pipeline} shows several possible
learning pipelines built from common components. A relevant evaluation metric for each
of these pipelines could allow the framework to automatically discover the best combinations.
This opens the door to the possibility of, in time, the framework
itself learning which algorithms and approaches work better in each task, or in each domain.
% ---

\begin{figure}[htb]
    \begin{center}
        \includegraphics[width=0.8\columnwidth]{Graphics/optimization.pdf}
        \caption{Schematic representation of the possible different evaluated pipelines inside the framework.}
        \label{fig:pipeline}
    \end{center}
\end{figure}

\subsection{Human input}

Most of the frameworks and solutions existing require a certain degree of human interaction.
In most cases, a domain expert is expected to interact with a computational tool to either validate or
refine the output of learning pipeline.
As explained before, our approach proposes to be completely automated,
in the sense that the final knowledge stored may never be revised by a human. The whole framework is thus
designed with the purpose of complete automation. All the decisions that determine which fragments
of the information extracted get stored are based on computational parameters that can
be automatically tunned.
This does not deny some cases where a domain expert could interact with the framework,
through some form of natural language interface, in order to access the stored knowledge.
However, this possibility of interaction is a result of the framework, not a necessity.

\subsection{Generalization}

With respect to domain restrictions, we can classify existing solutions into those which are
completely independent of domain (e.g., KnowItAll, LEILA, ISOLDE~\cite{isolde}) and those which are tailored
to particular domains (e.g., SOBA, Artequakt). The domain independent solutions are usually designed
so that there is no particular dependence tied to a domain, hence, the are reusable in several domains.
However, this means one system can be used in one domain or another, but it doesn't necessarily
means that the same system can learn a piece of knowledge of two different
domains \emph{simultaneously}.

If a system is simply designed to be domain-independent, and used to learn from two very different
domains, the expected result is some sort of combined ontology that represents both domains with
an approximation of the union of the concepts (entities and relations) therein.
This might not be the ideal representation, specially when extending this to many different domains.
Trying to build a single ontology that encompasses all the knowledge that can be mined from
several different domains (possible in the order of hundreds or thousands) can be significantly
more difficult than a simple summation or union of each of the single domains.

Our approach to multi-domain coverage is different. Even though the case study presented only involves
one ontology, our framework is designed to learn a new ontology every time
a new domain is discovered. This new ontology can be potentially merged into with
existing ontologies, but it can also simply be stored as a brand new piece of usable knowledge.
In time, the system would accumulate different ontologies for different domains.
When the need arises to use such knowledge %(e.g. to perform automated question answering),
the framework would decide which ontologies are relevant in a particular task. % (i.e., a particular query).
This in turn requires a more complex processing since an ontology merging approach could be
necessary to compute a final output for a task that involves multiple domains. However, we believe this increased
complexity is an affordable cost compared to the expressive power gained by representing as much knowledge
as possible without the restrictions of having a single taxonomy, or a single set of inference rules.

This belief is in part inspired by how the expert knowledge works in humans. It is true that
humans have a basic set of skills which could be considered domain-independent, such as our innate
abilities for pattern matching. These basic skills are used across many of the tasks we encounter daily.
The computational analogy is a system with a single general purpose learning algorithm that could
perform, for example, tasks as dissimilar as  speech recognition, image classification and translation
with the same process.
This is the preferred approach for a part of the community of researchers in machine learning~\cite{kaiser2017one},
who expect to build a general purpose intelligence out of a single general purpose learning algorithm
and a single general purpose internal representation.

However, for really complex tasks, we argue that humans use specialized representations,
learning algorithms, and inference techniques. A human expert in a highly complex domain (such as mathematics),
doesn't use the same techniques for inference, than those used in real time tasks such
as object and speech recognition. We acknowledge that inference in highly complex domains cannot be
performed with intuition-based tools. Building an intelligent
system that can perform inference in a number of highly complex domains simultaneously will
require using specialized representations and techniques in each domain. It is towards this
principle that we guide our design decisions.

  \section{Discusión}
