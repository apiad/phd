\chapter{Estado de la Cuestión}\label{Chap:SOTA}
\markboth{\MakeUppercase{Estado de la Cuesti\'on}}{}

Intro del estado del arte

- el descubrimiento de conocimiento en lenguaje natural

Humans consume the ambient information and transform
it into independent concepts, that can be related to each other. In general,
we only store a small part of all the information we process every day.
This information consists of the most important concepts and relations,
that can be useful in future situations.
Thus, we learn basically by separating what could be
considered noise from the information that we think will be useful in the future.
In general we said two types of memories: one for recent knowledge and
other where we stored the knowledge for the long time.

Nowadays, one of the biggest information sources in Internet
is text in natural language, which appears in news, opinions, messages, encyclopedias,
and many other forms of digital communication.
This is not accidental, since words~(computationally represented as text)
are one of the most important communication channels throughout the human history.
One of the most basic concepts recognizable in natural language that provide
useful information are actions. These are generally associated with two
distinctive concepts, the subject and the target. The first is who performs
the action, and the later is who receives the consequences of said action.
Correctly identifying actions, and their corresponding subjects and targets
is one of the most basic and fundamental challenges in order to understand
the information.

--------------------------

The accelerated growth of Internet has produced a surplus of information (news, email,
social media, blogs) that greatly exceeds our capacity for processing and consuming data.
Thus, building automatic systems that can extract knowledge from this flow of information has
become of the most active research fields in Computer Science. This phenomena has been
dubbed Big Data~\cite{bigdata}, and its study has attracted the attention of different
research communities
as business intelligence~\cite{chen2012business}, engineering including physical and
biological~\cite{wu2014data} and social media~\cite{shah2015big}.

- importancia de la semántica

% PAPER ICAII

Humans try to represent the world we live in and facts and events that happen using language. Language enables ideas to be structured so that they can be shared them with other people.
Generally, one of the most common ways of representing arbitrary pieces of knowledge is through the notion of concepts and actions~\cite{teleologies}.
By understanding the world as composed of concepts interacting with each other through actions, humans can communicate a large variety and
complexity of information and knowledge.
The simplest way of combining concepts and actions is through binary relations, where one concept acts as the subject and another as the target.
This simple structure is very common in human languages.
More than 75\% of languages have a syntactic structure that follows either the SVO (subject-verb-object) or SOV (subject-object-verb) order~\cite{cambridge}.
Also, Subject-Verb-Object is the most common order in Creole languages, which has been suggested as an indication that is the most natural order
for human psychology.~\cite{diamond2013rise}.
As an example, from the sentence ``Arnold drives a car'' and we obtain an Subject-Verb-Object triplet \verb|(Arnold, drives, car)|.

This structure is ubiquitous in many different problems in the field of Natural Language Processing.
Different authors have employed a Subject-Verb-Object for similar structure or representing knowledge extracted from natural text, in a variety of domains~\cite{never-ending-learner,emotinet}.
Thus, a representation based on Subject-Verb-Object triplet is suitable for the generic representation of knowledge, independently of the domain.

Ontologies are one of the most common representations of knowledge in a digital format.
Ontologies, knowledge bases, semantic nets, and similar computational representations, are often built on the concept of the Subject-Verb-Object triplet, even if the individual components are referred to by using different terms (i.e., relations in RDF triplets or predicates in logic knowledge bases).

The rise of the semantic web has encouraged the creation of large scale ontologies and knowledge bases in several domains.
Some ontologies represent general domains, such as DBPedia, and collect a large proportion of common human knowledge.
Others, such as Ivanovic and Budimac~(\cite{IVANOVIC20145158}) operate in a more specific domain but incorporate more detailed facts about the domain.
One of the biggest obstacles to the development of the semantic web is the amount of effort and time it takes for human experts to build ontologies by hand~\cite{gomez2006ontological, petasis2011ontology}.
In this sense, the field of ontology learning~\cite{buitelaar2005ontology} studies the techniques and methodologies that enable the automatic or semi-automatic extraction of ontologies from unstructured sources of information, such as natural text~\cite{mitchell2015never,emotinet}.

One of the most important issues in ontology learning from natural text is how to recognize which knowledge is relevant in a domain. Usually, in a corpus of natural text, a large portion of the information will be spurious or unimportant~\cite{Kanya2009InformationE}.
Humans have an innate ability to forget the spurious or unimportant facts that are presented to us daily, and only store for long term that piece of knowledge that is relevant for a particular purpose.
Several relevance metrics have been proposed~\cite{manning2008introduction, brank2005survey}.
In general, the most relevant knowledge can be related to the actions and concepts that appear most often in a domain.

% PAPER JBI

The exponential growth of the Internet in the last decades has produced a massive surplus of textual information in all areas of human endeavor. This scenario presents both an opportunity and a challenge for researchers. On the one hand, a growing body of scientific literature is readily available, where potential solutions for critical problems could be found by linking partial results published in distinct documents. On the other hand, the extent of the information available cannot be processed by humans alone in a reasonable time frame. Hence,  efforts have recently been directed towards designing automatic techniques that can discover relevant pieces of information in large corpora, make logical connections, and synthesize useful knowledge.
The first step in many of these techniques involves the collection, processing and annotation of data that can be used to train machine learning algorithms or build expert systems through the use of natural language processing techniques.

The digital health sector is of great interest to the research community given the potential social benefits derived from applying automatic knowledge discovery technologies. The research community has produced a large number of annotated corpora in different sub-domains of this sector, from specific (e.g., drug-disease~\cite{goldberg1996drug} or gene-protein interactions~\cite{tanabe2005genetag}) to broad in scope and domain (e.g., clinical trial reports~\cite{nye2018corpus}).
Domain-specific corpora and technologies are of critical importance in high-precision medicine.
However, systems built for very specific domains are arguably harder to generalize and extend than systems built on general-purpose conceptualizations.
As such, there is a growing interest in designing annotation models and corpora with general-purpose semantics that can be used in a variety of domains or as a component in more specialized systems.

Besides domain, language is another dimension that has been the focus of recent research.
Most of the largest linguistic resources are based on English sources, motivated in part by the abundance of available raw material~(e.g., online encyclopedias, research papers), which is not surprising given that English is the most predominant language in science, technology and communications.
However, English-based resources are not always directly applicable to other languages.
Even though automatic translation has reached impressive accuracy in open domains, it is still a challenge to create cross-language resources, such as with Spanish, which is less predominant in technical domains~\cite{villegas2018mespen}.
Instead of focusing on specific niche languages, one possible line of research is designing resources that are
language-agnostic, in the sense that they can be generalized to multiple languages with little effort, by virtue
of being based on underlying common characteristics shared by many languages.

Designing annotation models that can generalize to multiple domains requires deciding on a basic representation of language that covers a broad range of semantics.
Moreover, these representations should be as independent of syntax and grammatical rules as possible, if they are
expected to generalize to multiple languages.
Recent work~\cite{estevez2018gathering} suggests that Subject-Action-Target triplets can be used to detect a large number of semantic interactions in natural language, independent of domain and relatively independent of language, since
more than 75\% of human languages employ some variation of the Subject-Verb-Object grammatical structure~\cite{crystal2004cambridge}.
Likewise, several ontological representations often agree in a number of general-purpose relations, (e.g., \textit{is-a} hyponyms, \textit{part-of} holonyms) that are useful in any domain.
Other conceptualizations allow the capture of semantics closer to natural language, such as Abstract Meaning Representation, AMR~\cite{banarescu2013abstract}.
The construction of corpora annotated with general-purpose semantic structures like Subject-Action-Target and high-level ontological relations is the first step in the design of systems that can discover knowledge automatically in a variety of domains and scenarios.

Research in knowledge discovery requires not only linguistic resources~(e.g., annotated corpora) but also computational resources and infrastructures that enable researchers to systematically evaluate their results and compare them objectively with alternative approaches.
This involves the formal definition of tasks and the design of objective evaluation metrics that ensure fair comparison is possible.
Even better is a publicly available evaluation system where researchers can submit their results, guaranteeing the same evaluation criteria is applied and freeing researchers from reproducing the evaluation environment. Such a system would also guarantee a more transparent and reproducible research process, and would provide a centralized repository of existing approaches, helping new researchers to update on the state-of-the-art.

% RESUMEN DEL PROCESO

- explicar el proceso y las tareas
  - definir un esquema de anotacion, modelo semántico, que sea bueno
  - herramientas con las que hacer la anotacion
  - anotación asistida y utilidades para anotar mejor
  - anotar un corpus con metricas de calidad y merging, hablar de la metodologia de anotacion
  - entrenar sistemas de machine learning para la extraccion automática
  - diseñar entornos de evaluación (challenges) para comparar sistemas
  - construir ontologías

  \section{Modelos Semánticos de Anotación}

  Knowledge discovery is a field of computer science that shows an accelerated growth in the past three decades.
  Advances in this area have been applied in many domains, from databases~\cite{fayyad1996data, knowledgeDatabase} to
  images~\cite{lu2016visual} and natural language text~\cite{carlson2010toward}.
  Specifically in natural language text, this field is highly relevant in the biomedical and health domains,
  where it is used for performing tasks such as
  Named Entity Recognition~(NER), Relationship Extraction and Hypothesis Generation, among others.~\cite{simpson2012biomedical}.
  These tasks generally use annotated corpora for learning the characteristics that appear in the text and mapping them to knowledge structures.
  For each task, specific annotation models have been designed that focus on specific elements of the text.
  For example, in NER tasks is more important to focus on nominal phrases than other grammatical constructions.

  Despite that these domain-specific tasks are different, most of them share common characteristics. For example, most tasks deal with the detection of relevant entities and their relations. Hence, promoting general-purpose annotation models would allow the design of reusable and cross-domain knowledge discovery techniques.
  In this line, several domain-independent semantic representations have been developed~(e.g., AMR~\cite{amr}, PropBank~\cite{propbank}, FrameNet~\cite{framenet}).
  However, these representations rely heavily on fine-grained lexicons that define specific semantic roles for each word meaning. Therefore, developing knowledge discovery systems with this level of detail supposes great challenges. Using more coarse-grained semantic representation, even with the loss of some representational capacity, would simplify the creation of automatic techniques based on machine learning.
  This representation could also be used as the first stage in a pipeline for a domain-specific task, thus reusing resources and techniques in domains with few available resources.
  In this section we present a review of relevant annotation models from which we draw inspiration.
  We focus general-purpose annotation models~\ref{sec:general} as well as on annotation models that have been applied to the health domain~\ref{sec:health}.

  To provide more fine-grained search results, documents can be processed to extract the relevant semantic entities and facts mentioned.
The task of automatically discovering semantic knowledge from text is covered by research areas
such as ontology learning~\cite{cimiano2009ontology} and {learning by reading}~\cite{barker2007learning},
whose purpose is to build semantic networks that {capture} the knowledge present in large collections of text.
These semantic networks enable the use of search engines that provide an analysis beyond the textual content's relevance, by exploiting the semantic structure of the network.
In this context, processing health textual contents has attracted {great interest}~\cite{gonzalez2017capturing}, motivated by the large number of medical documents published yearly.

Several approaches exist for building semantic representations of knowledge.
In many cases, these representations use a domain-specific conceptualization.
Although this provides a more specialized representation, it makes these approaches harder to apply to a broad range of domains.
Alternatively, a general purpose conceptualization could be used, which is able to represent entities and facts from multiple knowledge domains.
Such conceptualization should be general enough so as to accommodate many different domains, but still to provide a degree of expressiveness necessary for knowledge mining tasks.
One possible conceptualization is using
{Subject-Action-Target triplets}~\cite{suilan2018}.
This structure has proven to be useful for representing knowledge in both specific
domains such as movie reviews~\cite{suilan2018} or sentiment
mining~\cite{emotinet} and in {general domain ontology learning}~\cite{mitchell2018never}. %%%%CHANGE fix cite info
Furthermore, Subject-Action-Target triplets automatically extracted from text can be
later linked to domain-specific relations through the use of semantic networks.
As an example, the \textit{SemRep} system~\cite{semrep} extracts Subject-Predicate-Object
triplets from natural eHealth texts. The predicates are linked to specific relations
in the UMLS~\cite{umls} semantic network.

Recent work in the development of Teleologies~\cite{teleologies} suggests that Action-Subject-Target triplets can be the base for general purpose conceptualizations across many different domains, since this triplet allows the capture of interactions between objects through the actions they perform on each other.
A small set of semantic relations, such as \textit{hyponomy} and
\textit{holonomy} can provide additional semantic structure to theAMR
representation. These ``general'' relations are common in most knowledge bases,
regardless of domain, such as WordNet~\cite{miller1998wordnet},
DBPedia~\cite{lehmann2015dbpedia}, and ConceptNet~\cite{conceptnet}.
Other possible conceptualizations allow the capture of semantics of natural language,
such as Abstract Meaning Representation~(AMR)~\cite{amr}. Despite the superior representational
power of AMR over simple structures such as Action-Subject-Target triplets {and basic semantic
relations}, the annotation process for AMR is considerably more complex both for humans and automated techniques.

Building corpora annotated with the Action-Subject-Target structure is the first step towards the design of systems that can automatically extract these annotations. Several corpora exist in the literature, annotated with a variety of different schemes, such as CLEF~\cite{kelly2016overview}, Yago~\cite{fabian2007yago} and Emotinet~\cite{emotinet}.
However, most of these resources are annotated with domain-specific conceptualizations that are difficult to extend to different knowledge domains.

  \subsection{General-purpose annotation models}\label{sec:general}

  Several general-purpose semantic annotation models have been developed, that attempt to represent the semantics of a sentence beyond the syntactic structure.
  These models are loosely based on the Subject-Verb-Object grammatical structure that is pervasive in human language.

  \paragraph{PropBank}

  PropBank~\cite{propbank} proposes a general purpose annotation schema, based on annotating predicates (verbs) as the main semantic constituents of a sentence. ProbBank's annotation schema is able to represent several semantic relations, including the agent that causes an action, the receiver of the effects of an action, time and location modifiers, and causal relationships.
  One key characteristic of PropBank is that every predicate defines custom semantic roles, i.e., the predicate ``\textit{accept}'' defines roles for the agent who accepts~(\texttt{ARG0}), the object that is accepted~(\texttt{ARG1}), and the agent from whom that object is accepted.

  Our annotation scheme was developed from a consensus among research
  groups at BBN, MITRE, New York University, and Penn. It was agreed that
  the first phase of annotation would focus on verbal predicates, leaving
  adjectives, deverbal nouns, and predicate nominatives aside for a later stage.
  Argument labels were chosen which could be easily mapped onto the labels
  used in most modern theories of argument structure while not being
  especially beholden to any particular theory.1
   Arguments are thus numbered
  Arg0, Arg1, Arg2, and so on depending on the valency of the verb in
  question. The meaning of each argument label is defined relative to each verb
  in a lexicon of ’Frames Files.’ Each set of argument labels and their
  definitions is called a frameset and provides a unique identifier for the verb
  sense, a meaning for that verb sense, and the set of expected argument giving
  both the Arg-numbers and a mnemonic tag or description for that Arg.
  Following the definitions is any number of example sentences demonstrating
  various syntactic realizations for that frameset.

  The training material for proposition recognition, PropBank, is being annotated in English, based on a consensus developed in 2000 among research groups at BBN, MITRE, New York University, and Penn. Taking as a starting point the Penn Treebank II Wall Street Journal Corpus of a million words (Marcus 1994), we are adding predicate argument structure annotation. Approximately one−quarter of the TreeBank, comprising largely texts of financial reporting, has been extracted and is serving as our initial focus for training and to provide an earlier delivery of fully−annotated text. This subcorpus should be completed in June of 2002, while the remainder of the corpus will be completed by the summer of 2003. The current project annotates only verbal predicates, setting aside nominalizations, adjectives, and prepositions for a later phase. In a separate paper (Kingsbury, Marcus \& Palmer, forthcoming) we discuss the differences between PropBank and similar resources such as Verbnet, Wordnet and Framenet. In creating annotations for argument structure, a combination of syntactic and semantic factors are used, although syntactic cues are foremost. The general method is the following: for any given predicate, a survey is made of the usages of the predicate and the usages divided into major senses if required. These senses are divided more on syntactic grounds than semantic, thus avoiding the fine−grained and often−arbitrary divisions of, e.g., WordNet. The expected arguments of each sense are then numbered sequentially from Arg0 to Arg5. According to the guidelines established by the ACE community described above, no attempt is made to make argument labels have the same "meaning" from one sense of a verb to another, so for example the "role" played by Arg2 in one sense of a given predicate may be played by Arg3 in another sense. On the other hand, we intend for  predicates belonging to the same VerbNet class to share similarly−labeled 2 BBN has already completed pronoun co−reference annotation on the same data arguments, in keeping with the near−synonymy of the predicates.

\paragraph{FrameNet}

  FrameNet~\cite{framenet} is a lexical database and an annotated corpus that models the semantic roles and relations in a natural language sentence through conceptual structures named \textit{frames}. Frames represent general-purpose concepts, or events, that define the possible semantic relations in which those concepts can be realized in natural language.

  FrameNet is based on a theory of meaning called Frame Semantics, deriving from the work of Charles J. Fillmore and colleagues (Fillmore 1976, 1977, 1982, 1985, Fillmore and Baker 2001, 2010). The basic idea is straightforward: that the meanings of most words can best be understood on the basis of a semantic frame, a description of a type of event, relation, or entity and the participants in it. For example, the concept of cooking typically involves a person doing the cooking (Cook), the food that is to be cooked (Food), something to hold the food while cooking (Container) and a source of heat (Heating\_instrument). In the FrameNet project, this is represented as a frame called Apply\_heat, and the Cook, Food, Heating\_instrument and Container are called frame elements (FEs) . Words that evoke this frame, such as fry, bake, boil, and broil, are called lexical units (LUs) of the Apply\_heat frame. Other frames are more complex, such as Revenge, which involves more FEs (Offender, Injury, Injured\_Party, Avenger, and Punishment) and others are simpler, such as Placing, with only an Agent (or Cause), a thing that is placed (called a Theme) and the location in which it is placed (Goal).

The lexical entry for each LU is derived from such annotations, and specifies the ways in which FEs are realized in syntactic structures headed by the word.

Many common nouns, such as tree, hat or tower, usually serve as dependents which head FEs, rather than clearly evoking their own frames, so we have devoted less effort to annotating them, since information about them is available from other lexicons, such as WordNet (Miller et al. 1990). We do, however, recognize that such nouns also have a minimal frame structure of their own, and in fact, the FrameNet database contains slightly more nouns than verbs.

Formally, FrameNet annotations are sets of triples that represent the FE realizations for each annotated sentence, each consisting of a frame element name (for example, Food), a grammatical function (say, Object) and a phrase type (say, noun phrase (NP)). We can think of these three types of annotation on each FE as "layers", but the grammatical function and phrase-type layers are not displayed in the web-based report system, to avoid visual clutter. The downloadable XML version of the data includes these three layers (and several more not discussed here) for all of the annotated sentences, along with complete frame and FE descriptions, frame-frame relations, and lexical entries for each annotated LU. Most of the annotations are of separate sentences annotated for only one LU, but there are also a collection of texts in which all the frame-evoking words have been annotated; the overlapping frames provide a rich representation of much of the meaning of the entire text. The FrameNet team have defined more than 1,000 semantic frames and have linked them together by a system of frame relations, which relate more general frames to more specific ones and provide a basis for reasoning about events and intentional actions.

Because the frames are basically semantic, they are often similar across languages; for example, frames about buying and selling involve the FEs Buyer, Seller, Goods, and Money, regardless of the language in which they are expressed. Several projects are underway to build FrameNets parallel to the English FrameNet project for languages around the the world, including Spanish, German, Chinese, and Japanese, and frame semantic analysis and annotation has been carried out in specialized areas from legal terminology to soccer to tourism.

\paragraph{VerbNet}

  VerbNet~\cite{verbnet} is a verb lexicon that also defines specific semantic roles for each verb. In VerbNet, verbs are organized in a hierarchy, and linked through different thematic roles, such as agents, cause, source, or topic. These elements allow to capture the semantic representation of sentences.
  PropBank semantic roles are similar to the thematic roles defined in VerbNet and frame elements in FrameNet. As such, there are resources that link these semantic structures~\cite{semlink}.

  VerbNet (VN) (Kipper-Schuler 2006) is the largest on-line network of English verbs that links their syntactic and semantic patterns. It is a hierarchical, domain-independent, broad-coverage verb lexicon with mappings to other lexical resources, such as WordNet (Miller, 1990; Fellbaum, 1998), PropBank (Kingsbury and Palmer, 2002), and FrameNet (Baker et al., 1998). VerbNet is organized into verb classes extending Levin (1993) classes through refinement and addition of subclasses to achieve syntactic and semantic coherence among members of a class. Each verb class in VN is completely described by thematic roles, selectional preferences of the arguments, and frames consisting of a syntactic description and a semantic representation with subevent structure patterned on the Dynamic Event Model of Pustejovsky and Moszkowicz (2011) and Pustejovsky (2013).

  Each VN class contains a set of syntactic descriptions, or syntactic frames, depicting the possible surface realizations of the argument structure for constructions such as transitive, intransitive, prepositional phrases, resultatives, and a large set of diathesis alternations. Semantic restrictions (such as animate, human, organization) are used to constrain the types of thematic roles allowed by the arguments, and further restrictions may be imposed to indicate the syntactic nature of the constituent likely to be associated with the thematic role. Syntactic frames may also be constrained in terms of which prepositions are allowed. Each frame is associated with explicit semantic information, expressed as a conjunction of boolean semantic predicates such as `motion,' `contact,' or `cause.' Each semantic predicate is associated with an event variable E that allows predicates to specify when in the event the predicate is true (start(E) for preparatory stage, during(E) for the culmination stage, and end(E) for the consequent stage). Figure 1. shows a complete entry for a frame in VerbNet class Hit-18.1.

  VerbNet has recently been integrated with 57 new classes from Korhonen and Briscoe's (2004) (K\&B) proposed extension to Levin's original classification (Kipper et al., 2006). This work has involved associating detailed syntactic-semantic descriptions to the K\&B classes, as well as organizing them appropriately into the existing VN taxonomy. An additional set of 53 new classes from Korhonen and Ryant (2005) (K\&R) have also been incorporated into VN. The outcome is a freely available resource which constitutes the most comprehensive and versatile Levin-style verb classification for English. After the two extensions VN has now also increased our coverage of PropBank tokens (Palmer et. al., 2005) from 78.45\% to 90.86\%, making feasible the creation of a substantial training corpus annotated with VN thematic role labels and class membership assignments, to be released in 2007. This will finally enable large-scale experimentation on the utility of syntax-based classes for improving the performance of syntactic parsers and semantic role labelers on new domains.

  Each verb argument is assigned one (usually unique) thematic role within the class. A few exceptions to this uniqueness are classes which contain verbs with symmetrical arguments, such as Chitchat-37.6 class, or the ContiguousLocation-47.8 class. These classes have indexed roles such as Actor1 and Actor2, as explained above.

\paragraph{AMR}

  A more recent proposal is Abstract Meaning Representation~\cite[ARM]{amr}. AMR constitutes a semantic representation schema for English sentences that also attempts to cover a wide range of semantic relations with a general-purpose model.
  AMR includes PropBank semantic roles, as well as coreference resolution within the same sentence, named entities and types, negation, and other modifiers in a graph structure that represents the meaning of a natural language sentence.
  However, even though AMR captures the full semantic meaning of a sentence, for the purpose of knowledge discovery it is still considerably abstract, and additional processing is necessary to extract concrete structures of knowledge~\cite{rao2017biomedical}.

  Our basic principles are:
AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse.
AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR.
AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase.
AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings are related to meanings.
AMR is heavily biased towards English. It is not an Interlingua.

  The annotation model proposed in this research shares similarities from general-purpose semantic annotation models such as AMR and PropBank.
  In contrast to these resources, our model makes no distinction between different types of actions, which are loosely related to verbs, as explained in Section~\ref{sec:model}. Instead, we define two general-purpose roles, the agent that performs and action, and the receiver of the effects of the action. These roles roughly correspond to \texttt{ARG0} and \texttt{ARG1} respectively in PropBank, although in specific cases their semantic meaning might differ.
  This simplification is directed towards enabling the automation of the annotation process with the use of machine learning techniques.
  Another key difference of our model is the inclusion of general-purpose taxonomic relations~(e.g, \textit{hypernomy}/\textit{hyponomy} and \textit{meronym}/\textit{holonym}) that are inferred from the sentence. These relations are directed towards easing the automatic construction of knowledge bases.

  \subsection{Annotations models in the health domain}\label{sec:health}

  Knowledge discovery tasks in the health domain are often supported by the construction of manually-annotated corpora.
  Several task-specific annotation models have been developed for this purpose. One example is the  {DrugSemantics} corpus~\cite{moreno2017drugsemantics} where product characteristics are annotated, and  {BARR2}~\cite{barr2} which is concerned with biomedical abbreviations.
  Many corpora include specific types of named entities relevant to the medical domain, such as {DDI}~\cite{ddi} which annotates drugs and other substances.
  Other examples include {i2b2}~\cite{i2b2} which annotates medications, dosages and other details of drug administration and  {CLEF}~\cite{clef} which annotate different types of conditions, devices and their results in specific clinical cases.
  Given the specificity of the annotated concepts, most of these resources are built by biomedical experts.

  The previous examples are corpora helpful in designing techniques oriented towards narrow tasks,
  where the annotation model is specifically designed to only consider portions of the text relevant to the concepts of interests (i.e., medical entities, genes, etc.).
  An alternative approach that attempts to model a wide range of the semantics of a document is {Bio-AMR}~\cite{bioamr}.
  This corpus contains health-related sentences annotated with their AMR structure, a general-purpose semantic representation of natural text.
  Another relevant resource is BioFrameNet~\cite{bioframenet}, an extension to FrameNet with specific semantic roles for the biomedical domain.
  A positive consequence of using general-purpose semantic annotations is that it doesn't necessarily require experts in biomedical areas to participate in the annotation process.

  The {eHealth-KD} corpus~\cite{martinez2018overview} attempts to achieve a middle ground by representing a broad range of knowledge with a simple annotation model based on Subject-Action-Target triplets and 4 additional semantic relations.
  However, after the annotation process several shortcomings were identified.
  One example is the necessity for including {causality} and {entailment} as explicit relations, rather than representing them through actions, given the importance of this type of assertions in medical texts.
  Likewise, the annotation lacks the ability to represent coreferences (``\textit{this}'', ``\textit{that}''), and for this reason many sentences cannot be fully annotated.
  Also, complex linguistic constructions that represent composite concepts (e.g., ``\textit{the patients that received treatment}'') are difficult to annotate, especially when they participate in other relations.
  This paper extends the annotation model used by the eHealth-KD corpus with semantic elements used in general-purpose annotation models, such as AMR and PropBank.
  This extension allows solving the aforementioned issues and increases its representational power without adding an overly complex set of new semantic roles and relations.

  \section{Herramientas de Anotación}

  An important element to consider in Knowledge Discovery research is the existence of computational
  resources and infrastructure that supports the development of new approaches.
  The creation of linguistic resources often stems from a process of manual annotation by human
  experts, which requires computational tools for the actual annotation as well as mechanisms for merging
  annotations and computing agreement, ideally in a collaborative environment.
  Once the resources are created, it is necessary to distribute the corresponding corpus, baselines, and tools among the research community, often through online source code sharing platforms.

  An extensive analysis and comparison of several annotation tools is provided in~\citet{annotation-tools}.
  Table~\ref{tab:annotation-tools} summarizes the main characteristics we considered relevant for this research and identifies the most appropriate annotation tool among  a subset of  popular alternatives.
  We consider as requisites web-based, open source annotation tools that allow multi-label span annotations as well as relation annotations. Support for collaborative annotation, at least partially, is also highly desirable.
  Of the analyzed tools, we identified Brat~\cite{brat} and WebAnno~\cite{webanno}, as they comply with all the aforementioned requisites. In our research, we preferred Brat to WebAnno because, even though WebAnno provides more features, Brat allows an easier setup. It is not only faster to start an annotation project using this tool, but also to train annotators to use its interface.

  \begin{table}[htb]
      \centering
      \resizebox{\textwidth}{!}{
      \begin{tabular}{r|cccccccccccc}
          \textbf{Characteristics} & \rotatebox{90}{\textbf{GATE Teamware}} & \rotatebox{90}{\textbf{Knowtator}} & \rotatebox{90}{\textbf{WebAnno}} & \rotatebox{90}{\textbf{Brat}} & \rotatebox{90}{\textbf{BioQRator}} & \rotatebox{90}{\textbf{CATMA}} & \rotatebox{90}{\textbf{prodigy}} & \rotatebox{90}{\textbf{TextAE}} & \rotatebox{90}{\textbf{LightTag}} & \rotatebox{90}{\textbf{Djangology}} & \rotatebox{90}{\textbf{MyMiner}} & \rotatebox{90}{\textbf{WAT-SL}} \\ \midrule
          multi-label annotations &     &     & \ok & \ok &     & \ok &     &     & \ok & \ok &     &     \\ % F1
          relation annotations    &     & \ok & \ok & \ok & \ok &     &     & \ok & \ok &     & \ap &     \\ % F3
          allows custom model    & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok \\ %
          collaborative interface & \ok &     & \ap & \ap & \ap & \ap & \ap &     & \ok & \ok &     & \ap \\ % F10
          web-based interface     & \ok &     & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok & \ok \\ %
          can be self-hosted      & \ok & \ok & \ok & \ok &     & \ok & \ok & \ok &     & \ok &     & \ok \\ %
          open source license     & \ok & \ok & \ok & \ok &     & \ok &     & \ok &     & \ok & \ok & \ok \\ % T2
          citation                &\cite{gate}&\cite{knowtator}&\cite{webanno}&\cite{brat}&\cite{bioqrator}&\cite{catma}&\cite{prodigy}&\cite{textae}&\cite{lighttag}&\cite{djangology}&\cite{myminer}&\cite{watsl}\\
          \bottomrule
      \end{tabular}}
      \caption{Qualitative comparison of popular annotation tools. Adapted from Table 3 in~\citet{annotation-tools}, Table~3. A symbol~\ap~indicates that the corresponding feature is only partially supported.}
      \label{tab:annotation-tools}
  \end{table}

  \subsection{Descripción de las herramientas de anotación}

  \begin{description}
    \item[GATE Teamware]
    an open-source text annotation framework and a methodology for the implementation and support of complex annotation projects. It has a web-based architecture, where a number of web services (e.g. document storage, automatic annotation) are made available via HTTPS and the users interact with the text annotation interfaces through a standard web browser.

    GATE Teamware is based on GATE (Cunningham et al. 2011b), a widely used, scalable and robust open-source NLP platform. GATE comes with numerous reusable text processing components for many natural languages, coupled with a graphical NLP development environment and user interfaces for visualisation and editing of linguistic annotations, parse trees, co-reference chains, and ontologies. GATE Teamware however was created specifically to be used by non-expert annotators, as well as to enable methodologically sound, efficient, and cost-effective corpus annotation projects over the web.

    In addition to its research uses, GATE Teamware has also been tested as a framework for cost-effective commercial annotation services, supplied either as in-house units or as outsourced specialist activities. Several test annotation projects have been conducted in the domains of bio-informatics and business intelligence, with minimal training and producing high quality corpora. For example, Meurs et al. (2011) apply GATE Teamware to the task of building a database of fungal enzymes for biofuel research. Their results show that using GATE Teamware for automatic pre-annotation and manual correction increases the speed with which papers can be processed for inclusion in the database by a factor of around 50 %.

    Similar to other server-side software, GATE Teamware installation is a specialised, non-trivial task with associated costs, in terms of significant time and staff expertise required. In order to lower this barrier and provide zero startup costs, we have made available cloud-based GATE Teamware virtual machines ,Footnote3 that can be turned on and off as required. In addition, the GATECloud.net (Tablan et al. 2013) integration makes it easy to choose a set of automatically annotated documents and send these into a GATE Teamware instance. There is also a virtual machine distribution that can be downloaded and run locally instead.

    \item[Knowtator]
    In Knowtator, an annotation
    schema is defined with Protégé class, instance,
    slot, and facet definitions using the Protégé knowledge-base editing functionality. The defined annotation schema can then be applied to a text
    annotation task without having to write any task
    specific software or edit specialized configuration
    files. Annotation schemas in Knowtator can model
    both syntactic (e.g. shallow parses) and semantic
    phenomena (e.g. protein-protein interactions).

    Knowtator approaches the definition of an annotation schema as a knowledge engineering task by
    leveraging Protégé’s strengths as a knowledgebase editor. Protégé has user interface components
    for defining class, instance, slot, and facet frames.
    A Knowtator annotation schema is created by defining frames using these user interface components as a knowledge engineer would when
    creating a conceptual model of some domain. For
    Knowtator the frame definitions model the phenomena that the annotation task seeks to capture.

    As a simple example, the co-reference annotation task that comes with Callisto can be modeled
    in Protégé with two class definitions called markable and chain. The chain class has two slots references and primary_reference which are
    constrained by facets to have values of type markable. This simple annotation schema can now be
    used to annotate co-reference phenomena occur274
    ring in text using Knowtator. Annotations in
    Knowtator created using this simple annotation
    schema.

    A key strength of Knowtator is its ability to relate annotations to each other via the slot definitions of the corresponding annotated classes. In
    the co-reference example, the slot references of the
    class chain relates the markable annotations for the
    text extents ‘the cat’ and ‘It’ to the chain annotation. The constraints on the slots ensure that the
    relationships between annotations are consistent.

    Protégé is capable of representing much more
    sophisticated and complex conceptual models
    which can be used, in turn, by Knowtator for text
    annotation. Also, because Protégé is often used to
    create conceptual models of domains relating to
    biomedical disciplines, Knowtator is especially
    well suited for capturing named entities and relations between named entities for those domains.

    \item[WebAnno]
    WebAnno is the third major release of the web-based annotation tool WebAnno (Yimam et al., 2013;
    Yimam et al., 2014) introducing new functionalities enabling the annotation of semantic structures:

    1. Slot-features allow the appropriate modelling of predicate-argument structures for SRL. We also
    support the following additional semantic annotation types: participants and circumstances for event
    annotation, n-ary relations for relation extraction, and slot-filling tasks for information extraction.

    2. Constraints help annotators by performing a context-sensitive filtering of the rich semantic tagsets.
    For example, the sense of a semantic predicate determines available argument roles. Such a filtering is
    necessary to avoid loosing valuable time by having annotators search through a large number of tags
    or to manually type in tags. Constraint rules can be defined manually or they can be automatically
    generated, e.g. from machine-readable lexical resources. To our knowledge, there is no other
    web-based annotation tool offering a comparable functionality.

    3. An improved annotation interface for a streamlined annotation process using a permanently visible
    sidebar instead of a pop-up dialog for editing annotations and their features.

    These new functionalities integrate well with the existing functionalities in WebAnno 2, in particular its
    support for the annotation of syntactic structures, thus enabling semantic annotation in coordination with
    syntactic annotation. To our knowledge, WebAnno 3 is presently the only web-based and team-oriented
    annotation tool to support both, the annotation of semantic as well as syntactic structures.

    WebAnno 3 was developed and implemented in close coordination with users in the context of an
    annotation project (cf. Mujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on ´
    German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates,
    their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts.
    Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker
    et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation
    is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn
    Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation
    (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic
    constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the
    sense label typically determines the available argument slots. The example below shows an annotation
    using FrameNet; the predicate ask receives the frame label Questioning (corresponding to its word sense)
    and its arguments are annotated as Addressee, Speaker, Message, and Iterations

    \item[Brat]
    BRAT is based on our previously released opensource STAV text annotation visualiser (Stenetorp et al., 2011b), which was designed to help
    users gain an understanding of complex annotations involving a large number of different semantic types, dense, partially overlapping text annotations, and non-projective sets of connections
    between annotations. Both tools share a vector
    graphics-based visualisation component, which
    provide scalable detail and rendering. BRAT integrates PDF and EPS image format export functionality to support use in e.g. figures in publications (Figure 1).

    2.2 Intuitive Annotation Interface

    We extended the capabilities of STAV by implementing support for annotation editing. This was
    done by adding functionality for recognising standard user interface gestures familiar from text editors, presentation software, and many other tools.
    In BRAT, a span of text is marked for annotation
    simply by selecting it with the mouse by “dragging” or by double-clicking on a word. Similarly,
    annotations are linked by clicking with the mouse
    on one annotation and dragging a connection to
    the other.

    BRAT is browser-based and built entirely using
    standard web technologies. It thus offers a familiar environment to annotators, and it is possible to start using BRAT simply by pointing a
    standards-compliant modern browser to an installation. There is thus no need to install or distribute any additional annotation software or to
    use browser plug-ins. The use of web standards
    also makes it possible for BRAT to uniquely identify any annotation using Uniform Resource Identifiers (URIs), which enables linking to individual
    annotations for discussions in e-mail, documents
    and on web pages, facilitating easy communication regarding annotations.

    2.3 Versatile Annotation Support

    BRAT is fully configurable and can be set up to
    support most text annotation tasks. The most basic annotation primitive identifies a text span and
    assigns it a type (or tag or label), marking for e.g.
    POS-tagged tokens, chunks or entity mentions
    (Figure 1 top). These base annotations can be
    connected by binary relations – either directed or
    undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation
    (Figure 1 middle and bottom). n-ary associations
    of annotations are also supported, allowing the annotation of event structures such as those targeted
    in the MUC (Sundheim, 1996), ACE (Doddington
    et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using
    attributes, binary or multi-valued flags that can
    be added to other annotations. Finally, annotators
    can attach free-form text notes to any annotation.
    In addition to information extraction tasks,
    these annotation primitives allow BRAT to be
    configured for use in various other tasks, such
    as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras
    and Marquez, 2005), and dependency annotation `
    (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement
    full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devanagar ¯ ¯ı characters. BRAT
    is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven
    different languages and including examples from
    corpora such as those introduced for the CoNLL
    shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and multilingual dependency parsing
    (Buchholz and Marsi, 2006).
    BRAT also implements a fully configurable system for checking detailed constraints on annotation semantics, for example specifying that a
    TRANSFER event must take exactly one of each
    of GIVER, RECIPIENT and BENEFICIARY arguments, each of which must have one of the types
    PERSON, ORGANIZATION or GEO-POLITICAL
    ENTITY, as well as a MONEY argument of type
    103
    Figure 3: Incomplete TRANSFER event indicated
    to the annotator
    MONEY, and may optionally take a PLACE argument of type LOCATION (LDC, 2005). Constraint
    checking is fully integrated into the annotation interface and feedback is immediate, with clear visual effects marking incomplete or erroneous annotations (Figure 3).
    2.4 NLP Technology Integration
    BRAT supports two standard approaches for integrating the results of fully automatic annotation
    tools into an annotation workflow: bulk annotation imports can be performed by format conversion tools distributed with BRAT for many
    standard formats (such as in-line and columnformatted BIO), and tools that provide standard
    web service interfaces can be configured to be invoked from the user interface.
    However, human judgements cannot be replaced or based on a completely automatic analysis without some risk of introducing bias and reducing annotation quality. To address this issue,
    we have been studying ways to augment the annotation process with input from statistical and
    machine learning methods to support the annotation process while still involving human annotator
    judgement for each annotation.
    As a specific realisation based on this approach,
    we have integrated a recently introduced machine learning-based semantic class disambiguation system capable of offering multiple outputs
    with probability estimates that was shown to be
    able to reduce ambiguity on average by over 75%
    while retaining the correct class in on average
    99\% of cases over six corpora (Stenetorp et al.,
    2011a). Section 4 presents an evaluation of the
    contribution of this component to annotator productivity.
    2.5 Corpus Search Functionality
    BRAT implements a comprehensive set of search
    functions, allowing users to search document colFigure 4: The BRAT search dialog
    lections for text span annotations, relations, event
    structures, or simply text, with a rich set of search
    options definable using a simple point-and-click
    interface (Figure 4). Additionally, search results
    can optionally be displayed using keyword-incontext concordancing and sorted for browsing
    using any aspect of the matched annotation (e.g.
    type, text, or context).

    \item[BioQRator]
    BioQRator ( 17 ) is a general-purpose user interface for annotating bio-entities and relationships. Simple and minimal network messages are used to communicate between BioQRator and text-mining resources. This enables one to easily create a customized interface for any bio-curation project if the task involved is to annotate entities and/or relationships. An important issue for curation systems is the multiple different formats that are used. To address this problem, we adopt BioC ( 18 , 19 ) as a standard input and output format. For input, BioC formatted documents or PubMed abstracts can be used. For output, annotated documents can be saved in the BioC or CSV (comma-separated values) format as well. More importantly, BioQRator provides an easy-to-use interactive web interface. It also supports multiple browsers including Chrome, Firefox and Safari (partially compatible with Internet Explorer).

    The BioCreative Interactive task (IAT) is a track designed for exploring user–system interactions, promoting development of useful text-mining tools and providing a communication channel for biocuration and text-mining communities ( 20 , 21 ). For BioCreative IV, a participating team defined a topic and tasks for curators’ evaluation. Biocurator volunteers were assigned to participating teams based on their interests in defined topics. The BioCreative IAT task is especially meaningful for text-mining communities because there has been little effort to formally evaluate curation tools. To demonstrate the usability of BioQRator in BioCreative IV, a PPI task was defined and PIE the search ( 22 , 23 ) was used for an external text-mining resource. Necessary databases such as PubMed, Entrez Gene and UniProt were used to provide web links for BioQRator.

    \item[CATMA]

    CATMA	(Computer	Aided	Text	Markup	and	Analysis),
    a	tool	developed	at	the	University	of	Hamburg	and	currently	used	by	over	60	 research	projects	worldwide.
    CATMA	offers	a	unique	combination	of	three	main	features	found	in	no	other	text	analysis	tool:

    -CATMA supports collaborative annotation and
    analysis – a text or text corpus can	be investigated individually, but also jointly by a group of students or researchers.

    -CATMA supports explorative, non-deterministic
    practices of text annotation – a	 discursive,	 debateoriented	approach	to	text	annotation	based	on	the	research	practices of hermeneutic disciplines is the underlying conceptual model.

    -CATMA integrates text annotation and text analysis in a web-based working	 environment	 – which
    makes	it	possible	to	combine	the	identification	of	textual	phenomena with their investigation in a seamless,
    iterative fashion.

    What	sets	CATMA	apart	from	other	digital	annotation	methods	is	its	‘undogmatic’	approach:	the	system
    does	 neither	 prescribe	 defined	 annotation	 schemata
    or	 rules,	 nor	 does	 it	 force	 the	 user	 to	 apply	 rigid
    yes/no,	right/wrong	taxonomies	to	texts	(even	though
    it	allows	for	more	prescriptive	schemata	as	well).	Rather,	CATMA’s	logic	invites	users	 to	explore	the	richness	and	multi-facettedness	of	textual	phenomena	according	to	their	needs:	Users	can	create,	expand,	and
    continuously	modify	their	own	individual	tagsets	– so
    if	a	text	passage	invites	more	than	one	interpretation,
    nothing	in	the	system	prevents	assigning	multiple,	or
    even	contradictory	annotations.	Despite	all	this	 flexibility,	CATMA	does	not	produce	idiosyncratic	annotations:	All	markup	 data	 can	 be	 exported	in	 TEI/XMLformat	and	reused	in	other	contexts.

    Since	CATMA	is	a	highly	intuitive	tool,	it	is	also	suitable	for	humanists	with	little	technical	knowledge:	the
    GUI	 allows	 for	 a	 quick	 kick-off,	 and	 CATMA’s	 query
    builder	(a	step-by-step	dialogue-based	widget)	helps
    users	retrieve	complex	information	from	texts	without
    having	to	learn	a	query	language.	Another	plus	on	the
    easy-to-use	 side	is	 the	 fact	 that	 CATMA’s	 automated
    distant-reading	 functions	are	 continuously	enhanced
    and	 extended	 – the	 current	 version	 5.0	 already	 features	 a	 number	 of	 automated	 annotation	 routines,
    among	others	the	identification	of	basic	narrative	features	in	texts.

    \item[prodigy]

    Prodigy is a modern annotation tool for creating training and evaluation data for machine learning models. You can also use Prodigy to help you inspect and clean your data, do error analysis and develop rule-based systems to use in combination with your statistical models.

    The Python library includes a range of pre-built workflows and command-line commands for various tasks, and well-documented components for implementing your own workflow scripts. Your scripts can specify how the data is loaded and saved, change which questions are asked in the annotation interface, and can even define custom HTML and JavaScript to change the behavior of the front-end. The web application is optimized for fast, intuitive and efficient annotation.

    Prodigy’s mission is to help you do more of all those manual or semi-automatic processes that we all know we don’t do enough of. To most data scientists, the advice to spend more time looking at your data is sort of like the advice to floss or get more sleep: it’s easy to recognize that it’s sound advice, but not always easy to put into practice. Prodigy helps by giving you a practical, flexible tool that fits easily into your workflow. With concrete steps to follow instead of a vague goal, annotation and data inspection will change from something you should do, to something you will do.

    \item[TextAE]

    Open source
    TextAE is developed as an open source project.
    Released under the MIT License.
     Unicode support
    It supports any language which is supported by UTF8.
    (However, we could test the feature only with a limited number of languages. If you find a problem with your language, pleast let us know, so that we can fix it.)
     Zero installation
    You can use a ready-for-use TextAE editor immediately without any installation process.
     Fully-featured GUI editor
    You can create or edit various types of annotation.
    named entity annotation
    relation annotation
    syntactic annotations
    …
     a default viewer/editor of PubAnnotation
    TextAE is developed as a default viewer/editor of PubAnnotation.
     REST client
    TextAE works as a REST client, which means
    it can get an annotation file from the net, and
    it can post an annotation file to the net.

    \item[LightTag]

    Work Faster With Our Optimized Interface
Keyboard Shortcuts
No tokenization assumptions
Full Unicode Support
Subword and phrase annotations
RTL and CJK languages
Entity, Classification and Relation annotations

Control Your Data Quality
LightTag's Review Mode and Reporting make it easy to ensure your data is perfect and your annotators are performing at their very best.

Control Your Data Quality
LightTag's Review Mode and Reporting make it easy to ensure your data is perfect and your annotators are performing at their very best.

    \item[Djangology]

    The Djangology
    annotation web application was originally created to meet the needs of a collaborative annotation
    project involving more than 250 international participants.
    The goal of the project was to create a gold standard corpus
    which is annotated with named entities of the domain
    of interest: medical studies of trauma, shock, and sepsis
    conditions. Abstracts from an annual conference dedicated
    to the subject and hosted by the the North American
    Shock Society 6 were used to identify the domain-specific
    named entities via an automated process. The named entity
    annotations had to then be validated by domain experts -
    the contributors to the conference. The Djangology system
    has been in use for two consecutive years (2008 and 2009),
    and has achieved an average contributor response rate of
    70%.

    The needs of the project led to a set of requirements common to similar highly-distributed collaborative annotation
    projects. An administration interface was needed to manage documents and users, as well as for the definition of
    annotation schemas. Annotations created via an automated
    process needed to be loaded into the system. Participants
    were notified via email and presented with a link to the
    web-based interface. After logging in, annotators were able
    to view a list of assigned documents. An intuitive webbased user interface was needed to allow participants to annotate documents with minimal instructional text. Easy and
    quick annotation access was crucial to the success of the
    project. As the time of domain experts is quite valuable,
    complicated installation or annotation instructions would
    be prohibitive. The system also needed to display interannotator agreement statistics, as well as the evaluation

    Djangology can be deployed on any web-accessible server
    and requires a Python installation, Django installation, and
    connectivity to a database server 9
    . Source code and installation instructions can be found at the project website http://djangology.sourceforge.net/. We estimate that
    end-to-end installation and configuration time for a Python
    and Django-savvy developer is less than an hour. Once
    deployed, the application can be accessed from any web
    browser - no browser plug-ins, JVM installation, or custom
    security settings are necessary, as the client-server communication is based on standard HTTP and Ajax requests.
    The application database schema (Figure 1(a)) and user interface can be rapidly extended and customized. For example, creating a new field to annotator accounts could be
    effortlessly achieved by just adding a new attribute to the
    corresponding Python model class. The corresponding web
    form and underlying database schema are transparently updated by the Django framework

    The Djangology 10 application presents administrators with
    an interface to create/modify annotation projects and manage users (Figure 2). Administrators can import documents
    (single document or batch mode) into a project, define the
    project annotation schema, create annotator accounts, and
    assign annotators to specific projects and to a list of documents. Existing annotations and documents could also be
    easily loaded into the system through custom Python scripts
    (stand-alone Django scripts) or through direct connection
    to the Djangology database. Djangology has been used to
    import manually created annotations in the Knowtator format and from the BioScope Corpus (Szarvas et al., 2008)
    as well as annotations created automatically by the Gate
    and UIMA (Ferrucci and Lally, 2004) frameworks and the
    Metamap c system from the National Library of Medicine.
    In the workflow of the system, contributors are typically
    emailed their system authentication information and presented with a link to the application (Figure 3). Once
    logged in, annotators can select one of their assigned documents and proceed with the web-based annotation interface.
    An Ajax-based web page allows contributors to highlight a
    fragment of text and assign it to one of the pre-defined annotation types (based on the project annotation schema).
    The procedure for entering new annotations and modifying
    existing annotations is intuitive and based on user interface
    conventions - text selection/right-click menu selection. The
    system is specifically designed to require minimum timeinvestment on the part of the involved annotators. No installation, configuration, or reading user manuals is necessary on the part of the contributors. Annotations are saved
    to the backend database as they are entered, ensuring that
    no work is lost. In order to save annotators’ effort, once a
    phrase is annotated, all occurrences of the phrase in the document are automatically annotated in the same type. Users
    are also given a facility to override the automatically created annotations or change the system’s default behavior. If
    desired, contributors could also mark documents as completed to alert the project administrator of the annotation
    progress.

    Once annotations are gathered from various contributors, project administrators have the ability to view interannotator agreement statistics - a variety of pair-wise
    project-based and document-based metrics are computed
    and presented in the user interface. As analysis of inter-annotator disagreement is a common task, an
    interface for a side-by-side comparison of document annotations is also provided.

    \item[MyMiner]
    MyMiner is an interactive web application based on a modular design with the purpose to assist users in biocuration and text annotation tasks. The MyMiner interface is intended to be user-friendly, not requiring installation of any local software. Each module has an export option for saving results. The time spent for processing a document is recorded in the exported file. To improve the user-friendliness, a common display layout has been adopted and conserved between application modules. The input document analysis area is located on the top of the page; options and tools are placed below the main curation zone. MyMiner combines PHP, JavaScript and AJAX to enhance user interactivity. The core of the MyMiner system covers four application modules that can be independently used or combined together following the steps of a biocuration pipeline (Supplementary Fig. S1).

    MyMiner handles any plain text, including article abstracts, document sentences, ontology terms or disease descriptions.

    The ‘File-labelling’ module is a simple to use manual text classification interface that allows classifying documents, abstracts, sentences or terms, offering the possibility to enter user-specified class labels. This module could be used for instance to classify documents as either relevant or not to a specific topic from a PubMed query. Its purpose is to cover the triage task (article selection) carried out by database annotators but it can also be used for any manual classification recording. The labelled data that result from this classification can serve as training and test sets for text categorization systems. To reduce the manual classification time, it provides the option of dynamically setting positive and negative text highlights. These are expressions that users can set at any time during the tagging process to highlight text relevant (marked in yellow) or non-relevant (marked in red) to the topic of interest. The system offers the possibility to upload the classification guidelines so that the annotator can refer to them when necessary. Users can pause and resume the curation process at any time by saving the classified document. To resume classification, the saved file is uploaded as input file. The time spent by a user to select the corresponding label is recorded. This may be useful to estimate the efficiency of annotators and the difficulty of the task.

    The ‘Compare File’ module facilitates the direct comparison of collections of labelled items generated by several approaches or persons. In addition, it is possible to create subsets from these collections based on the agreement or disagreement of annotation labels. This module could be used to compare and evaluate document classification methods between various persons or softwares. It displays a global summary with information covering: (i) the number of documents within each class; (ii) the average time needed to classify the text; (iii) the correlation between classification time and text length or (iv) the number of items tagged differently between annotators. This module allows extracting a collection of texts (known as a Gold Standard corpus) that have been labelled consistently by all annotators. Alternatively, the module can also be used to extract the borderline cases tagged differently. Hasty/inaccurate annotations can be detected from inter-annotator disagreements and/or a poor correlation between document size versus classification time. These cases can then be used to refine and improve the classification guidelines. The Compare File module has been used to estimate consistency of manual annotations between various individuals and methods (Supplementary Data Section 2).

    The purpose of the ‘Entity Tagging’ (entity mention recognition) module is to manually detect important conceptual objects within a document, a first step for further identification of annotation events and relations to populate knowledge databases. This module could be used to create a corpus of gene and protein mentions to test and train a Named Entity Recognition tool. This module offers an interactive interface allowing users to semi-automatically identify various kinds of entities within documents. It has been designed as a WYSIWYG (What You See Is What You Get) online editor that allows the addition of user-specified labels for new entity types. For the detection of important bio-entities, this module provides the automatic recognition of proteins, DNA, RNA, cell lines and cell types by integrating the ABNER tagger (Settles, 2005). The LINNAEUS system is incorporated into MyMiner to identify species and organisms (Gerner et al., 2010). Additionally, user-defined entities can be detected if terms-tags dictionaries are provided. To improve the accuracy of the annotations, tags can be edited and wrongly generated labels can be removed. To define simple relationships between entities and terms, a matrix check box display was added to this module (Supplementary Fig. S14).

    The ‘Entity Linking’ module facilitates the manual annotation of bioentities mentioned in a document with standardized identifiers. This module could be used to manually link articles to disease and protein identifiers to create a catalogue of proteins involved in pathologies. Gene/protein names are automatically recognized and displayed as a list that can be manually edited, and new entities can be added and incorrectly identified ones can be removed. For each gene/protein name, MyMiner suggests a ranked list of UniProt identifiers that utilize the UniProt search scoring mechanism (Arighi et al., 2011). Species mentions are normalized to NCBI taxon identifiers; OMIM identifiers are associated to diseases and ontology terms are linked to identifiers from submitted ontology files. For this purpose, MyMiner launches asynchronous queries to respective databases (UniProt, NCBI taxonomy, OMIM and user provided ontology file) using AJAX requests. For organisms, proteins, diseases and ontology terms, a short description is displayed to help validate potential candidate hits and to assist during the manual disambiguation of potential databases identifiers. Check boxes allow the selection of the most appropriate identifiers from the candidate list. If species are specified prior to a protein identifier search, species-specific constraints are applied to reduce the number of potential candidates from UniProt.

    \item[WAT-SL]
    WAT-SL (Web Annotation
    Tool for Segment Labeling), an open-source webbased annotation tool dedicated to segment labeling.1 WAT-SL provides all functionalities to efficiently run and manage segment labeling projects.
    Its self-descriptive annotation interface requires
    only a web browser, making it particularly convenient for remote annotation processes. The interface can be easily tailored to the requirements of
    the project using standard web technologies in order to focus on the specific segment labels at hand
    and to match the layout expectations of the annotators. At the same time, it ensures that the texts
    to be labeled remain readable during the whole
    annotation process. This process is server-based
    and preemptable at any point. The annotator’s
    progress can be constantly monitored, as all relevant interactions of the annotators are logged in a
    simple key-value based plain text format.

    WAT-SL is a ready-to-use and easily customizable
    web-based annotation tool that is dedicated to segment labeling and that puts the focus on easy usage for all involved parties: annotators, annotation
    curators, and annotation project organizers.
    In WAT-SL, the annotation process is split into
tasks, usually corresponding to single texts. Together, these tasks form a project.

    Once a segment is labeled, its background color
    changes, and the button displays an abbreviation
    of the respective label. To assist the annotators in
    forming a mental model of the annotation interface, the background colors of labeled segments
    match the label colors in the menu. All labels are
    saved automatically, avoiding any data loss in case
    of power outages, connection issues, or similar.
    In some cases, texts might be over-segmented,
    for example due to an automatic segmentation. If
    this is the case, WAT-SL allows annotators to mark
    a segment as being continued in the next segment.
    The interface will then visually connect these segments (cf. the buttons showing “->” in Figure 2).
    Finally, the annotation interface includes a text
    box for leaving comments to the project organizers. To simplify the formulation of comments,
    each segment is numbered, with the number being
    shown when the mouse cursor is moved over it.

    After an annotation process is completed, a curation phase usually follows where the annotations
    of different annotators are consolidated into one.
    The WAT-SL curation interface enables an efficient curation by mimicking the annotation interface with three adjustments (Figure 3): First, segments for which the majority of annotators agreed
    on a label are pre-labeled accordingly. Second, the
    menu shows for each label how many annotators
    chose it. And third, the label description shows
    (anonymized) which annotator chose the label, so
    that curators can interpret each label in its context.
    The curation may be accessed under the same
    URL as the annotation in order to allow annotators
    of some tasks being curators of other tasks.

    WAT-SL is a platform-independent and easily deployable standalone Java application, with few
    configurations stored in a simple “key = value”
    file. Among others, annotators are managed in this
    file by assigning a login, a password, and a set of
    tasks to each of them. For each task, the organizer
    of an annotation project creates a directory (see
    below). WAT-SL uses the directory name as the
    task name in all occasions. Once the Java archive
    file we provide is then executed, it reads all configurations and starts a server. The server is immediately ready to accept requests from the annotators.

  \end{description}

  The public distribution of annotated corpora and related resources, e.g., baselines, evaluation scripts,
  loading and formatting scripts, etc., is often enabled via open source code sharing platforms.
  Arguably the most popular options are Github\footnote{\url{https://github.com}} and
  Gitlab\footnote{\url{https://gitlab.com}}, which provide similar features despite minor
  differences in their core business models.
  It is also possible to share the corresponding resources via institutional hosting platforms or
  other ad-hoc solutions. This could be convenient in the case of legal requirements, complex licenses that are incompatible with open source idiosyncrasies or any other consideration
  that disallows full public sharing.
  In our case, all resources are publicly available in a collection of Gitlab repositories\footnote{\url{https://ehealthkd.gitlab.io}}.

  \section{Anotación Semi-Automática}

Machine learning, and specifically supervised learning, is one of the most effective tools for automating complex cognitive tasks, such as recognizing objects in images or understanding natural language text.
One of the main bottlenecks of supervised learning is the need for high-quality datasets of labeled samples on which statistical models can be trained.
These datasets are usually built by human experts in a lengthy and costly manual process.
Active learning~\cite{Cohn2010ActiveL} is an alternative paradigm to conventional supervised learning that has been proposed to reduce the costs involved in manual annotation .

The key idea underlying active learning is that a learning algorithm can perform better with less training examples if it is allowed to actively select which examples to learn from~\cite{survey}.
In the supervised learning context, this paradigm changes the role of the human expert.
In conventional supervised learning contexts, the human expert guides the learning process by providing a large dataset of labeled examples. However, in active learning the active role is shifted to the algorithm and the human expert becomes an oracle, participating in a labeling-training-query loop.
In the active paradigm,  a model is incrementally built by training on a partial collection of samples and then selecting one or more unlabeled samples to query the human oracle for labels and increase the training set.
This approach introduces the new problem of how to best select the query samples so as to maximize the model's performance while minimizing the effort of the human participant.

The simplest active learning scenario consists of  the classification of independent elements $x_i$ drawn from a pool of unlabeled samples.
Examples range from image classification~\cite{Gal2017DeepBA} to sentiment mining~\cite{Kranjc2015ActiveLF},  in which the minimal level of sampling (e.g., an image or text document) corresponds to the minimal level of decision. i.e, a single label is assigned to each $x_i$. More complex scenarios arise when the decision level is more fine-grained than the sampling level. In the domain of text mining, an interesting scenario is the task of entity and relation extraction from natural language text~\cite{zhang2012unified}.
In this scenario the sampling level is a sentence, but the minimal level of decision involves each token or pair of tokens in the sentence, and furthermore, these decisions are in general not independent within the same sentence.
In this case, it is not trivial to estimate how informative an unlabeled sample will be, since each sample has several sources of uncertainty.

This section reviews some of the most relevant research related with active learning in general, and specifically focused on entity detection and relation extraction.
One of the most important design decisions in active learning is how to intelligently select the novel unlabeled samples in the most efficient way. The underlying assumption is that we want to train a
model to the highest possible performance~(measured in precision, $F_1$, etc.) while minimizing the human cost (measured in time, number of samples manually labeled, or any other suitable metric).
This requirement is often framed as the selection of the \textit{most informative} unlabeled samples, and formalized in terms of a query strategy~\cite{survey}.
The most common query strategies for general-purpose active learning can be grouped into the following categories:

\begin{description}
\item[(i) Uncertainty sampling:] The most informative samples are considered those with the highest degree of uncertainty, given some measure of uncertainty for each sample~\cite{Lewis1994148}.

\item[(ii) Query by committee:] The most informative samples are considered those with the highest disagreement among a committee of either different models or different hypotheses from the same underlying model~\cite{seungquery}.

\item[(iii) Expected model change:] The most informative samples are considered those that produce the highest change in the model's hypothesis if they were included in the training set~\cite{NIPS2007_3252}.
In recent years, researchers in fields such as machine learning, knowledge discovery, data mining and
natural language processing, among others, have produced many approaches and techniques to
leverage the large amount of information in the Internet for a variety of tasks, from
building search~\cite{google} and recommender systems~\cite{youtube}
to improving medical diagnostics~\cite{watson}.

Among the different approaches relevant to knowledge discovery, we can recognize a
continuous spectrum of techniques, based on how much expert knowledge is used.
Heavily knowledge-based techniques are based
on rules defined in knowledge bases handcrafted by domain experts~\cite{chandrasekaran1986generic}.
These approaches have a great degree
of reliability and precision, and generally allow for more complexity in the extracted knowledge,
but are difficult to scale to large amounts of data.
In contrast, the statistical approaches consist of techniques based on pattern recognition with statistical
and probabilistic models~\cite{kevin2012machine}. These techniques scale better with large amounts of data~\cite{le2013building},
providing better recall, but often are limited to extracting simple models of knowledge,
and can be more sensitive to noisy, fake or biased information~\cite{bolukbasi2016man}.

Given these mutually complementary characteristics, several hybrid approaches have been proposed.
Recently, research areas such as ontology learning~\cite{cimiano2009ontology},
learning by reading~\cite{barker2007learning} or entity embedding~\cite{hu2015entity} have arisen.
In these areas, researchers combine techniques from machine learning, natural language
processing and knowledge representation to solve more complex problems that cannot
be dealt with using only the classical tools.

Many machine learning systems are designed to solve a domain-specific task, such as
assigning a class to an element from a predefined set of labels. These systems,
when trained with data for a particular domain, are often not applicable to other domains
or to scenarios where several different domains must be used together. Moreover,
often systems are designed to be trained once from a corpus, and don't allow for
a continuous improvement of the knowledge learned.
Recently there are attempts to build general-purpose learning systems that are always
improving while obtaining new knowledge, re-evaluating the old knowledge and refining their
own confidence~\cite{mitchell2015never}.

Additionally, it is interesting to design non-monolithic learning systems, but instead
built as a set of modular components that can be combined in different ways.
This composability would allow a continuous learning system not only to improve the
quality of the extracted knowledge, but also to learn how to tune its own internal
parameters to perform a better knowledge extraction in the future. It is conceivable
that such a system could gradually learn which types of basic processes (i.e., entity recognition, POS-tagging, etc.)
are most useful for a given domain or for a particular corpus. Likewise, such a system could
learn which types of probabilistic models provide the best results in a particular dataset.
\item[(iv) Variance and error reduction:] The most informative samples are those which produce the highest reduction in the model's generalization error or, as a proxy, its variance~\cite{roy2001toward}.
\end{description}

Expected model change (iii) and variance/error reduction (iv) strategies are heavily dependent on the specific learning model used.
In contrast, uncertainty sampling (i) and query by committee (ii) are  applicable in general with a high degree of model agnosticism.
Furthermore, relevant subsets of both strategies can be formalized under a single framework if we define the uncertainty as a measure of the entropy of the model's predicted output.
In this framework, query-by-committee can be implemented via weighted voting, thereby assigning empirical probabilities to the possible outputs.

Weighted density is a complimentary strategy in which the most informative samples are weighted by how representative they are of the input space, for example, by measuring their similarity to the remaining samples~\cite{settles2008analysis}.
This approach attempts to counter-balance a noticeable tendency to select outliers as the most informative samples ---a problem associated with other query strategies--- since outliers are often the samples that create the highest amount of uncertainty, disagreement or hypothesis change.

Recent advseplnances in natural language processing have produced an increased interest in active learning to alleviate the requirement for large annotated corpora~\cite{Olsson2009ALS, Tchoua2019ActiveLY}.
\citet{settles2008analysis} compare several strategies for active learning in sequence labeling scenarios, concluding that query strategies based on measures of sequence entropy combined with weighted sampling outperform other variants.
\citet{Meduri2020ACB} propose a comprehensive benchmark to evaluate different active learning strategies for entity matching.
In the task of named entity recognition, CRF models have been used to select query samples
\citep{Claveau2017StrategiesTS, Lin2019AlpacaTagAA}.
The task of relation extraction also benefits from active learning approaches, both in general-purpose settings~\cite{fu2013efficient} and in domain-specific settings~\cite{zhang2012unified}.
However, despite the growing body of research, it is still a challenge to apply active learning in joint entity recognition and relation extraction, especially in scenarios with low resources~\cite{Gao2019ActiveER}.

  \section{Recursos Lingüísticos}

  Different semantic relations have been established  in the state of the art, many of these giving rise to the construction of corpora. We focus on two approaches: corpora or annotation models to represent knowledge in many domains as well as those specifically about health.
  The table~\ref{tab:corpora}  presents the seven characteristics relevant to our corpus and indicates which of them are present in a sample of corpora from the state-of-the-art.
  These characteristics can be understood in the following terms:
  \begin{enumerate}
  \item \textit{general-purpose annotation:} applicability of the underlying annotation model to any domain;
  \item \textit{independence of syntax:} capturing semantic aspects rather than syntactic relations in sentences;
  \item \textit{ontological knowledge:} supporting inheritance and composition between concepts;
  \item \textit{composite concepts:} allowing the annotation of concepts that involve other sub-concepts;
  \item \textit{attributes:} modeling attributes for each annotated entity such as quantifiers~(e.g., number of occurrences) or qualifiers~(e.g., degree of certainty);
  \item \textit{contextual relations:} modeling relations that only occur when conditioned by a specific context; and,
  \item \textit{causality / entailment:} including relations for representing causality and/or entailment.
  \end{enumerate}

  \begin{table}[htb]
      \centering
      \begin{tabular}{ll|c|c|c|c|c|c|c|c}
          & \textbf{Characteristics} & \rotatebox{90}{\textbf{Ixa MedGS}~\cite{ORONOZ2015318}} & \rotatebox{90}{\textbf{DrugSemantics}~\cite{moreno2017drugsemantics}} & \rotatebox{90}{\textbf{DDI}~\cite{herrero2013ddi}} &
          \rotatebox{90}{\textbf{Bio AMR}~\cite{bioamr}} &
          \rotatebox{90}{\textbf{YAGO}~\cite{suchanek2007yago}} & \rotatebox{90}{\textbf{ConceptNet}~\cite{speer2017conceptnet}} & \rotatebox{90}{\textbf{eHealth-KD v1}~\cite{ehealth}} &
          \rotatebox{90}{\textbf{eHealth-KD v2}} \\ \midrule
          1 & general-purpose annotation &     &     &     & \ok & \ok & \ok & \ok & \ok \\
          2 & independence of syntax      & \ok & \ok & \ok &     & \ok & \ok & \ok & \ok \\
          3 & ontological knowledge      &     &     &     & \ok & \ok & \ok & \ok & \ok \\
          4 & composite concepts  &     &     &     & \ok &     &     & \ok & \ok \\
          5 & attributes        &     & \ok &     & \ok & \ok &     & \ok & \ok \\
          6 & contextual relations       &     &     &     & \ok &     &     &     & \ok \\
          7 & causality / entailment     & \ok &     &     & \ok &     & \ok &     & \ok \\
          \bottomrule
      \end{tabular}
      \caption{Comparison between the \textit{eHealth-KD v2} corpus and other corpora with respect to
      the characteristics that define our proposal.}
      \label{tab:corpora}
  \end{table}

  \begin{table}[h!]\centering
    \footnotesize{
    \begin{tabularx}{\hsize}{X|XXXXX}
        \hline
        \hline
      \textbf{Corpus} & \textbf{Drug Semantic} & \textbf{Ixa MedGS} & \textbf{CLEF} & \textbf{DDI} & {\textbf{BARR2}} \\
        \hline \\
      \textbf{Doc. Type} & Product summaries & Discharge summaries & Clinical documents & Abstracts & Clinical case studies \\
        \textbf{Annotation Type} & Manual & Auto/Manual check & Manual &  Auto/Manual check & Manual \\
        \textbf{Annotators} & Experts & Experts & Experts \& Non-experts & Expert & Experts \\
        \textbf{Schema} & Medical entities & Medical entities & Medical entities & Medical entities & Medical abbreviations \\
        \textbf{Language} & Spanish & Spanish & English & English & Spanish \\
        \textbf{Documents} & 5~(16\%) & 75~(0.01\%) & 150~(0.27\%) & 1025~(100\%) & 648(20\%) \\
        \textbf{Origin} & AEMPS & Galdacao-Usansolo Hospital & Royal Madersen Hospital & Medline, Drug Bank & PubMed, IBCECS \& SciELO \\
        \hline
        \hline
    \end{tabularx}
    } %%% CHANGE add BARR2 corpus
    \caption{Summary of related corpora annotated with domain-specific entities for the health domain.
            {Percentage values for \textbf{Documents} indicate how many of the original documents were actually annotated, as reported by the original authors.} %%%%CHANGE explain percentages
            \label{tab:stateofart}}
    \end{table}


    \begin{table}[h!]\centering
    \footnotesize{
    \begin{tabularx}{\hsize}{X|XXXX}
        \hline
        \hline
      \textbf{Corpus} & \textbf{Bio AMR} & \textbf{Yago} & \textbf{Emotinet} & \textbf{eHealth-KD} \\
        \hline \\
      \textbf{Doc. Type} & Sentences & Sentences & Posts & Sentences\\
        \textbf{Annotation Type} & Manual & Automatic & Manual & Manual \\
        \textbf{Annotators} & Non-experts & Non-experts & Non-experts & Non-experts \\
        \textbf{Schema} & AMR & {SPO} & {SAOE} & {SAT+R} \\ %%%% CHANGE add basic semantic relations
        \textbf{Language} & English & English & Spanish \& English \& Italian & Spanish \\
        \textbf{Documents}  & 6542 &  ----- &  & 1173(11.8\%) \\
        \textbf{Origin} & PubMed & Wikipedia, WordNet & Blog & Medline Spanish XML\\
        \hline
        \hline
    \end{tabularx}
    }
    \caption{Summary of related corpora annotated with a general-purpose schema, or not specific to the health domain.
    {Percentage values for \textbf{Documents} indicate how many of the original documents were actually annotated, as reported by the original authors. SPO: Subject, Predicate, Object triplets; SAT+R: Subject, Action, Target triplets and additional Relations (see section~\ref{sec:problem}); SAOE:  Subject, Action, Object, Emotion tuples.}%%%%CHANGE explain percentages and abbreviations
    \label{tab:stateofart2}}
    \end{table}

    \subsection{Descripción de los recursos}

    \begin{description}
      \item[Ixa MedGS]
      \item[DrugSemantics]
      \item[DDI]
      \item[Bio AMR]
      \item[YAGO]
      \item[ConceptNet]
      \item[CLEF]
      \item[BARR2]
      \item[Emotinet]
    \end{description}

    Most health-related corpora are annotated using self-defined health
    related entities relevant to the task at hand.
    Of these, arguably one of the most used is the CLEF
    corpus~\cite{kelly2016overview}.
    This corpus contains 150 English clinical documents, manually annotated by a
    team of experts (clinical and biologists) and non-experts.
    In contrast, the DDI corpus~\cite{herrero2013ddi}, which contains 1025 English
    documents from Medline was pre-annotated automatically, and then manually
    checked by domain experts (Pharmacists).
    Similar corpora in Spanish language exist.
    The Drug Semantic corpus~\cite{moreno2017drugsemantics} is an example, where
    domain experts (Registered Nurses and students) manually annotated Spanish
    summaries of product characteristics.
    {Likewise, the BARR2}~\cite{barr2} {corpus contains manually annotated abbreviation-definition
    pairs in Spanish clinical papers extracted from bibliographic databases.}
    %%% CHANGE add BARR2 corpus
    On the other hand, the Ixa MedGS corpus~\cite{oronoz2015creation} was
    pre-annotated automatically and then manually checked by domain experts in
    Pharmacology.
    An interesting alternative is the Bio AMR corpus~\cite{bioamr}, which contains
    AMR annotations of several medical documents, hence combining a general purpose
    annotation schema in a specific domain.

    In the context of general domain knowledge, one of the most relevant resources
    for our research is YAGO~\cite{fabian2007yago}.
    It consists of a large knowledge base automatically extracted from Wikipedia,
    WordNet, and other sources.
    Since YAGO is intended to represent general domain knowledge, its semantic
    structure is defined in terms of fact triples, in the spirit of RDF and other
    ontological representations.
    In contrast, the Emotinet knowledge base~\cite{emotinet} is oriented towards a
    specific domain (emotions), and is built from the manual annotation of blog
    entries, using a general semantic structure that links entities, actions, and
    emotions.
    Although Emotinet is designed for a particular domain, its structure is rather
    general, in the sense that it can readily represent any type of event or action
    performed by entities.

    As Table \ref{tab:stateofart} shows, the type of documents used is highly
    variable, which provokes large differences in terms of the length of documents,
    structure of discourse and vocabulary.
    An interesting characteristic is the type of annotation, either manual,
    pre-automated with expert review, or fully automated.
    Although recent research shows an increasing tendency towards pre-automated
    or fully automated annotation, manual annotation is still regarded as
    more reliable.

    Health related corpora are usually annotated by experts with a domain-specific
    semantic structure, such as entities related to diseases, drugs, genes, or
    treatments.
    Given the complexity of the concepts in the medical domain, annotators usually include
    medical doctors or other specialists of the medical domain.
    In these resources, very few general-purpose natural language features are used.
    This provides a greater detail of semantic information, since the entities and
    relations are relevant for the domain at hand.
    However, in the same sense, it might discard important information in the text
    which cannot be represented with the structure defined.
    This may or may not be an issue for a specific line of research.
    In our case, we consider it important to extract as much knowledge as
    possible from each source.
    In contrast, general purpose corpora or knowledge bases are usually annotated
    by non-experts with a semantic structure designed to represent as much
    knowledge as possible.
    This strategy tends to increase recall (a larger amount of facts is extracted)
    but it might extract irrelevant or incorrect facts.
    In these cases, the annotation schema relies largely on natural language semantics, such as Subject-Predicate-Object triples.

    The trend of representing knowledge with a general structure has been aided by recent
    advances in Teleologies~\cite{teleologies} that provide a theoretical framework for
    representing general purpose facts using a small set of concepts (objects,
    actions and functions).
    In contrast with Abstract Meaning Representation (AMR), the Teleologies framework is not
    specifically aimed at natural language understanding, but at representing
    the semantics of a general knowledge domain. This type of framework is less dependent
    on the linguistic characteristics of a specific language.
    The Subject-Action-Target structure defined in this paper is based on a simplification
    of the Teleologies conceptualization, applied to the domain of medical texts.
    However, inspired by general purpose knowledge bases, we also include a few specific
    semantic relations that are broadly used in general purpose ontologies and semantic networks.
    This combination (i.e. SAT+R, see Section~\ref{sec:problem}) makes the annotation schema used in eHealth-KD novel.

    \subsection{Características generales}

  \paragraph{General-purpose annotation}
  General-purpose annotation models are often used in corpora extracted from encyclopedic sources, such as \textit{YAGO}~\cite{suchanek2007yago} and \textit{ConceptNet}~\cite{speer2017conceptnet}, both of which contain facts automatically extracted from Wikipedia~(among other sources). In contrast, domain-specific annotation models are usually employed when the source is more restricted to a specific domain. Examples include \textit{Ixa MedGS}~\cite{ORONOZ2015318}, which contains health related concepts for diseases, causes and medications; \textit{DrugSemantics}~\cite{moreno2017drugsemantics}, which annotates health entities, drugs and procedures; and, \textit{DDI}~\cite{herrero2013ddi}, which annotates drug-drug interactions. A middle ground is the \textit{Bio AMR}~\cite{bioamr} corpus, which applies a general purpose annotation model~(AMR)~\cite{banarescu2013abstract} to health documents. The \textit{eHealth-KD v2} corpus is similar to the latter in this respect, since the annotation model defined is general, but it is applied specifically to health sentences in this research.
  The \textit{eHealth-KD v2} corpus constitutes the result of the evolution of the \textit{eHealth-KD v1}~\cite{ehealth} corpus.

  Most of the aforementioned resources are focused on capturing the semantics of sentences, in the sense that very different sentences with the same facts are likely to be similarly annotated. We consider \textit{BioAMR} less independent of syntax because even though AMR is a semantic annotation model---far more abstract than dependency parsing, for example---, it still relies heavily on sentence grammatical structure. Hence, a significant change in the sentence structure is likely to change the annotation, even if the underlying semantic message remains unchanged. For example, since AMR uses PropBank~\cite{propbank} roles, changing a word for a semantically similar word, including a synonym, will probably change the corresponding annotation and thereby the available roles.
  This also makes AMR and similar resources language-dependent, not only in practice given their dependence on the existence
  of word banks, but also in nature. While attempting to apply AMR in Spanish, \citet{migueles2018annotating} show that even though AMR is theoretically language-agnostic,
  the existing annotation guidelines are biased towards English and must be adapted to capture linguistic phenomena
  that don't exist in English.
  The annotation model designed in this research for the \textit{eHealth-KD v2} corpus, attempts to achieve a higher level of syntactic independence, in part by using a smaller set of entities, relations and roles than AMR. More specifically, our annotation model does not distinguish semantic roles for each possible \texttt{Action}, instead relying on general purpose roles~(i.e., \texttt{subject} and \texttt{target}, see Section~\ref{subsec:model}).

  \paragraph{Ontological knowledge}
  General-purpose annotation models often allow ontological knowledge to be represented in the form of inheritance and composition between concepts. In this context, we consider the ability to recognize and annotate  these ontological relations in the source text. Health-related annotation models do not usually deal with this problem, mainly because the entities and relations to annotate form a predefined ontology where composition and hierarchy, if any exist, are already conceived in the annotation model itself. However, general purpose annotations often include relations like \textit{ConceptNet}'s \texttt{is-a} or \texttt{part-of} that directly represent these ontological concepts, and are thus able to extract ontological representations from natural text.

  \paragraph{Composite concepts}
  The model designed for the \textit{eHealth-KD v2} corpus also includes relations specifically for this purpose, mostly inspired by \textit{ConceptNet} and \textit{YAGO}.
  Composite concepts, in contrast, refer to the ability to annotate concepts that are formed by a fine-grained combination of other entities, in the same sentence. For example, take the sentence: ``\textit{the doctors that work the night shift get paid extra hours}''. \textit{AMR} allows for the representation of the concept that not all doctors, but only those that work the night shift, are the ones who get paid extra hours. Our proposal also includes several annotation patterns to deal with this type of scenario.

  \paragraph{Attributes}
  Attributes are often used to further refine the meaning of annotated entities. Examples include quantifiers in \textit{AMR}, or modifiers that specify a degree of uncertainty, or a negation of a concept. Our proposal includes four general-purpose attributes that model uncertainty, negation and qualifiers for expressing emphasis.

  \paragraph{Contextual relations}
  Contextual relations, as defined in the \textit{eHealth-KD v2} corpus, allow  facts that only occur
  under certain conditions to be represented, for example, in a specific time frame or location or under certain assumptions. This allows for a finer-grained semantic annotation. \textit{BioAMR} inherits this ability from \textit{AMR}, which allows modifiers for expressing \textit{how}, \textit{when}, \textit{where} or \textit{why} some event occurs. In our proposal, we provide contextual relations that specify time and location, and an additional general-purpose relation for other conditions.

  \paragraph{Causality and entailment}
  Causality and entailment are general-purpose relations that allow some level of inference or reasoning. The \textit{Ixa MedGS} corpus defines a \texttt{causes} relation, since it is relevant in the domain the corpus is modeling. Likewise, \textit{AMR} and \textit{ConceptNet} include similar relations. Our proposal includes both causality and entailment as two different relations with well-defined semantic meanings.

  \section{Sistemas de Aprendizaje Automático para el Descubrimiento de Conocimiento}

  In recent years, researchers in fields such as machine learning, knowledge discovery, data mining and
natural language processing, among others, have produced many approaches and techniques to
leverage the large amount of information in the Internet for a variety of tasks, from
building search~\cite{google} and recommender systems~\cite{youtube}
to improving medical diagnostics~\cite{watson}.

Among the different approaches relevant to knowledge discovery, we can recognize a
continuous spectrum of techniques, based on how much expert knowledge is used.
Heavily knowledge-based techniques are based
on rules defined in knowledge bases handcrafted by domain experts~\cite{chandrasekaran1986generic}.
These approaches have a great degree
of reliability and precision, and generally allow for more complexity in the extracted knowledge,
but are difficult to scale to large amounts of data.
In contrast, the statistical approaches consist of techniques based on pattern recognition with statistical
and probabilistic models~\cite{kevin2012machine}. These techniques scale better with large amounts of data~\cite{le2013building},
providing better recall, but often are limited to extracting simple models of knowledge,
and can be more sensitive to noisy, fake or biased information~\cite{bolukbasi2016man}.

Given these mutually complementary characteristics, several hybrid approaches have been proposed.
Recently, research areas such as ontology learning~\cite{cimiano2009ontology},
learning by reading~\cite{barker2007learning} or entity embedding~\cite{hu2015entity} have arisen.
In these areas, researchers combine techniques from machine learning, natural language
processing and knowledge representation to solve more complex problems that cannot
be dealt with using only the classical tools.

Many machine learning systems are designed to solve a domain-specific task, such as
assigning a class to an element from a predefined set of labels. These systems,
when trained with data for a particular domain, are often not applicable to other domains
or to scenarios where several different domains must be used together. Moreover,
often systems are designed to be trained once from a corpus, and don't allow for
a continuous improvement of the knowledge learned.
Recently there are attempts to build general-purpose learning systems that are always
improving while obtaining new knowledge, re-evaluating the old knowledge and refining their
own confidence~\cite{mitchell2015never}.

Additionally, it is interesting to design non-monolithic learning systems, but instead
built as a set of modular components that can be combined in different ways.
This composability would allow a continuous learning system not only to improve the
quality of the extracted knowledge, but also to learn how to tune its own internal
parameters to perform a better knowledge extraction in the future. It is conceivable
that such a system could gradually learn which types of basic processes (i.e., entity recognition, POS-tagging, etc.)
are most useful for a given domain or for a particular corpus. Likewise, such a system could
learn which types of probabilistic models provide the best results in a particular dataset.

  The speed and volume at which information is produced has increased exponentially in the last decade,
   mainly due to the rise of the social networks and the mobile technology. In order to cope with this volume of information,
   it is necessary to be able to process massive amounts of data continuously.
    The field of machine learning provides tools for the automatic extraction of information and knowledge from different
    sources of data.
    Machine learning not only allows to automate processes and tasks of knowledge discovery or text mining, but also provides a large
    improvement in the scalability of these processes~\cite{wu2014data}. By using mass computing resources, it is possible to process millions
    of raw documents in a reasonable time, far exceeding what can be done by domain experts.
    Recent improvements in computing capabilities and access to larger datasets have given rise
    to the field of deep learning, which has improved the state of the art in
    several of the classic machine learning tasks~\cite{lecun2015deep}.

	Arguably, the two most common approaches in machine learning are supervised and unsupervised learning~\cite{kevin2012machine}.
    Supervised learning can be used for recognizing specific elements of knowledge in a source of data. For example, tagging
    pieces of text to indicate that they define an entity~\cite{nadeau2007survey} (e.g., a person, organization, or place), recognizing relations between
    said entities, or assigning a sentiment or opinion score~\cite{liu2012sentiment} to a fragment of text. On the other hand, unsupervised learning
    can help with finding relevant structure in a large set of elements. Clustering algorithms can be used to detect similar
    concepts or to extract abstract concepts from groups of more concrete elements. Other techniques can be used for reducing
    the amount of information, for example, to remove noisy, uncertain or irrelevant pieces of
    information~\cite{bingham2001random}.

    In general, most machine learning algorithms are not designed to represent the learned knowledge in complex structures,
    such as those defined by human domain experts (i.e., ontologies). In turn, the representations often have a simple structure, such
 	as a probability distribution or a correlation matrix~\cite{bengio2013representation}.
    When applying these algorithms to a real problem, an domain-specific
    interpretation of those representations has to be made.
    % Esta es una debilidad imporante, la no explicación
    Furthermore, many of the most powerful machine learning models are difficult to explain, in the sense
    that when the system produces an answer, a human expert cannot easily understand and reproduce the inference steps
    that the system performs~\cite{olden2002illuminating}.
    % creo mejor decir que si la representacion es tan importante las ontologias son una forma de representar conocimiento
    % y entonces meter el gancho de embeddings y entity embeddings
    Choosing an appropriate representation is decisive for the success of most machine learning techniques~\cite{bengio2012deep}.
    In recent years, there has been an increased interest in the problem of automatically learning relevant representations.
    Word embeddings~\cite{mikolov} and more general entity embeddings~\cite{hu2015entity} represent the first steps towards powering
    deep learning approaches with more explainable internal representations.
    Since ontologies are, by definition, representations of a given conceptualization, it is conceivable that using ontologies
    as seeds for the representation of a given domain, the performance of data mining processes based on machine learning can
    be improved.

  \section{Entornos de Evaluación Competitivos}

  A strategy often used to encourage research on a specific task is the organization of a shared
  evaluation campaign. In contrast with regular research, evaluation campaigns often have a fixed
  time frame, and evaluation resources are not fully disclosed~(e.g., gold annotations for
  test sets are hidden) to allow a fair comparison in a friendly competitive environment.
  In this section, we analyze relevant efforts for organizing evaluation campaigns for both the biomedical domain or for dealing with entity and relation extraction.

  Several online services allow researchers to organize machine learning challenges and competitions, providing automatic grading, user management, and other useful features.
  Kaggle\footnote{\url{https://kaggle.com}} is arguably the most popular choice, its main limitation for our purposes being that to host a challenge, organizers must contact the service providers. Possible alternatives are AIcrowd\footnote{\url{https://www.aicrowd.com}} and
  Codalab\footnote{\url{https://codalab.org}} which provide free options for challenge organizers.

  The CLEF eHealth Evaluation Lab has proposed several challenges in the biomedical domain, including
  named entity recognition~\cite{clef2013} and information extraction~\cite{clef2014} in English,
  and later editions in French documents~\cite{clef2015, clef2016}.
  In these challenges, medical reports from MEDLINE, EMEA and similar sources are annotated with disorders, medical terms, acronyms and abbreviations, which provide evaluation scenarios
  for several NLP tasks, including entity recognition, normalization and disambiguation.
  Another relevant task is proposed by~\citet{semeval2017-task9} in Semeval 2017, focused on AMR parsing and generation from biomedical
  sentences in English. Applying a general-purpose conceptualization, such as AMR, to specific domains encouraged participants to bridge the gap between developing generalizable techniques and applying domain-specific heuristics.
  However, AMR parsing is already a complex problem in itself, which can negatively impact on researcher participation in these challenges if they are not specialized in AMR.
  Simpler, general-purpose models can encourage a greater degree of participation given the easier entry curve.
  An example of the latter is the Semeval 2017 Task 10~\cite{semeval2017-task10},
  a challenge regarding keyphrase and relation extraction from scientific documents,
  with a simple model based on three entity classes and two general-purpose relations.
  This task received a much larger number of submissions than the former, even though both challenges where hosted on the same venue and aimed at similar audiences.

  As can be expected, English is the most prominently used language in NER-related challenges, given the larger
  number of available corpora and resources. However, important efforts have been devoted to fostering research
  in less prominent languages. Relevant to our discussion are the IberLEF campaigns that focus on Iberian languages,
  such as Spanish, Portuguese, Catalan, and other regional variations.
  Two examples of recent NER-related tasks are the Portuguese Named Entity~\cite{glauber2019iberlef} challenge and the MEDDOCAN~\cite{marimon2019automatic} document anonymization challenge.
  The first proposes entity recognition and relation extraction in the general domain, in Portuguese.
  The second proposes the identification of privacy-sensitive entity mentions in medical documents, e.g., names, addresses,
  dates, ages, etc.

  Outside the frame of a competition, open, long-running evaluation systems allow
  researchers to evaluate their approaches with official evaluation metrics.
  This can also provide a centralized repository of the state-of-the-art, where existing approaches are
  summarized and linked to existing papers.
  In this regard, this research proposes an online evaluation system that allows a comparison of
  new approaches with officially published results at any time. Based on this infrastructure, official evaluation campaigns with a more competitive design are organized in scheduled time-frames.

  \section{Representación del Conocimiento}

  There are many resources used in the task of
natural language processing. Among the most common ones, ontologies, as representations
of concepts, data types and their relations in an explicit computational structure.
Some of the most popular ontologies used in NLP are WordNet~\cite{} and DBpedia~\cite{}.
For their ubiquity and usefulness, research in ontologies has increased in the last years,
particularly regarding the automatic creation of these structures,
giving birth to the field of Ontology Learning.
Ontology Learning has the potential of reducing the cost of creating
and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.

Ontology Learning deals with the problem of automatically creating an
ontology from several resources. Examples include mining analysis, mostly
the study of text in natural language~/cite{}. In this
particular task, several approaches have been presented, from syntactic and
semantic analysis of corpora~\cite{} to statistical and machine learning based
approaches~\cite{}.

Several tools and systems have been proposed for this task.
Text2Onto~\cite{cimiano2005text2onto} is a framework for
data-driven change discovery which employs a probabilistic
ontology model (POM).
OntoLT~\cite{buitelaar2004ontolt} extracts concepts and
relations automatically from linguistically annotated text collections,
by means of rule set, which maps linguistic classes to ontology classes.
A newer approach is OntoGain~\cite{drymonas2010unsupervised},
which uses an unsupervised approach,
and exploits the inherent multi-word term's lexical information
in order to extract higher level concepts.

One of the issues with these approaches is the amount of spurious
information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
In general, there will be many unimportant
or redundant pieces of information in the analyzed corpora. A naive
approach that doesn't take this issue into consideration will create
immense ontologies with very little useful information. To tackle
this problem, OntoGain proposes a hierarchical clustering scheme
that attempts to identify general concepts and relations.

In general these resources are focused in the extraction of knowledge and
pay much less attention to the task of find relevant knowledge.
An ontology is interesting not just because of its size, but
because of how is such information selected, processed and stored.
In this problem we focus our work, attempting not only
to recognize information, but also to reduce this information and obtain
information with more relevance.

-----------

The problem of discovering, storing and using knowledge in a computationally effective
form has been widely studied~\cite{mitchell2015never, ROSPOCHER2016132, cimiano2009ontology}.
This problem has been treated from two different but complementary research areas:
the fields of knowledge representation and machine learning.
The knowledge representation community provides means for
computationally representing and operating with stored knowledge in forms that
can ensure some degree of logical consistency.
Conversely, the machine learning community provides
tools for obtaining useful knowledge from large collections of structured and unstructured data.
Recenlty, a new discipline named ontology learning has arisen, which draws ideas
and techniques from both the knowledge representation and the machine learning fields.
This field deals with the problem of automatically building ontological representations
of knowledge from a variety of data sources. As such, the theory of ontology learning
is relevant in the design of the framework presented in this paper.
In this section we present a brief review of the relevant concepts and technologies
in each of these three domains, as well as some approaches similar to our proposal.

	\subsection{Knowledge Representation and Reasoning}

    Since the dawn of computer science, one of the problems that has attracted wide attention
    is that of representing knowledge in a computational format, such that automatic reasoning
    can be performed to discover new, previously unknown truths~\cite{sowa2000knowledge}.
    Arguably, the most popular knowledge representation technology in use are
    ontologies~\cite{guarino1995formal}, which have
    become the \emph{de facto} standard.
    %Definición
	Ontologies can been defined as a formal specification of a conceptualization~\cite{asuncion2003}.
    This represents concepts, relations between these concepts, instances of these concepts and inference rules
    for deriving new relations.

    As such, ontologies can be considered as a combination of two predominant approaches
    for knowledge representation: those based on formal logic~\cite{brachman1992knowledge}
    and those based on graphs of semantic relations~\cite{chein2008graph}.
    In logic-based approaches the facts are represented as logic predicates or functions
    and reasoning is enabled through the application of formal inference rules.
    In contrast, graph-based representations express facts as nodes~(objects)
    and edges~(relations) and reasoning is built on top of graph traversing methods.
    However, in ontologies objects and their attributes and relations are represented in a graph of concepts,
    which can also be interpreted as a set of predicates and functions on these objects.
    On top of this layer, inference rules can be added,
    which enable logical reasoning methods to be used for deriving new attributes and
    relations between existing concepts.

	Relations in an ontology can be of a specific domain, but often some general domain
    relations are represented, such
    as \textit{is-a} and \textit{part-of}. These types of relations allow representing more abstract
    or complex concepts out of the composition of more concrete or simple concepts.
    Hence many ontologies contain some kind of taxonomy of increasingly abstract concepts,
    that are also interconnected
    with each other using other semantical relations which can be domain specific.
    % Ventajas
    These resources allow representing complex frameworks of knowledge, down to a degree of specificity which
    enables the design of fully automated reasoning tools which use this knowledge for
    a variety of computational tasks.
    % Dificultades o desventajas
  	Due to the high complexity of the concepts and relations that are represented,
    and the experience needed to recognize the most relevant concepts of a domain,
    ontologies are usually manually constructed by domain experts~\cite{wong2012ontology}.
    Thus, building an ontology is a process that requires a long time and a large number of experts to
    define and populate it with relevant instances that refer to objects and relations.
    This makes really difficult build hand crafted ontologies and ensure their maintainability, due to day a day huge quantity of new and valuable information, desirable to convert into knowledge, appear in the World Wide Web.
    Another important outcome of this process is that experts usually represent only facts that are absolutely
    true in the domain. Although existing ontology formats can be extended to deal with fuzzy~\cite{fuzzyontology} or
    vague data~\cite{bobillo2011fuzzy}, manually assigning a degree of belief to a specific fact is a complex task.

%     % Ejemplos
    It is possible to distinguish between two types of ontologies: general domain (or upper ontologies) and domain-specific (or simply domain ontologies).
    Domain-specific ontologies are those which deal with the concepts and relations of the particular knowledge domain.
    As examples, we can cite ontologies in the medical sciences~\cite{rector2003opengalen,gene2004gene},
    or the software engineering field~\cite{4641930}.
    Other ontologies are more general since they can be used in different domains, or they are used for general
    purpose tasks which are employed in many areas.
    WordNet~\cite{miller1995wordnet} is a general purpose ontology that contains most of the words of the English language,
    and syntactic and semantic relations between them.
    It is used in many tasks in natural language processing and text mining.
    DBPedia~\cite{mendes2012dbpedia} is an encyclopedic ontology that contains part of the knowledge present in the
    Wikipedia\footnote{http://www.wikipedia.org}.
    It relates people, historical events, facts, locations, and other concepts, in a structured and queryable format.
    Since ontologies have a unified form of representing a single fact, concept or relation using Uniform Resource Locator~(URL), it is
    possible and very common for different ontologies to link each other. For example, many domain specific ontologies
    have entities which are linked to the corresponding entry in DBPedia. The approach of linking and referencing to other
  	widely known ontologies, known as \textit{linked data}~\cite{bizer2009linked},
    enables the standarization of the representation of shared knowledge and eases the tasks of querying and analyzing it.

    Ontologies are an effective tool for representing knowledge in a wide
    variety of domains and scenarios~\cite{staab2010handbook}.
    They are flexibly enough to adapt to a particular domain and powerful enough to represent complex concepts.
    However, one of the most complex task in this sense is maintaining
    an ontology up-to-date with respect to the massive amount of unstructured data that is generated and published every day.
	Therefore, the need arises for computational tools to build ontologies with automated or semi-automated processes.

    \subsection{Ontology Learning}

    In the intersection of machine learning and knowledge representation,
      the field of ontology learning has arisen to deal with the
      complexity of manually maintaining and updating ontologies.
      This field draws techniques and tools from both communities,
      to automate part of the process of creating and maintaining ontologies.
    Ontology learning has the potential of reducing the cost of creating
    and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.
      This problem is also addressed in
      learning by reading~\cite{barker2007learning}, a field which draws techniques
    the natural language processing and knowledge representation and reasoning communities.
    The purpose is to build a formal representation of some particular field given unrestricted
    textual data related to the field. This representation must also allow fully automatic
    reasoning.
      Learning by reading can be considered as a particular case of ontology learning,
      even though it is only concerned with textual input, and the
      output is not necessarily formated as an ontology.

      In the field of ontology learning, two general high level tasks can be distinguished:
      ontology population and ontology enrichment~\cite{petasis2011ontology}.
      Ontology population deals with the
      sub-problem of finding new instances for an already defined ontology, while
      ontology enrichment deals with adding new concepts and relations to an existing
      ontology. There is an overlap between these tasks, and most of the existing
      approaches cannot be classified purely in these terms.
      In this field, several tools have been proposed, which
      combine different approaches and solve different subsets of the ontology
      learning tasks. A brief review of these systems can help defining the
      main characteristics that our framework should have.
      % No sé si decir algo como a continuación se presentan varias de ellas
      % atendiendo a sus características mas relevantes. Esto nos permite conocer
      %la diversidad de enfoques y las potensialidades que debemos tener en cuenta
      %para crear nuestro sistema

      % general (early approach)
      Early approaches, such as SYNDIKATE~\cite{syndikate}, deal only with populating a
      knowledge base, with a predefined ontological structure~(classes and relations).
      % web approaches
      Since the Web is a rich source of information, several approaches have focused on extracting knowledge
      from it, exploiting the semi-structured format of web resources.
      Some systems like ARTEQUAKT~\cite{artequakt} and SOBA~\cite{soba} are domain-specific,
      respectively focusing on the art and the sports domains.
      Other systems, like WEB-$>$KB~\cite{webkb} attempt to build general domain knowledge bases from the
      web, exploiting also the structure of links between pages to identify relations.
      % from structured data
      Another example is the VIKEF~\cite{vikef} system, which uses product catalogs as sources of data, hence exploiting the
      inherent structure present in this type of data.
      % bootstraping with human knowledge
      Even though most systems attempt fully-automatic extraction, some examples like ADAPTATIVA~\cite{adaptativa} include
      a bootstrapping strategy, where human experts provide feedback about the extracted knowledge.

      In order to extract relevant knowledge from unrestricted text, NLP techniques have
      been introduced in systems such as OPTIMA~\cite{optima} and ISODLE~\cite{isolde}.
      % rule-based
      The use of natural language features can be used build rule-based systems,
      like the OntoLT~\cite{buitelaar2004ontolt} proposal, that extract concepts and
    relations via a mapping of linguistic classes to ontology classes.
      % based on statistical models
      An alternative approach is to use statistical or probabilistic models,
      exemplified by systems such LEILA~\cite{leila} or Text2Onto~\cite{cimiano2005text2onto}.
    % relevance
      Another example is KnowItAll~\cite{knowitall}, which introduces a point-wise mutual information
      metric to select relevant instances.

    Once instances of entities are relations are extracted from text, a natural question is
      wether more abstract knowledge can be inferred from these examples.
      The systems who address this issue often use unsupervised techniques to attempt
      to discover inherent structures. Two relevant examples of this approach
      are OntoGain~\cite{drymonas2010unsupervised} and ASIUM~\cite{asium},
      which attempt to automatically build a hierarchy of concepts using clustering techniques.
      % multimedia
      The BOEMIE~\cite{boemie} system is another interesting example, since it
      attempts to automatically infer abstract concepts from the concrete instances found,
      but focuses not only on text, but also on multimedia sources such as images, and videos.
    % never-ending learning
      Most of the mentioned systems usually focus on one iteration of the extraction process.
      However, more recent approaches, like NELL~\cite{mitchell2015never}, attempt
      to learn continuosly from a stream of web data, and increase over time
      both the amount and the quality of the knowledge discovered.

    One of the issues with many of these approaches is the amount of spurious
    information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
    In general, there will be many unimportant
    or redundant pieces of information in the analyzed corpora. A naive
    approach that doesn't take this issue into consideration will create
    immense ontologies with very little useful information. To tackle
    this problem, OntoGain proposes a hierarchical clustering scheme
    that attempts to identify general concepts and relations.

    In general, these tools are focused on the extraction of knowledge and
    on the task of finding relevant knowledge.
      When the extracting knowledge from a trustworthy source, even if a natural
      language source, it makes sense to
      focus on optimizing recall, i.e., obtaining as much information as possible.
      If the input source is a set of medical papers, or the main
      web page of an institution, there is a high chance that most of the information
      present in those documents is correct. Hence, an ontology extraction procedure
      that maximizes recall will obtain good results.

      However, when the input source is of a lesser quality, such as blogs or
      social media posts, there is a greater chance that some, or even most,
      of the information is fake or incorrect.
      If we consider also the so called phenomena of the \textit{post-truth},
      and acknowledge that some authors are deliberately sharing fake news
      or facts, the problem becomes much harder, and pressing.
      Even if deliberate lies were not an issue, most of the information shared
      in social media and similar sources is irrelevant in a long term.
      In this context, the problem of extracting a useful ontology from a large
      corpora of Internet sources becomes less a problem of recognizing the pieces
      of information lying in the corpus, and more a problem of filtering and selecting
      the relevant information, once extracted.

      Despite the existence of some general purpose systems,
      there is no proposal of an architecture for a computational framework
      that can simultaneously and continuosly learn from the most varied sources of information online.
      Another challenge in this aspect is to obtain a computationally convenient representation
      of this knowledge, independent of the domain, source and format of the input data.
      Furthermore, such system has to explicitly deal with the large amount of spurious,
      irrelevant, or deliberately fake information spread through web sources.

    \subsection{Quality Metrics}\label{sec:evaluation}

% 	Once a computational system is implemented following the guidelines of the framework
%     presented in this paper, it is necessary to perform some evaluation on its output.
%     However, being a complex system, with many interrelated components, evaluating it
%     is not as simple as comparing the actual output with the expected output.
    In this section we present a methodology for evaluating such a framework and obtaining interesting metrics that can validate its performance across the large range of tasks the framework is intended to enable.

    In a computational system or software framework, there are interesting software engineering metrics to evaluate. 
    Such a system should be highly modular and extensible, so that it can be easily adapted to new input formats, or new algorithms can be easily plugged in and integrated into the whole pipeline.
    The modular design of the framework can help in achieving a high degree of extensibility.

	  Besides these high level metrics, each of the tasks performed by the framework can be evaluated separately. 
    Most of these tasks have a definite performance metric that can be used to rate the degree of correctness of such task. 
    For many of the tasks described in the previous sections we can find standard performance metrics in the literature that can used to evaluate each particular process.

%     For example, a sensor for
%     extracting entities can be evaluated individually by running it on a tagged corpus
%     and computing precision and recall.

    An aggregate metric of these individual performances could provide a high level overview of the performance of the whole framework.
    However, designing an aggregate metric that provides a practical, interpretable, measure of the quality of the framework performance, can be very complex.
    Each of the different tasks performed by the framework can have a very different baseline performance. 
    A 90\% precision can be a very good result in some complex tasks, such as dependency parsing~\cite{AlbertiABCGKKMO17}, but mediocre in other tasks, such as image classification~\cite{Russakovsky2015}. 
    Moreover, this baseline number can vary not only across tasks, but in the same task, according to which test suite (or corpus)
    is used.

	Going up one level of abstraction, for the general problem of ontology learning there are also several evaluation metrics and methodologies available, such as OntoRand~\cite{ontorand} and OntoMetric~\cite{ontometric}. 
  However, most of these methodologies are designed for evaluating a single ontology that is either created or modified using techniques from ontology learning. 
  Once more, extending these methodologies to a collection of ontologies is not as straightforward as aggregating or averaging the individual results. 
  On the other hand, when dealing with a collection of ontologies, other concerns can arise, such as intra-ontology consistency, which are not usually considered when evaluating a single ontology. 
  As a final consideration, the framework by design, is expected to maintain a degree of internal inconsistency in order to better cover multiple and possibly overlapping domains.
  We present some of the most commonly described approaches in the literature for evaluating ontologies~\cite{petasis2011ontology} and describe how they can be used in our context.

    \paragraph{M1- Comparison with a gold standard.}
	  This approach consists in comparing a learned ontology with a baseline ontology for the same domain~\cite{corcoglioniti2016frame}. 
    The baseline ontology is assumed to be both correct and largely representative of the domain. 
    This method provides a great trade-off of speed versus accuracy, since both ontologies can be automatically compared in a number of metrics without human intervention, and the results have a high reliability because the baseline ontology is created by experts. 
    Some disadvantages do exist, for example, it is not always easy to find a good baseline ontology for a given domain, especially if the domain is not very well defined or is very novel. 
    On the other hand, even two ontologies extracted from the same domain by experts can have wide differences with respect to the structure, and particularly to the names the classes and relations are assigned. 
    This requires some form of normalization and mapping between both ontologies prior to comparison. 
    This metric is difficult to use, particularly if we're creating new characteristics.

    \paragraph{M2- Evaluación experta.}
    Un término medio alternativo al enfoque anterior es tener un experto en el dominio (o varios) para simplemente mirar la ontología resultante y evaluarla de acuerdo con algunas métricas predefinidas~\cite{ROSPOCHER2016132}.
    Este es posiblemente el método más confiable, en el sentido de que proporciona el mayor grado de validación que uno podría aspirar.
    Sin embargo, la clara desventaja radica en la cantidad limitada de información que un ser humano puede procesar en un tiempo razonable.
    Esta desventaja se agrava en el caso en que se crea una ontología a partir de un corpus de datos muy grande, como es el propósito de nuestro marco.
    En este caso, se podría analizar un pequeño subconjunto de los datos y extrapolar los resultados desde allí, pero esta idea agrega la complejidad de determinar un subconjunto que es lo suficientemente relevante pero que sigue siendo de un tamaño manejable.
    Esta métrica suele ser cara o difícil de usar.

    \paragraph{M3- Evaluación a través de una aplicación.}
    Un enfoque más práctico consiste en encontrar una aplicación interesante y evaluar si el uso de una ontología aprendida proporciona una mejora en esa aplicación~\cite{gurevych2003semantic}.
    Por ejemplo, usar una ontología aprendida sobre sentimientos humanos y frases relacionadas para mejorar el desempeño de un problema estándar de minería de opiniones.
    Si el uso del conocimiento representado en la ontología proporciona un impulso al rendimiento, medido por el enfoque estándar en la aplicación dada, obtenemos una validación confiable de que el proceso para aprender ontología, al menos, tiene un beneficio práctico medible.
    En cierto sentido, esta es una de las evaluaciones más valiosas para realizar, porque proporciona una línea base de comparación inmediata para un problema práctico.
    Los métodos anteriores que solo evalúan la ontología internamente no garantizan necesariamente que su contenido sea útil, incluso si es correcto según todas las métricas.
    Otra ventaja es que el proceso de evaluación se puede automatizar por completo y escalar para que coincida con la complejidad y el tamaño de la aplicación de destino.
    Como desventaja, validar un caso de uso no es necesariamente una métrica de la calidad general del conocimiento aprendido, y no está claro si esos resultados se replicarán en diferentes dominios y aplicaciones.
    Esta métrica de hecho parece más simple pero en muchos casos no es necesario mejorar una tarea, sino un conjunto amplio.

    \paragraph{M4- Evaluación basada en datos.}
    Finalmente, se puede realizar una evaluación basada en datos, comparando las entidades y relaciones en una ontología con un corpus de datos, no usados ​​durante la construcción de la ontología, pero representativos del mismo dominio~\cite{brank2005survey}.
    La ontología se puede evaluar contando el número de entidades superpuestas presentes en ella con las que se encuentran en el corpus.
    Se debe tener cuidado para permitir alguna variación en el corpus con respecto a la ontología, por ejemplo, usando alguna forma de expansión de consultas.
    Este enfoque se ha utilizado para comparar relativamente diferentes ontologías creadas por expertos con el mismo corpus y decidir qué ontología proporciona el mejor `` ajuste '' con el corpus~\cite{brewster2004data}.
    Sin embargo, obtener una métrica absoluta de ajuste entre una ontología y un corpus es más difícil, principalmente porque no se sabe de antemano cuál es el valor de ajuste que uno debería esperar lograr.
    Otro posible problema de este enfoque, en el caso particular de las ontologías que se han aprendido del texto, es introducir inadvertidamente un sesgo en la evaluación.
    Si los métodos utilizados para comparar la ontología y el corpus de texto están correlacionados con los utilizados para construir la ontología, entonces los resultados serán de dudosa validez.
    Por ejemplo, si se usa un algoritmo NER durante la construcción de la ontología, y se usa el mismo algoritmo en el corpus para reconocer entidades relevantes; o si se usa alguna métrica de co-ocurrencia para detectar relaciones en ambos casos.
    Esta métrica es compleja de definir y en muchas ocasiones no es representativa.

    \vspace{1em}

    Evaluar un solo método de aprendizaje de ontología es una tarea compleja, como lo demuestran los múltiples enfoques propuestos en la comunidad.
    Por lo tanto, es muy poco probable que podamos encontrar una única métrica automatizada para medir el rendimiento general de un marco como el propuesto.
    El mejor enfoque parece ser utilizar una combinación de los métodos existentes, adaptados a nuestro escenario, con la complejidad añadida de tratar con múltiples ontologías al mismo tiempo.
    En algunos casos, se puede encontrar un patrón oro y utilizarlo para obtener una comparación de referencia.
    En otros casos, siempre que se agregue una interfaz adecuada para consultar fácilmente el conocimiento, un experto en el dominio puede interactuar con el marco y dar una evaluación cualitativa para el dominio de interés.
    Desde un punto de vista pragmático, la evaluación más interesante y valiosa parece ser encontrar problemas prácticos relevantes que se puedan resolver o mejorar al utilizar nuestro marco.

    %The phenomenon of the post-truth era brings additional challenges. In this context, we believe
    %that for the problem of knowledge discovery the task of evaluating the relevance of the knowledge
    %extracted becomes more important than ever before.
----------
\subsection{Discusión de las tareas de aprendizaje}

En la comunidad de aprendizaje de ontología, se han desarrollado varios marcos que atacan problemas similares a los que presentamos.
Algunos de los enfoques encontrados en la literatura se concentran en una tarea en particular, es decir, creación, población o enriquecimiento de ontologías, entre otras.
Por ejemplo, marcos como KnowItAll~\cite{knowitall}, Artequakt~\cite{artequakt} y SOBA~\cite{soba} están orientados principalmente hacia la tarea de población de ontologías.
Otros, como ASIUM~\cite{asium}, VIKEF~\cite{vikef} y SYNDICATE~\cite{syndikate} están orientados principalmente al enriquecimiento de ontologías.
Sin embargo, muchas de las tareas o subproblemas que deben resolverse en cualquiera de estos dominios son muy similares y pueden reutilizarse.
Por lo tanto, han surgido marcos más generales como Text2Onto~\cite{cimiano2005text2onto} o BOEMIE~\cite{boemie} que tratan con una combinación de estas tareas.
Nuestro marco está diseñado para manejar las tareas comunes de aprendizaje de ontologías, incluida la creación, población, enriquecimiento, fusión y mapeo de ontologías.
Cada una de estas tareas se realiza en uno o más de los módulos principales del marco, no como características independientes o aisladas, sino como parte integral del proceso de aprendizaje del marco.

El módulo para procesar datos no estructurados se puede interpretar como una creación de ontología y una tubería de población, porque la entrada son datos no estructurados sin procesar y la salida es una ontología completamente nueva donde todo, desde clases y relaciones hasta instancias específicas, se crea automáticamente desde cero.
Por otro lado, una vez que estas ontologías recién creadas son entregadas al módulo para el descubrimiento del conocimiento, se realizan varios procesos que se pueden interpretar como formas de enriquecimiento de ontologías, ya que se pueden descubrir nuevas relaciones y se pueden definir automáticamente reglas de inferencia.
Finalmente, en el módulo de procesamiento de datos estructurados se realiza un proceso de fusión y mapeo de ontologías, para normalizar las ontologías entrantes frente al conocimiento ya almacenado.

\subsection{Cantidad y complejidad}

En cuanto a la cantidad y complejidad de conceptos reconocidos, las soluciones existentes se pueden dividir en aquellas que solo extraen entidades (ej., KnowItAll), aquellas que solo extraen relaciones (ej., ADAPTATIVA~\cite{adaptativa}, LEILA~\cite{leila}) y aquellos que intentan extraer ambos (por ejemplo, Artequakt, Web-> KB~\cite{webkb}, BOEMIE).
Nuestro enfoque corresponde a esta última categoría, con el agregado de que intenta extraer también conceptos más abstractos a través de técnicas como la agrupación jerárquica.
El descubrimiento de reglas de inferencia es otra tarea relevante que pretendemos integrar en el módulo de descubrimiento de conocimiento, como parte del proceso de generación de nuevo conocimiento.
Este proceso enriquece las ontologías ya construidas al agregar conocimientos de nivel superior, en forma de predicados lógicos o axiomas.
Estos, a su vez, pueden usarse más adelante para descubrir instancias o relaciones faltantes, o para detectar valores atípicos y errores.


\subsection{Uso del aprendizaje automático}

La mayoría de los sistemas emplean algún tipo de herramientas de aprendizaje automático para la mayoría de las tareas. En particular, muchos emplean herramientas de PNL para procesar texto natural y extraer conocimientos, y técnicas estadísticas para detectar agrupaciones.
Sin embargo, en general, la arquitectura de estos sistemas generalmente sigue una tubería diseñada muy estrictamente, donde los componentes se conectan cuidadosamente entre sí. Nuestra propuesta es diferente en el sentido de que pretendemos que nuestro sistema se pueda aprender completamente en el futuro.
Todos los componentes, algoritmos y partes pueden medirse y evaluarse automáticamente, si el conocimiento de cómo interactúan estas partes se describe en una ontología organizacional dentro del marco.

% NUEVO
La ontología organizacional, una vez definida, debe proporcionar una descripción de los módulos del marco, hasta un grado de detalle que permita realizar un razonamiento automatizado sobre qué componentes se pueden conectar.
Esta ontología podría usarse en un diseñador de canalización automático, es decir, un módulo que aprende a combinar componentes específicos (algoritmos, fuentes de datos, etc.) para una tarea de aprendizaje en particular.
Al autoevaluar su propio desempeño en cada tarea en particular (dada una combinación de métricas de evaluación, este módulo podría aprender, por ejemplo, si para una fuente de datos en particular es más conveniente aplicar extracción de entidades o eliminar palabras vacías, o qué algoritmo de agrupamiento funciona mejor .
Como ejemplo ilustrativo, varios canales de aprendizaje posibles son construidos a partir de componentes comunes.
Una métrica de evaluación relevante para cada una de estas canalizaciones podría permitir que el marco descubra automáticamente las mejores combinaciones.
Esto abre la puerta a la posibilidad de que, con el tiempo, el propio framework aprenda qué algoritmos y enfoques funcionan mejor en cada tarea o en cada dominio.

\subsection{Entrada humana}

La mayoría de los marcos y soluciones existentes requieren un cierto grado de interacción humana. En la mayoría de los casos, se espera que un experto en el dominio interactúe con una herramienta computacional para validar o refinar el resultado del proceso de aprendizaje.
Como se explicó anteriormente, nuestro enfoque propone ser completamente automatizado, en el sentido de que el conocimiento final almacenado nunca puede ser revisado por un humano. Por lo tanto, todo el marco está diseñado con el propósito de una automatización completa. Todas las decisiones que determinan qué fragmentos de la información extraída se almacenan se basan en parámetros computacionales que pueden ajustarse automáticamente.
Esto no niega algunos casos en los que un experto en el dominio podría interactuar con el marco, a través de alguna forma de interfaz de lenguaje natural, para acceder al conocimiento almacenado.
Sin embargo, esta posibilidad de interacción es un resultado del marco, no una necesidad.

\subsection{Generalización}

Con respecto a las restricciones de dominio, podemos clasificar las soluciones existentes en aquellas que son completamente independientes del dominio (por ejemplo, KnowItAll, LEILA, ISOLDE~\cite{isolde}) y aquellas que están adaptadas
a dominios particulares (por ejemplo, SOBA, Artequakt). Las soluciones independientes del dominio generalmente se diseñan de manera que no haya una dependencia particular ligada a un dominio, por lo tanto, son reutilizables en varios dominios. Sin embargo, esto significa que un sistema puede usarse en un dominio u otro, pero no significa necesariamente que el mismo sistema pueda aprender un poco de conocimiento de dos dominios diferentes \emph{simultáneamente}.

Si un sistema está simplemente diseñado para ser independiente del dominio y se utiliza para aprender de dos dominios muy diferentes, el resultado esperado es una especie de ontología combinada que representa ambos dominios con una aproximación de la unión de los conceptos (entidades y relaciones) en ellos. . Esta puede no ser la representación ideal, especialmente cuando se extiende a muchos dominios diferentes. Tratando de construir una ontología única que abarque todo el conocimiento que se puede extraer de
varios dominios diferentes (posibles en el orden de cientos o miles) pueden ser significativamente más difíciles que una simple suma o unión de cada uno de los dominios individuales.

Nuestro enfoque de la cobertura multidominio es diferente. Aunque el estudio de caso presentado solo involucra una ontología, nuestro marco está diseñado para aprender una nueva ontología cada vez que se descubre un nuevo dominio. Esta nueva ontología puede potencialmente fusionarse con ontologías existentes, pero también puede simplemente almacenarse como una nueva pieza de conocimiento utilizable. Con el tiempo, el sistema acumularía diferentes ontologías para diferentes dominios.
Cuando surge la necesidad de utilizar dicho conocimiento (por ejemplo, para realizar respuestas automáticas a preguntas), el marco decidiría qué ontologías son relevantes en una tarea en particular. (es decir, una consulta en particular).
Esto, a su vez, requiere un procesamiento más complejo, ya que podría ser necesario un enfoque de fusión de ontologías para calcular un resultado final para una tarea que involucra múltiples dominios. Sin embargo, creemos que esta mayor complejidad es un costo asequible en comparación con el poder expresivo obtenido al representar tanto conocimiento como sea posible sin las restricciones de tener una única taxonomía o un solo conjunto de reglas de inferencia.

Esta creencia se inspira en parte en cómo funciona el conocimiento experto en humanos. Es cierto que los humanos tenemos un conjunto básico de habilidades que podrían considerarse independientes del dominio, como nuestras habilidades innatas para la coincidencia de patrones. Estas habilidades básicas se utilizan en muchas de las tareas con las que nos enfrentamos a diario.
La analogía computacional es un sistema con un único algoritmo de aprendizaje de propósito general que podría realizar, por ejemplo, tareas tan diferentes como el reconocimiento de voz, la clasificación de imágenes y la traducción con el mismo proceso.
Este es el enfoque preferido por una parte de la comunidad de investigadores en aprendizaje automático ~ \ cite {kaiser2017one},
que esperan construir una inteligencia de propósito general a partir de un solo algoritmo de aprendizaje de propósito general y una única representación interna de propósito general.

Sin embargo, para tareas realmente complejas, argumentamos que los humanos usan representaciones especializadas, algoritmos de aprendizaje y técnicas de inferencia. Un experto humano en un dominio altamente complejo (como las matemáticas), no utiliza las mismas técnicas para la inferencia que las que se utilizan en tareas en tiempo real como el reconocimiento de objetos y de voz. Reconocemos que la inferencia en dominios muy complejos no se puede realizar con herramientas basadas en la intuición. La construcción de un sistema inteligente que pueda realizar inferencias en varios dominios altamente complejos simultáneamente requerirá el uso de representaciones y técnicas especializadas en cada dominio. Es hacia este principio que guiamos nuestras decisiones de diseño.

  \section{Discusión}
