\chapter{Estado de la Cuestión}\label{Chap:SOTA}
\markboth{\MakeUppercase{Estado de la Cuestión}}{}

Intro del estado del arte

- el descubrimiento de conocimiento en lenguaje natural

Humans consume the ambient information and transform
it into independent concepts, that can be related to each other. In general,
we only store a small part of all the information we process every day.
This information consists of the most important concepts and relations,
that can be useful in future situations.
Thus, we learn basically by separating what could be
considered noise from the information that we think will be useful in the future.
In general we said two types of memories: one for recent knowledge and 
other where we stored the knowledge for the long time.

The design of complex machine learning algorithms and techniques necessarily
to mimic human behaviors have been there since the beginning of the artificial intelligent.
This work presents and initial approach to the problem translate the 
humans experience in the reading process whit the objective to
execute the learning, to the machine. 
For this we are facing the challenge to process text for detect elements
and interactions between ours. Moreover, it is  necessary research the most better 
proposal of artificial intelligent to transform the information in knowledge
and them represent this knowledge whit semantics.    
In particular we analyze the process of
creating a representation of knowledge (in the form of an ontology)
obtained. 

Nowadays, one of the biggest information sources in Internet
is text in natural language, which appears in news, opinions, messages, encyclopedias, 
and many other forms of digital communication.
This is not accidental, since words~(computationally represented as text) 
are one of the most important communication channels throughout the human history.
One of the most basic concepts recognizable in natural language that provide
useful information are actions. These are generally associated with two
distinctive concepts, the subject and the target. The first is who performs
the action, and the later is who receives the consequences of said action.
Correctly identifying actions, and their corresponding subjects and targets
is one of the most basic and fundamental challenges in order to understand
the information.

--------------------------

The accelerated growth of Internet has produced a surplus of information (news, email,
social media, blogs) that greatly exceeds our capacity for processing and consuming data.
Thus, building automatic systems that can extract knowledge from this flow of information has
become of the most active research fields in Computer Science. This phenomena has been
dubbed Big Data~\cite{bigdata}, and its study has attracted the attention of different 
research communities 
as business intelligence~\cite{chen2012business}, engineering including physical and 
biological~\cite{wu2014data} and social media~\cite{shah2015big}.

In recent years, researchers in fields such as machine learning, knowledge discovery, data mining and
natural language processing, among others, have produced many approaches and techniques to
leverage the large amount of information in the Internet for a variety of tasks, from
building search~\cite{google} and recommender systems~\cite{youtube} 
to improving medical diagnostics~\cite{watson}.

Among the different approaches relevant to knowledge discovery, we can recognize a
continuous spectrum of techniques, based on how much expert knowledge is used. 
Heavily knowledge-based techniques are based 
on rules defined in knowledge bases handcrafted by domain experts~\cite{chandrasekaran1986generic}. 
These approaches have a great degree
of reliability and precision, and generally allow for more complexity in the extracted knowledge,
but are difficult to scale to large amounts of data.
In contrast, the statistical approaches consist of techniques based on pattern recognition with statistical
and probabilistic models~\cite{kevin2012machine}. These techniques scale better with large amounts of data~\cite{le2013building}, 
providing better recall, but often are limited to extracting simple models of knowledge,
and can be more sensitive to noisy, fake or biased information~\cite{bolukbasi2016man}.

Given these mutually complementary characteristics, several hybrid approaches have been proposed.
Recently, research areas such as ontology learning~\cite{cimiano2009ontology},
learning by reading~\cite{barker2007learning} or entity embedding~\cite{hu2015entity} have arisen.
In these areas, researchers combine techniques from machine learning, natural language
processing and knowledge representation to solve more complex problems that cannot
be dealt with using only the classical tools.

Many machine learning systems are designed to solve a domain-specific task, such as
assigning a class to an element from a predefined set of labels. These systems,
when trained with data for a particular domain, are often not applicable to other domains
or to scenarios where several different domains must be used together. Moreover,
often systems are designed to be trained once from a corpus, and don't allow for
a continuous improvement of the knowledge learned.
Recently there are attempts to build general-purpose learning systems that are always
improving while obtaining new knowledge, re-evaluating the old knowledge and refining their
own confidence~\cite{mitchell2015never}.

Additionally, it is interesting to design non-monolithic learning systems, but instead
built as a set of modular components that can be combined in different ways.
This composability would allow a continuous learning system not only to improve the
quality of the extracted knowledge, but also to learn how to tune its own internal
parameters to perform a better knowledge extraction in the future. It is conceivable
that such a system could gradually learn which types of basic processes (i.e., entity recognition, POS-tagging, etc.)
are most useful for a given domain or for a particular corpus. Likewise, such a system could
learn which types of probabilistic models provide the best results in a particular dataset.


- importancia de la semántica

- explicar el proceso y las tareas
  - definir un esquema de anotacion, modelo semántico, que sea bueno
  - herramientas con las que hacer la anotacion
  - anotación asistida y utilidades para anotar mejor
  - anotar un corpus con metricas de calidad y merging, hablar de la metodologia de anotacion
  - entrenar sistemas de machine learning para la extraccion automática
  - diseñar entornos de evaluación (challenges) para comparar sistemas
  - construir ontologías

  \section{Modelos Semánticos de Anotación de Lenguaje Natural}

  \section{Herramientas de Anotación}

  \section{Complementos para la Anotación Semi-Automática}

Machine learning, and specifically supervised learning, is one of the most effective tools for automating
complex cognitive tasks, such as recognizing objects in images or understanding natural language text.
One of the main bottlenecks of supervised learning is the need for high-quality datasets of labeled samples
on which statistical models can be trained.
These datasets are usually built by human experts in a lengthy and costly manual process.
Active learning~\cite{Cohn2010ActiveL} is an alternative paradigm to conventional supervised learning that has been proposed to reduce the costs involved in manual annotation .

The key idea underlying active learning is that a learning algorithm can perform
better with less training examples if it is allowed to actively select which examples to learn from~\cite{survey}.
In the supervised learning context, this paradigm changes the role of the human expert.  In conventional
supervised learning contexts, the human expert guides the learning process by providing a large dataset
of labeled examples. However, in active learning the active role is shifted to the algorithm and
the human expert becomes an oracle, participating in a labeling-training-query loop.
In the active paradigm,  a model is incrementally built by training on a partial collection of samples and then selecting
one or more unlabeled samples to query the human oracle for labels and increase the training set.
This approach introduces the new problem of how to best select the query samples so as to maximize the model's performance while minimizing the effort of the human participant.

The simplest active learning scenario consists of  the classification of independent elements $x_i$ drawn from
a pool of unlabeled samples. Examples range from image classification~\cite{Gal2017DeepBA} to sentiment mining~\cite{Kranjc2015ActiveLF},  in which the minimal level of sampling (e.g., an image or text document) corresponds to the minimal level
of decision. i.e, a single label is assigned to each $x_i$. More complex scenarios arise when the decision level is more fine-grained than the sampling level. In the domain of text mining, an interesting scenario is the task of
entity and relation extraction from natural language text~\cite{zhang2012unified}. In this scenario the
sampling level is a sentence, but the minimal level of decision involves each token or pair of tokens in the sentence, and furthermore, these decisions are in general not independent within the same sentence.
In this case, it is not trivial to estimate how informative an unlabeled sample will be, since each sample has several sources of uncertainty.

This section reviews some of the most relevant research related with active learning in general,
and specifically focused on entity detection and relation extraction.
One of the most important design decisions in active learning is how to intelligently select the novel
unlabeled samples in the most efficient way. The underlying assumption is that we want to train a
model to the highest possible performance~(measured in precision, $F_1$, etc.) while minimizing the human cost
(measured in time, number of samples manually labeled, or any other suitable metric).
This requirement is often framed as the selection of the \textit{most informative} unlabeled samples, and
formalized in terms of a query strategy~\cite{survey}.
The most common query strategies for general-purpose active learning can be grouped into the following categories:

\begin{description}
\item[(i) Uncertainty sampling:] The most informative samples are considered those with the highest degree of uncertainty, given some measure of uncertainty for each sample~\cite{Lewis1994148}.

\item[(ii) Query by committee:] The most informative samples are considered those with the highest disagreement among a committee of either different models or different hypotheses from the same underlying model~\cite{seungquery}.

\item[(iii) Expected model change:] The most informative samples are considered those that produce the highest change in the model's hypothesis if they were included in the training set~\cite{NIPS2007_3252}.

\item[(iv) Variance and error reduction:] The most informative samples are those which produce the highest reduction in the model's generalization error or, as a proxy, its variance~\cite{roy2001toward}.
\end{description}

Expected model change (iii) and variance/error reduction (iv) strategies are heavily dependent on the specific learning model used.
In contrast, uncertainty sampling (i) and query by committee (ii) are  applicable in general with a high degree of model agnosticism.
Furthermore, relevant subsets of both strategies can be formalized under a single framework if we define the uncertainty as a measure of the entropy of the model's predicted output.
In this framework, query-by-committee can be implemented via weighted voting, thereby assigning empirical probabilities to the possible outputs.

Weighted density is a complimentary strategy in which the most informative samples are weighted by how representative they are of the input space, for example, by measuring their similarity to the remaining samples~\cite{settles2008analysis}.
This approach attempts to counter-balance a noticeable tendency to select outliers as the most informative samples ---a problem associated with other query strategies--- since outliers are often the samples that create the highest amount of uncertainty, disagreement or hypothesis change.

Recent advseplnances in natural language processing have produced an increased interest in active learning to alleviate the requirement for large annotated corpora~\cite{Olsson2009ALS, Tchoua2019ActiveLY}.
\citet{settles2008analysis} compare several strategies for active learning in sequence labeling scenarios, concluding that query strategies based on measures of sequence entropy combined with weighted sampling outperform other variants.
\citet{Meduri2020ACB} propose a comprehensive benchmark to evaluate different active learning strategies for entity matching.
In the task of named entity recognition, CRF models have been used to select query samples
\citep{Claveau2017StrategiesTS, Lin2019AlpacaTagAA}.
The task of relation extraction also benefits from active learning approaches, both in general-purpose settings~\cite{fu2013efficient} and in domain-specific settings~\cite{zhang2012unified}.
However, despite the growing body of research, it is still a challenge to apply active learning in joint entity recognition and relation extraction, especially in scenarios with low resources~\cite{Gao2019ActiveER}.

  \section{Recursos Lingüísticos}

  \section{Sistemas de Aprendizaje Automático para el Descubrimiento de Conocimiento}

  \section{Entornos de Evaluación Competitivos}

  \section{Representación del Conocimiento}

  There are many resources used in the task of
natural language processing. Among the most common ones, ontologies, as representations
of concepts, data types and their relations in an explicit computational structure.
Some of the most popular ontologies used in NLP are WordNet~\cite{} and DBpedia~\cite{}.
For their ubiquity and usefulness, research in ontologies has increased in the last years,
particularly regarding the automatic creation of these structures,
giving birth to the field of Ontology Learning.
Ontology Learning has the potential of reducing the cost of creating
and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.

Ontology Learning deals with the problem of automatically creating an
ontology from several resources. Examples include mining analysis, mostly
the study of text in natural language~/cite{}. In this
particular task, several approaches have been presented, from syntactic and
semantic analysis of corpora~\cite{} to statistical and machine learning based
approaches~\cite{}. 

Several tools and systems have been proposed for this task. 
Text2Onto~\cite{cimiano2005text2onto} is a framework for
data-driven change discovery which employs a probabilistic 
ontology model (POM).
OntoLT~\cite{buitelaar2004ontolt} extracts concepts and
relations automatically from linguistically annotated text collections,
by means of rule set, which maps linguistic classes to ontology classes.
A newer approach is OntoGain~\cite{drymonas2010unsupervised}, 
which uses an unsupervised approach,
and exploits the inherent multi-word term's lexical information
in order to extract higher level concepts.

One of the issues with these approaches is the amount of spurious
information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
In general, there will be many unimportant
or redundant pieces of information in the analyzed corpora. A naive
approach that doesn't take this issue into consideration will create
immense ontologies with very little useful information. To tackle
this problem, OntoGain proposes a hierarchical clustering scheme
that attempts to identify general concepts and relations.

In general these resources are focused in the extraction of knowledge and
pay much less attention to the task of find relevant knowledge. 
An ontology is interesting not just because of its size, but 
because of how is such information selected, processed and stored.
In this problem we focus our work, attempting not only 
to recognize information, but also to reduce this information and obtain
information with more relevance.

-----------

The problem of discovering, storing and using knowledge in a computationally effective
form has been widely studied~\cite{mitchell2015never, ROSPOCHER2016132, cimiano2009ontology}. 
This problem has been treated from two different but complementary research areas:
the fields of knowledge representation and machine learning.
The knowledge representation community provides means for
computationally representing and operating with stored knowledge in forms that
can ensure some degree of logical consistency. 
Conversely, the machine learning community provides
tools for obtaining useful knowledge from large collections of structured and unstructured data.
Recenlty, a new discipline named ontology learning has arisen, which draws ideas
and techniques from both the knowledge representation and the machine learning fields.
This field deals with the problem of automatically building ontological representations
of knowledge from a variety of data sources. As such, the theory of ontology learning
is relevant in the design of the framework presented in this paper.
In this section we present a brief review of the relevant concepts and technologies
in each of these three domains, as well as some approaches similar to our proposal.

	\subsection{Knowledge Representation and Reasoning}

    Since the dawn of computer science, one of the problems that has attracted wide attention
    is that of representing knowledge in a computational format, such that automatic reasoning
    can be performed to discover new, previously unknown truths~\cite{sowa2000knowledge}.
    Arguably, the most popular knowledge representation technology in use are
    ontologies~\cite{guarino1995formal}, which have
    become the \emph{de facto} standard.
    %Definición
	Ontologies can been defined as a formal specification of a conceptualization~\cite{asuncion2003}.
    This represents concepts, relations between these concepts, instances of these concepts and inference rules
    for deriving new relations. 

    As such, ontologies can be considered as a combination of two predominant approaches 
    for knowledge representation: those based on formal logic~\cite{brachman1992knowledge}
    and those based on graphs of semantic relations~\cite{chein2008graph}.
    In logic-based approaches the facts are represented as logic predicates or functions 
    and reasoning is enabled through the application of formal inference rules.
    In contrast, graph-based representations express facts as nodes~(objects)
    and edges~(relations) and reasoning is built on top of graph traversing methods.
    However, in ontologies objects and their attributes and relations are represented in a graph of concepts, 
    which can also be interpreted as a set of predicates and functions on these objects. 
    On top of this layer, inference rules can be added,
    which enable logical reasoning methods to be used for deriving new attributes and 
    relations between existing concepts.
    
	Relations in an ontology can be of a specific domain, but often some general domain 
    relations are represented, such
    as \textit{is-a} and \textit{part-of}. These types of relations allow representing more abstract
    or complex concepts out of the composition of more concrete or simple concepts.
    Hence many ontologies contain some kind of taxonomy of increasingly abstract concepts, 
    that are also interconnected
    with each other using other semantical relations which can be domain specific.
    % Ventajas
    These resources allow representing complex frameworks of knowledge, down to a degree of specificity which
    enables the design of fully automated reasoning tools which use this knowledge for 
    a variety of computational tasks.
    % Dificultades o desventajas
  	Due to the high complexity of the concepts and relations that are represented,
    and the experience needed to recognize the most relevant concepts of a domain,
    ontologies are usually manually constructed by domain experts~\cite{wong2012ontology}.
    Thus, building an ontology is a process that requires a long time and a large number of experts to
    define and populate it with relevant instances that refer to objects and relations.
    This makes really difficult build hand crafted ontologies and ensure their maintainability, due to day a day huge quantity of new and valuable information, desirable to convert into knowledge, appear in the World Wide Web.
    Another important outcome of this process is that experts usually represent only facts that are absolutely
    true in the domain. Although existing ontology formats can be extended to deal with fuzzy~\cite{fuzzyontology} or
    vague data~\cite{bobillo2011fuzzy}, manually assigning a degree of belief to a specific fact is a complex task.

%     % Ejemplos
%     It is possible to distinguish between two types of ontologies: general domain (or upper ontologies) and domain-specific (or simply domain ontologies).
%     Domain-specific ontologies are those which deal with the concepts and relations of the particular knowledge domain.
%     As examples, we can cite ontologies in the medical sciences~\cite{rector2003opengalen,gene2004gene},
%     or the software engineering field~\cite{4641930}.
%     Other ontologies are more general since they can be used in different domains, or they are used for general
%     purpose tasks which are employed in many areas.
%     WordNet~\cite{miller1995wordnet} is a general purpose ontology that contains most of the words of the English language,
%     and syntactic and semantic relations between them.
%     It is used in many tasks in natural language processing and text mining.
%     DBPedia~\cite{mendes2012dbpedia} is an encyclopedic ontology that contains part of the knowledge present in the
%     Wikipedia\footnote{http://www.wikipedia.org}.
%     It relates people, historical events, facts, locations, and other concepts, in a structured and queryable format.
%     Since ontologies have a unified form of representing a single fact, concept or relation using Uniform Resource Locator~(URL), it is
%     possible and very common for different ontologies to link each other. For example, many domain specific ontologies
%     have entities which are linked to the corresponding entry in DBPedia. The approach of linking and referencing to other
%   	widely known ontologies, known as \textit{linked data}~\cite{bizer2009linked},
%     enables the standarization of the representation of shared knowledge and eases the tasks of querying and analyzing it.

    Ontologies are an effective tool for representing knowledge in a wide
    variety of domains and scenarios~\cite{staab2010handbook}.
    They are flexibly enough to adapt to a particular domain and powerful enough to represent complex concepts.
    However, one of the most complex task in this sense is maintaining
    an ontology up-to-date with respect to the massive amount of unstructured data that is generated and published every day.
	Therefore, the need arises for computational tools to build ontologies with automated or semi-automated processes.

	\subsection{Machine Learning}

%	The speed and volume at which information is produced has increased exponentially in the last decade,
%    mainly due to the rise of the social networks and the mobile technology. In order to cope with this volume of information,
%    it is necessary to be able to process massive amounts of data continuously.
    The field of machine learning provides tools for the automatic extraction of information and knowledge from different
    sources of data. 
    Machine learning not only allows to automate processes and tasks of knowledge discovery or text mining, but also provides a large
    improvement in the scalability of these processes~\cite{wu2014data}. By using mass computing resources, it is possible to process millions
    of raw documents in a reasonable time, far exceeding what can be done by domain experts.
    Recent improvements in computing capabilities and access to larger datasets have given rise
    to the field of deep learning, which has improved the state of the art in
    several of the classic machine learning tasks~\cite{lecun2015deep}.

	Arguably, the two most common approaches in machine learning are supervised and unsupervised learning~\cite{kevin2012machine}.
    Supervised learning can be used for recognizing specific elements of knowledge in a source of data. For example, tagging
    pieces of text to indicate that they define an entity~\cite{nadeau2007survey} (e.g., a person, organization, or place), recognizing relations between
    said entities, or assigning a sentiment or opinion score~\cite{liu2012sentiment} to a fragment of text. On the other hand, unsupervised learning
    can help with finding relevant structure in a large set of elements. Clustering algorithms can be used to detect similar
    concepts or to extract abstract concepts from groups of more concrete elements. Other techniques can be used for reducing
    the amount of information, for example, to remove noisy, uncertain or irrelevant pieces of
    information~\cite{bingham2001random}.

    In general, most machine learning algorithms are not designed to represent the learned knowledge in complex structures,
    such as those defined by human domain experts (i.e., ontologies). In turn, the representations often have a simple structure, such
 	as a probability distribution or a correlation matrix~\cite{bengio2013representation}.
    When applying these algorithms to a real problem, an domain-specific
    interpretation of those representations has to be made. 
    % Esta es una debilidad imporante, la no explicación
    Furthermore, many of the most powerful machine learning models are difficult to explain, in the sense
    that when the system produces an answer, a human expert cannot easily understand and reproduce the inference steps
    that the system performs~\cite{olden2002illuminating}.
    % creo mejor decir que si la representacion es tan importante las ontologias son una forma de representar conocimiento
    % y entonces meter el gancho de embeddings y entity embeddings
    Choosing an appropriate representation is decisive for the success of most machine learning techniques~\cite{bengio2012deep}.
    In recent years, there has been an increased interest in the problem of automatically learning relevant representations.
    Word embeddings~\cite{mikolov} and more general entity embeddings~\cite{hu2015entity} represent the first steps towards powering
    deep learning approaches with more explainable internal representations.
    Since ontologies are, by definition, representations of a given conceptualization, it is conceivable that using ontologies
    as seeds for the representation of a given domain, the performance of data mining processes based on machine learning can
    be improved.
    \subsection{Ontology Learning}

    In the intersection of machine learning and knowledge representation,
      the field of ontology learning has arisen to deal with the 
      complexity of manually maintaining and updating ontologies.
      This field draws techniques and tools from both communities, 
      to automate part of the process of creating and maintaining ontologies.
    Ontology learning has the potential of reducing the cost of creating
    and, most importantly, maintaining large and complex ontologies~\cite{cimiano2009ontology}.
      This problem is also addressed in 
      learning by reading~\cite{barker2007learning}, a field which draws techniques
    the natural language processing and knowledge representation and reasoning communities.
    The purpose is to build a formal representation of some particular field given unrestricted
    textual data related to the field. This representation must also allow fully automatic
    reasoning.    
      Learning by reading can be considered as a particular case of ontology learning,
      even though it is only concerned with textual input, and the
      output is not necessarily formated as an ontology.
      
      In the field of ontology learning, two general high level tasks can be distinguished:
      ontology population and ontology enrichment~\cite{petasis2011ontology}.
      Ontology population deals with the
      sub-problem of finding new instances for an already defined ontology, while
      ontology enrichment deals with adding new concepts and relations to an existing
      ontology. There is an overlap between these tasks, and most of the existing
      approaches cannot be classified purely in these terms.  
      In this field, several tools have been proposed, which
      combine different approaches and solve different subsets of the ontology
      learning tasks. A brief review of these systems can help defining the 
      main characteristics that our framework should have.
      % No sé si decir algo como a continuación se presentan varias de ellas 
      % atendiendo a sus características mas relevantes. Esto nos permite conocer 
      %la diversidad de enfoques y las potensialidades que debemos tener en cuenta
      %para crear nuestro sistema
  
      % general (early approach)
      Early approaches, such as SYNDIKATE~\cite{syndikate}, deal only with populating a 
      knowledge base, with a predefined ontological structure~(classes and relations).
      % web approaches
      Since the Web is a rich source of information, several approaches have focused on extracting knowledge
      from it, exploiting the semi-structured format of web resources.
      Some systems like ARTEQUAKT~\cite{artequakt} and SOBA~\cite{soba} are domain-specific,
      respectively focusing on the art and the sports domains.
      Other systems, like WEB-$>$KB~\cite{webkb} attempt to build general domain knowledge bases from the 
      web, exploiting also the structure of links between pages to identify relations.
      % from structured data
      Another example is the VIKEF~\cite{vikef} system, which uses product catalogs as sources of data, hence exploiting the
      inherent structure present in this type of data.
      % bootstraping with human knowledge
      Even though most systems attempt fully-automatic extraction, some examples like ADAPTATIVA~\cite{adaptativa} include
      a bootstrapping strategy, where human experts provide feedback about the extracted knowledge.
  
      In order to extract relevant knowledge from unrestricted text, NLP techniques have
      been introduced in systems such as OPTIMA~\cite{optima} and ISODLE~\cite{isolde}.
      % rule-based
      The use of natural language features can be used build rule-based systems,
      like the OntoLT~\cite{buitelaar2004ontolt} proposal, that extract concepts and
    relations via a mapping of linguistic classes to ontology classes.
      % based on statistical models
      An alternative approach is to use statistical or probabilistic models,
      exemplified by systems such LEILA~\cite{leila} or Text2Onto~\cite{cimiano2005text2onto}.    
    % relevance
      Another example is KnowItAll~\cite{knowitall}, which introduces a point-wise mutual information
      metric to select relevant instances.
    
    Once instances of entities are relations are extracted from text, a natural question is
      wether more abstract knowledge can be inferred from these examples.
      The systems who address this issue often use unsupervised techniques to attempt
      to discover inherent structures. Two relevant examples of this approach
      are OntoGain~\cite{drymonas2010unsupervised} and ASIUM~\cite{asium}, 
      which attempt to automatically build a hierarchy of concepts using clustering techniques.
      % multimedia
      The BOEMIE~\cite{boemie} system is another interesting example, since it 
      attempts to automatically infer abstract concepts from the concrete instances found,
      but focuses not only on text, but also on multimedia sources such as images, and videos.
    % never-ending learning
      Most of the mentioned systems usually focus on one iteration of the extraction process.
      However, more recent approaches, like NELL~\cite{mitchell2015never}, attempt
      to learn continuosly from a stream of web data, and increase over time
      both the amount and the quality of the knowledge discovered.
      
    One of the issues with many of these approaches is the amount of spurious
    information they generate~\cite{Maimon:2015:OLT:2870689.2870690}.
    In general, there will be many unimportant
    or redundant pieces of information in the analyzed corpora. A naive
    approach that doesn't take this issue into consideration will create
    immense ontologies with very little useful information. To tackle
    this problem, OntoGain proposes a hierarchical clustering scheme
    that attempts to identify general concepts and relations.
  
    In general, these tools are focused on the extraction of knowledge and
    on the task of finding relevant knowledge.
      When the extracting knowledge from a trustworthy source, even if a natural
      language source, it makes sense to
      focus on optimizing recall, i.e., obtaining as much information as possible.
      If the input source is a set of medical papers, or the main
      web page of an institution, there is a high chance that most of the information
      present in those documents is correct. Hence, an ontology extraction procedure
      that maximizes recall will obtain good results.
      
      However, when the input source is of a lesser quality, such as blogs or
      social media posts, there is a greater chance that some, or even most,
      of the information is fake or incorrect.
      If we consider also the so called phenomena of the \textit{post-truth},
      and acknowledge that some authors are deliberately sharing fake news
      or facts, the problem becomes much harder, and pressing.
      Even if deliberate lies were not an issue, most of the information shared
      in social media and similar sources is irrelevant in a long term.
      In this context, the problem of extracting a useful ontology from a large
      corpora of Internet sources becomes less a problem of recognizing the pieces
      of information lying in the corpus, and more a problem of filtering and selecting
      the relevant information, once extracted.
  
      Despite the existence of some general purpose systems, 
      there is no proposal of an architecture for a computational framework
      that can simultaneously and continuosly learn from the most varied sources of information online.
      Another challenge in this aspect is to obtain a computationally convenient representation
      of this knowledge, independent of the domain, source and format of the input data.
      Furthermore, such system has to explicitly deal with the large amount of spurious,
      irrelevant, or deliberately fake information spread through web sources.
  
    \subsection{Quality Metrics}\label{sec:evaluation}

% 	Once a computational system is implemented following the guidelines of the framework
%     presented in this paper, it is necessary to perform some evaluation on its output.
%     However, being a complex system, with many interrelated components, evaluating it
%     is not as simple as comparing the actual output with the expected output.
    In this section we present a methodology for evaluating such a framework and obtaining
    interesting metrics that can validate its performance across the large range of
    tasks the framework is intended to enable.

    In a computational system or software framework, there are interesting
    software engineering metrics to evaluate. Such a system should be highly modular and
    extensible, so that it can be easily adapted to new input formats, or new algorithms
    can be easily plugged in and integrated into the whole pipeline. 
    The modular design of the framework can help in achieving a high
    degree of extensibility. 

	Besides these high level metrics, each of the tasks performed by the framework can be
    evaluated separately. Most of these tasks have a definite performance metric that can
    be used to rate the degree of correctness of such task. For many of the tasks described in the previous
    sections we can find standard performance metrics in the literature that can used to
    evaluate each particular process.
    
%     For example, a sensor for
%     extracting entities can be evaluated individually by running it on a tagged corpus
%     and computing precision and recall. 

    An aggregate metric of these individual performances could provide a high level
    overview of the performance of the whole framework.
    However, designing an aggregate metric that provides a practical, interpretable,
    measure of the quality of the framework performance, can be very complex.
    Each of the different tasks performed by the framework can have a very different
    baseline performance. A 90\% precision can be a very good result
    in some complex tasks, such as dependency parsing~\cite{AlbertiABCGKKMO17}, but mediocre in other tasks, such
    as image classification~\cite{Russakovsky2015}. Moreover, this baseline number can
    vary not only across tasks, but in the same task, according to which test suite (or corpus)
    is used.

    \begin{figure}[htb]
    	\begin{center}
        	\includegraphics[width=0.6\columnwidth]{Graphics/evaluation.pdf}
            \caption{Schematic representation of the process for evaluate knowledge.}
            \label{fig:evaluation}
        \end{center}
    \end{figure}
    
	Going up one level of abstraction, for the general problem of ontology learning there are also
    several evaluation metrics and methodologies available, such as OntoRand~\cite{ontorand} 
    and OntoMetric~\cite{ontometric}. However, most of these
    methodologies are designed for evaluating a single ontology that is either created or
    modified using techniques from ontology learning. Once more, extending these methodologies
    to a collection of ontologies is not as straightforward as aggregating or averaging
    the individual results. On the other hand, when dealing with a collection of ontologies,
    other concerns can arise, such as intra-ontology consistency, which are not usually
    considered when evaluating a single ontology. As a final consideration, the framework
    by design, is expected to maintain a degree of internal inconsistency in order
    to better cover multiple and possibly overlapping domains.
    We present some of the most commonly described approaches in the literature for 
    evaluating ontologies~\cite{petasis2011ontology}
    and describe how they can be used in our context.

    \paragraph{M1- Comparison with a gold standard.}
	This approach consists in comparing a learned ontology with a baseline ontology
    for the same domain~\cite{corcoglioniti2016frame}. The baseline ontology is
    assumed to be both correct and largely representative of the domain. This method
    provides a great trade-off of speed versus accuracy, since both ontologies
    can be automatically compared in a number of metrics without human intervention,
    and the results have a high reliability because the baseline ontology is created
    by experts. Some disadvantages do exist, for example, it is not always easy to
    find a good baseline ontology for a given domain, especially if the domain is
    not very well defined or is very novel. On the other hand, even two ontologies
    extracted from the same domain by experts can have wide differences with respect to
    the structure, and particularly to the names the classes and relations are
    assigned. This requires some form of normalization and mapping between both ontologies
    prior to comparison. This metric is difficult to use, particularly if
    we're creating new characteristics.

    \paragraph{M2- Expert evaluation.}
    An alternative middle ground to the previous approach is having a domain
    expert (or several) to simply look at the resulting ontology and evaluate
    it according to some predefined metrics~\cite{ROSPOCHER2016132}.
    This is arguably the most reliable method, in the sense that it provides
    the highest degree of validation one could aspire. However, the clear
    disadvantage lies in the limited amount of information that a human can
    process in a reasonable time. This disadvantage is worsened in the case
    where an ontology is created from a very large corpus of data, as is
    the purpose of our framework. In this case, a small subset of the data
    could be analyzed, and results extrapolated from there, but this idea
    adds the complexity of determining a subset that is relevant enough but
    remains of a manageable size.
    This metric is usually expensive or difficult to use.

    \paragraph{M3- Evaluation through an application.}
    A more practical approach consists in finding an interesting application and
    evaluating if the use of a learned ontology provides an improvement in that
    application~\cite{gurevych2003semantic}. For example, using a learned ontology
    about human feelings and related phrases to improve the performance of a
    standard opinion mining problem. If using the knowledge represented in the
    ontology provides a boost to performance, as measured by the standard
    approach in the given application, we obtain a reliable validation that
    the process for learning ontology, at least, has a measurable practical benefit.
    In some sense, this is one of the most valuable evaluations to perform,
    because it provides an immediate comparison baseline for a practical problem.
    The previous methods which only evaluate the ontology internally do not
    necessarily guarantee that its content will be useful, even if it's correct by
    all metrics. Another upside is that the evaluation process can be completely
    automated, and scaled to match the complexity and size of the target application.
    As a downside, validating a use case is not necessarily a metric for the overall
    quality of the learned knowledge, and it's not clear if those results will
    replicate in different domains and applications.
    This metric indeed seem more simple but in many cases not need
    improvement a task, but a wide set.

    \paragraph{M4- Data driven evaluation.}
    Finally, a data driven evaluation can be performed, by comparing the entities
    and relations in an ontology to a corpus of data, not used during the
    construction of the ontology, but representative of the same
    domain~\cite{brank2005survey}. The ontology can be evaluated by counting
    the number of overlapping entities present in it against those found in the corpus.
    Care must be taken to allow for some variation in the corpus with respect to
    the ontology, for example, using some form of query expansion.
    This approach has been used to relatively compare different expert-made
    ontologies against the same corpus and deciding which ontology provides
    the best ``fit'' with the corpus~\cite{brewster2004data}.
    However, obtaining an absolute metric of fit between an ontology and a
    corpus is harder, mainly because it is not known beforehand what is the
    value of fit one should expect to achieve.
    Another possible issue of this approach, in the particular case of
    ontologies that have been learned from the text, is to inadvertently
    introduce a bias in the evaluation. If the methods used to compare the
    ontology and the text corpus are correlated with those used to build the ontology,
    then the results will be of a dubious validity.
    For example, if one uses a NER algorithm during the building of the ontology,
    and the same algorithm is used in the corpus to recognize relevant entities;
    or if some co-occurrence metric is used for detecting relations in both cases.
    This metric is complex to define and on many occasions is unrepresentative.

    \vspace{1em}

    Evaluating a single ontology learning method is a complex task, as shown by the multiple approaches
    proposed in the community. Hence, it's is very unlikely we can find a single automated metric
    to measure the overall
    performance of a framework as the one proposed. The best approach seems to be using a combination of the
    existing methods, adapted to our scenario, with the added complexity of dealing with multiple ontologies
    at the same time. In some cases a gold standard can be found and used to obtain a baseline comparison. In
    other cases, provided a suitable interface for easily querying the knowledge is added, a domain expert can
    interact with the framework and give a qualitative evaluation for the domain of interest.
    From a pragmatical point of the view, the most interesting and valuable evaluation seems to be finding
    relevant practical problems that can be solved, or improved, when using our framework.

    %The phenomenon of the post-truth era brings additional challenges. In this context, we believe
    %that for the problem of knowledge discovery the task of evaluating the relevance of the knowledge
    %extracted becomes more important than ever before.
----------
    \subsection{Learning tasks disscusion}

In the ontology learning community, several frameworks have been developed which attack problems
similar to the ones we present. Some of the approaches found in the literature concentrate on one
particular task, i.e., ontology creation, population, or enrichment, among others. For example,
frameworks like KnowItAll~\cite{knowitall}, Artequakt~\cite{artequakt} and SOBA~\cite{soba} are oriented mainly towards the task of ontology
population. Others, such as ASIUM~\cite{asium}, VIKEF~\cite{vikef} and SYNDICATE~\cite{syndikate} are oriented mainly towards ontology enrichment.
However, many of the tasks or subproblems that need to be solved in either of these domains are
very similar and can be reused. Hence, more general frameworks such as Text2Onto~\cite{cimiano2005text2onto} or BOEMIE~\cite{boemie} have
arisen which deal with a combination of these tasks.
Our framework is designed to handle the common ontology learning tasks,
including ontology creation, population, enrichment, merging and mapping.
Each of these tasks is performed in one or more of the framework's main module,
not as independent or isolated features,
but as an integral part of the framework's learning process.

The module for processing unstructured data can be interpreted as an ontology
creation and population pipeline, because the input is raw unstructured data and the output is
a brand new ontology where everything from classes and relations to specific instances is
created automatically from scratch.
On the other hand, once these newly created ontologies are delivered to the module for knowledge
discovery, several processes are performed which can be interpreted as forms of ontology
enrichment, since new relations can be discovered and inference rules can be automatically defined.
Finally, in the module for processing structured data a process of ontology merging and mapping is
performed, to normalize the incoming ontologies against the knowledge already stored.

\subsection{Quantity and complexity}

Regarding the quantity and complexity of concepts recognized, the existing solutions can
be divided into those that only extract entities (e.g., KnowItAll), those who only extract
relations (e.g., ADAPTATIVA~\cite{adaptativa}, LEILA~\cite{leila}), and those which attempt to extract both (e.g., Artequakt, Web->KB~\cite{webkb}, BOEMIE).
Our approach corresponds to this last category, with the addition that it attempts to extract also
more abstract concepts via techniques such as hierarchical clustering.
The discovery of inference rules is another relevant task that we intend to integrate
in the module for knowledge discovery, as part of the process for generating new knowledge.
This process enriches the ontologies already built by adding higher-level knowledge, in the form
of logical predicates or axioms. These can in turn be used later for discovering missing instances
or relations, or for detecting outliers and mistakes.

\subsection{Use of machine learning}

Most systems employ some form of machine learning tools for most of the
tasks. In particular, many employ NLP tools to process natural text and extract pieces of knowledge,
and statistical techniques to detect clusters. However, in general, the architecture of these
systems generally follows a very strictly designed pipeline, where components are carefully
connected to each other. Our proposal is different in the sense that we pretend our system
to be fully learnable in the future. All components, algorithms, and parts, can be measured and evaluated
automatically, if the knowledge of how these parts interact is described in an organizational
ontology inside the framework.

% NUEVO
The organizational ontology, once defined, should provide a description of the framework's
modules, down to a degree of detail that allows performing automated reasoning about which
components can be connected. This ontology could be used in an automatic pipeline
designer, i.e., a module that learns how to combine specific components
(algorithms, data sources, etc.) for a particular learning task. By self-evaluating
its own performance on each particular task (given a combination of the evaluation
metrics presented in section~\ref{sec:evaluation}), this module could learn, for example,
if for a particular data source is more convenient applying entity extraction or removing
stopwords, or which clustering algorithm performs better.
As an illustrative example, Figure~\ref{fig:pipeline} shows several possible
learning pipelines built from common components. A relevant evaluation metric for each
of these pipelines could allow the framework to automatically discover the best combinations.
This opens the door to the possibility of, in time, the framework
itself learning which algorithms and approaches work better in each task, or in each domain.
% ---

\begin{figure}[htb]
    \begin{center}
        \includegraphics[width=0.8\columnwidth]{Graphics/optimization.pdf}
        \caption{Schematic representation of the possible different evaluated pipelines inside the framework.}
        \label{fig:pipeline}
    \end{center}
\end{figure}

\subsection{Human input}

Most of the frameworks and solutions existing require a certain degree of human interaction.
In most cases, a domain expert is expected to interact with a computational tool to either validate or
refine the output of learning pipeline.
As explained before, our approach proposes to be completely automated,
in the sense that the final knowledge stored may never be revised by a human. The whole framework is thus
designed with the purpose of complete automation. All the decisions that determine which fragments
of the information extracted get stored are based on computational parameters that can
be automatically tunned.
This does not deny some cases where a domain expert could interact with the framework,
through some form of natural language interface, in order to access the stored knowledge.
However, this possibility of interaction is a result of the framework, not a necessity.

\subsection{Generalization}

With respect to domain restrictions, we can classify existing solutions into those which are
completely independent of domain (e.g., KnowItAll, LEILA, ISOLDE~\cite{isolde}) and those which are tailored
to particular domains (e.g., SOBA, Artequakt). The domain independent solutions are usually designed
so that there is no particular dependence tied to a domain, hence, the are reusable in several domains.
However, this means one system can be used in one domain or another, but it doesn't necessarily
means that the same system can learn a piece of knowledge of two different
domains \emph{simultaneously}.

If a system is simply designed to be domain-independent, and used to learn from two very different
domains, the expected result is some sort of combined ontology that represents both domains with
an approximation of the union of the concepts (entities and relations) therein.
This might not be the ideal representation, specially when extending this to many different domains.
Trying to build a single ontology that encompasses all the knowledge that can be mined from
several different domains (possible in the order of hundreds or thousands) can be significantly
more difficult than a simple summation or union of each of the single domains.

Our approach to multi-domain coverage is different. Even though the case study presented only involves
one ontology, our framework is designed to learn a new ontology every time
a new domain is discovered. This new ontology can be potentially merged into with
existing ontologies, but it can also simply be stored as a brand new piece of usable knowledge.
In time, the system would accumulate different ontologies for different domains.
When the need arises to use such knowledge %(e.g. to perform automated question answering),
the framework would decide which ontologies are relevant in a particular task. % (i.e., a particular query).
This in turn requires a more complex processing since an ontology merging approach could be
necessary to compute a final output for a task that involves multiple domains. However, we believe this increased
complexity is an affordable cost compared to the expressive power gained by representing as much knowledge
as possible without the restrictions of having a single taxonomy, or a single set of inference rules.

This belief is in part inspired by how the expert knowledge works in humans. It is true that
humans have a basic set of skills which could be considered domain-independent, such as our innate
abilities for pattern matching. These basic skills are used across many of the tasks we encounter daily.
The computational analogy is a system with a single general purpose learning algorithm that could
perform, for example, tasks as dissimilar as  speech recognition, image classification and translation
with the same process.
This is the preferred approach for a part of the community of researchers in machine learning~\cite{kaiser2017one},
who expect to build a general purpose intelligence out of a single general purpose learning algorithm
and a single general purpose internal representation.

However, for really complex tasks, we argue that humans use specialized representations,
learning algorithms, and inference techniques. A human expert in a highly complex domain (such as mathematics),
doesn't use the same techniques for inference, than those used in real time tasks such
as object and speech recognition. We acknowledge that inference in highly complex domains cannot be
performed with intuition-based tools. Building an intelligent
system that can perform inference in a number of highly complex domains simultaneously will
require using specialized representations and techniques in each domain. It is towards this
principle that we guide our design decisions.

  \section{Discusión}
